This is python.info, produced by makeinfo version 4.8 from
build/texinfo/python.texi.

Generated by Sphinx 1.1pre.
INFO-DIR-SECTION Documentation tools
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY

     Python 2.7.2, April 02, 2012

     Georg Brandl

     Copyright (C) 1990-2012, Python Software Foundation


File: python.info,  Node: Inline markup<2>,  Next: Cross-linking markup,  Prev: Showing code examples,  Up: Additional Markup Constructs

10.4.5 Inline markup
--------------------

As said before, Sphinx uses interpreted text roles to insert semantic
markup in documents.

  Names of local variables, such as function/method arguments, are an
exception, they should be marked simply with `*var*'.

  For all other roles, you have to write `:rolename:`content`'.

  There are some additional facilities that make cross-referencing
roles more versatile:

   * You may supply an explicit title and reference target, like in
     reST direct hyperlinks: `:role:`title <target>`' will refer to
     _target_, but the link text will be _title_.

   * If you prefix the content with `!', no reference/hyperlink will be
     created.

   * For the Python object roles, if you prefix the content with `~',
     the link text will only be the last component of the target.  For
     example, `:meth:`~Queue.Queue.get`' will refer to
     `Queue.Queue.get' but only display `get' as the link text.

     In HTML output, the link's `title' attribute (that is e.g. shown
     as a tool-tip on mouse-hover) will always be the full target name.

  The following roles refer to objects in modules and are possibly
hyperlinked if a matching identifier is found:

 -- Describe: mod
     The name of a module; a dotted name may be used.  This should also
     be used for package names.

 -- Describe: func
     The name of a Python function; dotted names may be used.  The role
     text should not include trailing parentheses to enhance
     readability.  The parentheses are stripped when searching for
     identifiers.

 -- Describe: data
     The name of a module-level variable or constant.

 -- Describe: const
     The name of a "defined" constant.  This may be a C-language
     `#define' or a Python variable that is not intended to be changed.

 -- Describe: class
     A class name; a dotted name may be used.

 -- Describe: meth
     The name of a method of an object.  The role text should include
     the type name and the method name.  A dotted name may be used.

 -- Describe: attr
     The name of a data attribute of an object.

 -- Describe: exc
     The name of an exception. A dotted name may be used.

  The name enclosed in this markup can include a module name and/or a
class name.  For example, `:func:`filter`' could refer to a function
named `filter' in the current module, or the built-in function of that
name.  In contrast, `:func:`foo.filter`' clearly refers to the `filter'
function in the `foo' module.

  Normally, names in these roles are searched first without any further
qualification, then with the current module name prepended, then with
the current module and class name (if any) prepended.  If you prefix
the name with a dot, this order is reversed.  For example, in the
documentation of the *note codecs: 63. module, `:func:`open`' always
refers to the built-in function, while `:func:`.open`' refers to *note
codecs.open(): a22.

  A similar heuristic is used to determine whether the name is an
attribute of the currently documented class.

  The following roles create cross-references to C-language constructs
if they are defined in the API documentation:

 -- Describe: cdata
     The name of a C-language variable.

 -- Describe: cfunc
     The name of a C-language function. Should include trailing
     parentheses.

 -- Describe: cmacro
     The name of a "simple" C macro, as defined above.

 -- Describe: ctype
     The name of a C-language type.

  The following role does possibly create a cross-reference, but does
not refer to objects:

 -- Describe: token
     The name of a grammar token (used in the reference manual to
     create links between production displays).

  The following role creates a cross-reference to the term in the
glossary:

 -- Describe: term
     Reference to a term in the glossary.  The glossary is created
     using the `glossary' directive containing a definition list with
     terms and definitions.  It does not have to be in the same file as
     the `term' markup, in fact, by default the Python docs have one
     global glossary in the `glossary.rst' file.

     If you use a term that's not explained in a glossary, you'll get a
     warning during build.

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


  The following roles don't do anything special except formatting the
text in a different style:

 -- Describe: command
     The name of an OS-level command, such as `rm'.

 -- Describe: dfn
     Mark the defining instance of a term in the text.  (No index
     entries are generated.)

 -- Describe: envvar
     An environment variable.  Index entries are generated.

 -- Describe: file
     The name of a file or directory.  Within the contents, you can use
     curly braces to indicate a "variable" part, for example:

         ... is installed in :file:`/usr/lib/python2.{x}/site-packages` ...

     In the built documentation, the `x' will be displayed differently
     to indicate that it is to be replaced by the Python minor version.

 -- Describe: guilabel
     Labels presented as part of an interactive user interface should
     be marked using `guilabel'.  This includes labels from text-based
     interfaces such as those created using *note curses: 79. or other
     text-based libraries.  Any label used in the interface should be
     marked with this role, including button labels, window titles,
     field names, menu and menu selection names, and even values in
     selection lists.

 -- Describe: kbd
     Mark a sequence of keystrokes.  What form the key sequence takes
     may depend on platform- or application-specific conventions.  When
     there are no relevant conventions, the names of modifier keys
     should be spelled out, to improve accessibility for new users and
     non-native speakers.  For example, an _xemacs_ key sequence may be
     marked like `:kbd:`C-x C-f`', but without reference to a specific
     application or platform, the same sequence should be marked as
     `:kbd:`Control-x Control-f`'.

 -- Describe: keyword
     The name of a keyword in Python.

 -- Describe: mailheader
     The name of an RFC 822-style mail header.  This markup does not
     imply that the header is being used in an email message, but can
     be used to refer to any header of the same "style."  This is also
     used for headers defined by the various MIME specifications.  The
     header name should be entered in the same way it would normally be
     found in practice, with the camel-casing conventions being
     preferred where there is more than one common usage. For example:
     `:mailheader:`Content-Type`'.

 -- Describe: makevar
     The name of a *make* variable.

 -- Describe: manpage
     A reference to a Unix manual page including the section, e.g.
     `:manpage:`ls(1)`'.

 -- Describe: menuselection
     Menu selections should be marked using the `menuselection' role.
     This is used to mark a complete sequence of menu selections,
     including selecting submenus and choosing a specific operation, or
     any subsequence of such a sequence.  The names of individual
     selections should be separated by `-->'.

     For example, to mark the selection "Start > Programs", use this
     markup:

         :menuselection:`Start --> Programs`

     When including a selection that includes some trailing indicator,
     such as the ellipsis some operating systems use to indicate that
     the command opens a dialog, the indicator should be omitted from
     the selection name.

 -- Describe: mimetype
     The name of a MIME type, or a component of a MIME type (the major
     or minor portion, taken alone).

 -- Describe: newsgroup
     The name of a Usenet newsgroup.

 -- Describe: option
     A command-line option of Python.  The leading hyphen(s) must be
     included.  If a matching `cmdoption' directive exists, it is
     linked to.  For options of other programs or scripts, use simple
     ```code``' markup.

 -- Describe: program
     The name of an executable program.  This may differ from the file
     name for the executable for some platforms.  In particular, the
     `.exe' (or other) extension should be omitted for Windows programs.

 -- Describe: regexp
     A regular expression. Quotes should not be included.

 -- Describe: samp
     A piece of literal text, such as code.  Within the contents, you
     can use curly braces to indicate a "variable" part, as in `:file:'.

     If you don't need the "variable part" indication, use the standard
     ```code``' instead.

  The following roles generate external links:

 -- Describe: pep
     A reference to a Python Enhancement Proposal.  This generates
     appropriate index entries. The text "PEP _number_" is generated;
     in the HTML output, this text is a hyperlink to an online copy of
     the specified PEP.

 -- Describe: rfc
     A reference to an Internet Request for Comments.  This generates
     appropriate index entries. The text "RFC _number_" is generated;
     in the HTML output, this text is a hyperlink to an online copy of
     the specified RFC.

  Note that there are no special roles for including hyperlinks as you
can use the standard reST markup for that purpose.


File: python.info,  Node: Cross-linking markup,  Next: Paragraph-level markup,  Prev: Inline markup<2>,  Up: Additional Markup Constructs

10.4.6 Cross-linking markup
---------------------------

To support cross-referencing to arbitrary sections in the
documentation, the standard reST labels are "abused" a bit: Every label
must precede a section title; and every label name must be unique
throughout the entire documentation source.

  You can then reference to these sections using the
`:ref:`label-name`' role.

  Example:

    .. _my-reference-label:

    Section to cross-reference
    --------------------------

    This is the text of the section.

    It refers to the section itself, see :ref:`my-reference-label`.

The `:ref:' invocation is replaced with the section title.


File: python.info,  Node: Paragraph-level markup,  Next: Table-of-contents markup,  Prev: Cross-linking markup,  Up: Additional Markup Constructs

10.4.7 Paragraph-level markup
-----------------------------

These directives create short paragraphs and can be used inside
information units as well as normal text:

 -- Describe: note
     An especially important bit of information about an API that a
     user should be aware of when using whatever bit of API the note
     pertains to.  The content of the directive should be written in
     complete sentences and include all appropriate punctuation.

     Example:

         .. note::

            This function is not suitable for sending spam e-mails.



 -- Describe: warning
     An important bit of information about an API that a user should be
     aware of when using whatever bit of API the warning pertains to.
     The content of the directive should be written in complete
     sentences and include all appropriate punctuation.  In the
     interest of not scaring users away from pages filled with
     warnings, this directive should only be chosen over `note' for
     information regarding the possibility of crashes, data loss, or
     security implications.

 -- Describe: versionadded
     This directive documents the version of Python which added the
     described feature to the library or C API. When this applies to an
     entire module, it should be placed at the top of the module
     section before any prose.

     The first argument must be given and is the version in question;
     you can add a second argument consisting of a _brief_ explanation
     of the change.

     Example:

         .. versionadded:: 2.5
            The *spam* parameter.

     Note that there must be no blank line between the directive head
     and the explanation; this is to make these blocks visually
     continuous in the markup.

 -- Describe: versionchanged
     Similar to `versionadded', but describes when and what changed in
     the named feature in some way (new parameters, changed side
     effects, etc.).

      * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 


 -- Describe: impl-detail
     This directive is used to mark CPython-specific information.  Use
     either with a block content or a single sentence as an argument,
     i.e. either

         .. impl-detail::

            This describes some implementation detail.

            More explanation.

     or

         .. impl-detail:: This shortly mentions an implementation detail.

     "*CPython implementation detail:*" is automatically prepended to
     the content.

 -- Describe: seealso
     Many sections include a list of references to module documentation
     or external documents.  These lists are created using the
     `seealso' directive.

     The `seealso' directive is typically placed in a section just
     before any sub-sections.  For the HTML output, it is shown boxed
     off from the main flow of the text.

     The content of the `seealso' directive should be a reST definition
     list.  Example:

         .. seealso::

            Module :mod:`zipfile`
               Documentation of the :mod:`zipfile` standard module.

            `GNU tar manual, Basic Tar Format <http://link>`_
               Documentation for tar archive files, including GNU tar extensions.



 -- Describe: rubric
     This directive creates a paragraph heading that is not used to
     create a table of contents node.  It is currently used for the
     "Footnotes" caption.

 -- Describe: centered
     This directive creates a centered boldfaced paragraph.  Use it as
     follows:

         .. centered::

            Paragraph contents.




File: python.info,  Node: Table-of-contents markup,  Next: Index-generating markup,  Prev: Paragraph-level markup,  Up: Additional Markup Constructs

10.4.8 Table-of-contents markup
-------------------------------

Since reST does not have facilities to interconnect several documents,
or split documents into multiple output files, Sphinx uses a custom
directive to add relations between the single files the documentation
is made of, as well as tables of contents.  The `toctree' directive is
the central element.

 -- Describe: toctree
     This directive inserts a "TOC tree" at the current location, using
     the individual TOCs (including "sub-TOC trees") of the files given
     in the directive body.  A numeric `maxdepth' option may be given
     to indicate the depth of the tree; by default, all levels are
     included.

     Consider this example (taken from the library reference index):

         .. toctree::
            :maxdepth: 2

            intro
            strings
            datatypes
            numeric
            (many more files listed here)

     This accomplishes two things:

        * Tables of contents from all those files are inserted, with a
          maximum depth of two, that means one nested heading.
          `toctree' directives in those files are also taken into
          account.

        * Sphinx knows that the relative order of the files `intro',
          `strings' and so forth, and it knows that they are children
          of the shown file, the library index.  From this information
          it generates "next chapter", "previous chapter" and "parent
          chapter" links.

     In the end, all files included in the build process must occur in
     one `toctree' directive; Sphinx will emit a warning if it finds a
     file that is not included, because that means that this file will
     not be reachable through standard navigation.

     The special file `contents.rst' at the root of the source
     directory is the "root" of the TOC tree hierarchy; from it the
     "Contents" page is generated.


File: python.info,  Node: Index-generating markup,  Next: Grammar production displays,  Prev: Table-of-contents markup,  Up: Additional Markup Constructs

10.4.9 Index-generating markup
------------------------------

Sphinx automatically creates index entries from all information units
(like functions, classes or attributes) like discussed before.

  However, there is also an explicit directive available, to make the
index more comprehensive and enable index entries in documents where
information is not mainly contained in information units, such as the
language reference.

  The directive is `index' and contains one or more index entries.
Each entry consists of a type and a value, separated by a colon.

  For example:

    .. index::
       single: execution; context
       module: __main__
       module: sys
       triple: module; search; path

This directive contains five entries, which will be converted to
entries in the generated index which link to the exact location of the
index statement (or, in case of offline media, the corresponding page
number).

  The possible entry types are:

single
     Creates a single index entry.  Can be made a subentry by
     separating the subentry text with a semicolon (this notation is
     also used below to describe what entries are created).

pair
     `pair: loop; statement' is a shortcut that creates two index
     entries, namely `loop; statement' and `statement; loop'.

triple
     Likewise, `triple: module; search; path' is a shortcut that
     creates three index entries, which are `module; search path',
     `search; path, module' and `path; module search'.

module, keyword, operator, object, exception, statement, builtin
     These all create two index entries.  For example, `module:
     hashlib' creates the entries `module; hashlib' and `hashlib;
     module'.

  For index directives containing only "single" entries, there is a
shorthand notation:

    .. index:: BNF, grammar, syntax, notation

This creates four index entries.


File: python.info,  Node: Grammar production displays,  Next: Substitutions,  Prev: Index-generating markup,  Up: Additional Markup Constructs

10.4.10 Grammar production displays
-----------------------------------

Special markup is available for displaying the productions of a formal
grammar.  The markup is simple and does not attempt to model all
aspects of BNF (or any derived forms), but provides enough to allow
context-free grammars to be displayed in a way that causes uses of a
symbol to be rendered as hyperlinks to the definition of the symbol.
There is this directive:

 -- Describe: productionlist
     This directive is used to enclose a group of productions.  Each
     production is given on a single line and consists of a name,
     separated by a colon from the following definition.  If the
     definition spans multiple lines, each continuation line must begin
     with a colon placed at the same column as in the first line.

     Blank lines are not allowed within `productionlist' directive
     arguments.

     The definition can contain token names which are marked as
     interpreted text (e.g. `unaryneg ::= "-" `integer`') - this
     generates cross-references to the productions of these tokens.

     Note that no further reST parsing is done in the production, so
     that you don't have to escape `*' or `|' characters.

  The following is an example taken from the Python Reference Manual:

    .. productionlist::
       try_stmt: try1_stmt | try2_stmt
       try1_stmt: "try" ":" `suite`
                : ("except" [`expression` ["," `target`]] ":" `suite`)+
                : ["else" ":" `suite`]
                : ["finally" ":" `suite`]
       try2_stmt: "try" ":" `suite`
                : "finally" ":" `suite`



File: python.info,  Node: Substitutions,  Prev: Grammar production displays,  Up: Additional Markup Constructs

10.4.11 Substitutions
---------------------

The documentation system provides three substitutions that are defined
by default.  They are set in the build configuration file `conf.py'.

 -- Describe: |release|
     Replaced by the Python release the documentation refers to.  This
     is the full version string including alpha/beta/release candidate
     tags, e.g. `2.5.2b3'.

 -- Describe: |version|
     Replaced by the Python version the documentation refers to. This
     consists only of the major and minor version parts, e.g. `2.5',
     even for version 2.5.1.

 -- Describe: |today|
     Replaced by either today's date, or the date set in the build
     configuration file.  Normally has the format `April 14, 2007'.


File: python.info,  Node: Differences to the LaTeX markup,  Next: Building the documentation,  Prev: Additional Markup Constructs,  Up: Documenting Python

10.5 Differences to the LaTeX markup
====================================

Though the markup language is different, most of the concepts and
markup types of the old LaTeX docs have been kept - environments as
reST directives, inline commands as reST roles and so forth.

  However, there are some differences in the way these work, partly due
to the differences in the markup languages, partly due to improvements
in Sphinx.  This section lists these differences, in order to give
those familiar with the old format a quick overview of what they might
run into.

* Menu:

* Inline markup: Inline markup<3>.
* Information units: Information units<2>.
* Structure::


File: python.info,  Node: Inline markup<3>,  Next: Information units<2>,  Up: Differences to the LaTeX markup

10.5.1 Inline markup
--------------------

These changes have been made to inline markup:

   * *Cross-reference roles*

     Most of the following semantic roles existed previously as inline
     commands, but didn't do anything except formatting the content as
     code.  Now, they cross-reference to known targets (some names have
     also been shortened):

         _mod_ (previously _refmodule_ or _module_) 
         _func_ (previously _function_) 
         _data_ (new) 
         _const_ 
         _class_ 
         _meth_ (previously _method_) 
         _attr_ (previously _member_) 
         _exc_ (previously _exception_) 
         _cdata_ 
         _cfunc_ (previously _cfunction_) 
         _cmacro_ (previously _csimplemacro_) 
         _ctype_ 

     Also different is the handling of _func_ and _meth_: while
     previously parentheses were added to the callable name (like
     `\func{str()}'), they are now appended by the build system -
     appending them in the source will result in double parentheses.
     This also means that `:func:`str(object)`' will not work as
     expected - use ```str(object)``' instead!

   * *Inline commands implemented as directives*

     These were inline commands in LaTeX, but are now directives in
     reST:

         _deprecated_ 
         _versionadded_ 
         _versionchanged_ 

     These are used like so:

         .. deprecated:: 2.5
            Reason of deprecation.

     Also, no period is appended to the text for _versionadded_ and
     _versionchanged_.

         _note_ 
         _warning_ 

     These are used like so:

         .. note::

            Content of note.


   * *Otherwise changed commands*

     The _samp_ command previously formatted code and added quotation
     marks around it.  The _samp_ role, however, features a new
     highlighting system just like _file_ does:

          `:samp:`open({filename}, {mode})`' results in
          `open(_filename_, _mode_)'

   * *Dropped commands*

     These were commands in LaTeX, but are not available as roles:

         _bfcode_ 
         _character_ (use ```'c'``') 
         _citetitle_ (use ``Title <URL>`_') 
         _code_ (use ```code``') 
         _email_ (just write the address in body text) 
         _filenq_ 
         _filevar_ (use the `{...}' highlighting feature of _file_) 
         _programopt_, _longprogramopt_ (use _option_) 
         _ulink_ (use ``Title <URL>`_') 
         _url_ (just write the URL in body text) 
         _var_ (use `*var*') 
         _infinity_, _plusminus_ (use the Unicode character) 
         _shortversion_, _version_ (use the `|version|' and `|release|' substitutions) 
         _emph_, _strong_ (use the reST markup) 

   * *Backslash escaping*

     In reST, a backslash must be escaped in normal text, and in the
     content of roles.  However, in code literals and literal blocks,
     it must not be escaped.  Example: `:file:`C:\\Temp\\my.tmp`' vs.
     ```open("C:\Temp\my.tmp")``'.


File: python.info,  Node: Information units<2>,  Next: Structure,  Prev: Inline markup<3>,  Up: Differences to the LaTeX markup

10.5.2 Information units
------------------------

Information units (_...desc_ environments) have been made reST
directives.  These changes to information units should be noted:

   * *New names*

     "desc" has been removed from every name.  Additionally, these
     directives have new names:

         _cfunction_ (previously _cfuncdesc_) 
         _cmacro_ (previously _csimplemacrodesc_) 
         _exception_ (previously _excdesc_) 
         _function_ (previously _funcdesc_) 
         _attribute_ (previously _memberdesc_) 

     The _classdesc*_ and _excclassdesc_ environments have been
     dropped, the _class_ and _exception_ directives support classes
     documented with and without constructor arguments.

   * *Multiple objects*

     The equivalent of the _...line_ commands is:

         .. function:: do_foo(bar)
                       do_bar(baz)

            Description of the functions.

     IOW, just give one signatures per line, at the same indentation
     level.

   * *Arguments*

     There is no _optional_ command.  Just give function signatures
     like they should appear in the output:

         .. function:: open(filename[, mode[, buffering]])

            Description.

     Note: markup in the signature is not supported.

   * *Indexing*

     The _...descni_ environments have been dropped.  To mark an
     information unit as unsuitable for index entry generation, use the
     _noindex_ option like so:

         .. function:: foo_*
            :noindex:

            Description.


   * *New information units*

     There are new generic information units: One is called "describe"
     and can be used to document things that are not covered by the
     other units:

         .. describe:: a == b

            The equals operator.

     The others are:

         .. cmdoption:: -O

            Describes a command-line option.

         .. envvar:: PYTHONINSPECT

            Describes an environment variable.




File: python.info,  Node: Structure,  Prev: Information units<2>,  Up: Differences to the LaTeX markup

10.5.3 Structure
----------------

The LaTeX docs were split in several toplevel manuals.  Now, all files
are part of the same documentation tree, as indicated by the _toctree_
directives in the sources (though individual output formats may choose
to split them up into parts again).  Every _toctree_ directive embeds
other files as subdocuments of the current file (this structure is not
necessarily mirrored in the filesystem layout).  The toplevel file is
`contents.rst'.

  However, most of the old directory structure has been kept, with the
directories renamed as follows:

   * `api' -> `c-api'

   * `dist' -> `distutils', with the single TeX file split up

   * `doc' -> `documenting'

   * `ext' -> `extending'

   * `inst' -> `installing'

   * `lib' -> `library'

   * `mac' -> merged into `library', with `mac/using.tex' moved to
     `using/mac.rst'

   * `ref' -> `reference'

   * `tut' -> `tutorial', with the single TeX file split up


File: python.info,  Node: Building the documentation,  Prev: Differences to the LaTeX markup,  Up: Documenting Python

10.6 Building the documentation
===============================

You need to have Python 2.4 or higher installed; the toolset used to
build the docs is written in Python.  It is called _Sphinx_, it is not
included in this tree, but maintained separately.  Also needed are the
docutils, supplying the base markup that Sphinx uses, Jinja, a
templating engine, and optionally Pygments, a code highlighter.

* Menu:

* Using make::
* Without make::


File: python.info,  Node: Using make,  Next: Without make,  Up: Building the documentation

10.6.1 Using make
-----------------

Luckily, a Makefile has been prepared so that on Unix, provided you have
installed Python and Subversion, you can just run

    make html

to check out the necessary toolset in the `tools/' subdirectory and
build the HTML output files.  To view the generated HTML, point your
favorite browser at the top-level index `build/html/index.html' after
running "make".

  Available make targets are:

        * "html", which builds standalone HTML files for offline
          viewing.

        * "htmlhelp", which builds HTML files and a HTML Help project
          file usable to convert them into a single Compiled HTML
          (.chm) file - these are popular under Microsoft Windows, but
          very handy on every platform.

          To create the CHM file, you need to run the Microsoft HTML
          Help Workshop over the generated project (.hhp) file.

        * "latex", which builds LaTeX source files as input to
          "pdflatex" to produce PDF documents.

        * "text", which builds a plain text file for each source file.

        * "linkcheck", which checks all external references to see
          whether they are broken, redirected or malformed, and outputs
          this information to stdout as well as a plain-text (.txt)
          file.

        * "changes", which builds an overview over all
          versionadded/versionchanged/ deprecated items in the current
          version. This is meant as a help for the writer of the
          "What's New" document.

        * "coverage", which builds a coverage overview for standard
          library modules and C API.

        * "pydoc-topics", which builds a Python module containing a
          dictionary with plain text documentation for the labels
          defined in `tools/sphinxext/pyspecific.py' - pydoc needs
          these to show topic and keyword help.

  A "make update" updates the Subversion checkouts in `tools/'.


File: python.info,  Node: Without make,  Prev: Using make,  Up: Building the documentation

10.6.2 Without make
-------------------

You'll need to install the Sphinx package, either by checking it out via

    svn co http://svn.python.org/projects/external/Sphinx-0.6.5/sphinx tools/sphinx

or by installing it from PyPI.

  Then, you need to install Docutils, either by checking it out via

    svn co http://svn.python.org/projects/external/docutils-0.6/docutils tools/docutils

or by installing it from <http://docutils.sf.net/>.

  You also need Jinja2, either by checking it out via

    svn co http://svn.python.org/projects/external/Jinja-2.3.1/jinja2 tools/jinja2

or by installing it from PyPI.

  You can optionally also install Pygments, either as a checkout via

    svn co http://svn.python.org/projects/external/Pygments-1.3.1/pygments tools/pygments

or from PyPI at <http://pypi.python.org/pypi/Pygments>.

  Then, make an output directory, e.g. under `build/', and run

    python tools/sphinx-build.py -b<builder> . build/<outputdirectory>

where `<builder>' is one of html, text, latex, or htmlhelp (for
explanations see the make targets above).


File: python.info,  Node: Python HOWTOs,  Next: Python Frequently Asked Questions,  Prev: Documenting Python,  Up: Top

11 Python HOWTOs
****************

Python HOWTOs are documents that cover a single, specific topic, and
attempt to cover it fairly completely. Modelled on the Linux
Documentation Project's HOWTO collection, this collection is an effort
to foster documentation that's more detailed than the Python Library
Reference.

  Currently, the HOWTOs are:

* Menu:

* Python Advocacy HOWTO::
* Porting Extension Modules to 3.0: Porting Extension Modules to 3 0.
* Curses Programming with Python::
* Descriptor HowTo Guide::
* Idioms and Anti-Idioms in Python::
* Functional Programming HOWTO::
* Logging HOWTO::
* Logging Cookbook::
* Regular Expression HOWTO::
* Socket Programming HOWTO::
* Sorting HOW TO::
* Unicode HOWTO::
* HOWTO Fetch Internet Resources Using urllib2::
* HOWTO Use Python in the web::

Python Advocacy HOWTO

* Reasons to Use Python::
* Arguments and Rebuttals::
* Useful Resources::

Reasons to Use Python

* Programmability::
* Prototyping::
* Simplicity and Ease of Understanding::
* Java Integration::

Porting Extension Modules to 3.0

* Conditional compilation::
* Changes to Object APIs::
* Module initialization and state::
* Other options: Other options<2>.

Changes to Object APIs

* str/unicode Unification::
* long/int Unification::

Curses Programming with Python

* What is curses?::
* Starting and ending a curses application::
* Windows and Pads::
* Displaying Text::
* User Input::
* For More Information::

What is curses?

* The Python curses module::

Displaying Text

* Attributes and Color::

Descriptor HowTo Guide

* Abstract::
* Definition and Introduction::
* Descriptor Protocol::
* Invoking Descriptors: Invoking Descriptors<2>.
* Descriptor Example::
* Properties::
* Functions and Methods::
* Static Methods and Class Methods::

Idioms and Anti-Idioms in Python

* Language Constructs You Should Not Use::
* Exceptions: Exceptions<8>.
* Using the Batteries::
* Using Backslash to Continue Statements::

Language Constructs You Should Not Use

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::

Functional Programming HOWTO

* Introduction: Introduction<13>.
* Iterators: Iterators<2>.
* Generator expressions and list comprehensions::
* Generators: Generators<2>.
* Built-in functions::
* Small functions and the lambda expression::
* The itertools module::
* The functools module::
* Revision History and Acknowledgements::
* References::

Introduction

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::

Iterators

* Data Types That Support Iterators::

Generators

* Passing values into a generator::

The itertools module

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::

The functools module

* The operator module::
* The functional module::

References

* General::
* Python-specific::
* Python documentation::

Logging HOWTO

* Basic Logging Tutorial::
* Advanced Logging Tutorial::
* Logging Levels::
* Useful Handlers::
* Exceptions raised during logging::
* Using arbitrary objects as messages::
* Optimization::

Basic Logging Tutorial

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::

Advanced Logging Tutorial

* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::

Logging Levels

* Custom Levels::

Logging Cookbook

* Using logging in multiple modules::
* Multiple handlers and formatters::
* Logging to multiple destinations::
* Configuration server example::
* Sending and receiving logging events across a network::
* Adding contextual information to your logging output::
* Logging to a single file from multiple processes::
* Using file rotation::

Adding contextual information to your logging output

* Using LoggerAdapters to impart contextual information::
* Using Filters to impart contextual information::

Regular Expression HOWTO

* Introduction: Introduction<14>.
* Simple Patterns::
* Using Regular Expressions::
* More Pattern Power::
* Modifying Strings::
* Common Problems::
* Feedback::

Simple Patterns

* Matching Characters::
* Repeating Things::

Using Regular Expressions

* Compiling Regular Expressions::
* The Backslash Plague::
* Performing Matches::
* Module-Level Functions: Module-Level Functions<2>.
* Compilation Flags::

More Pattern Power

* More Metacharacters::
* Grouping::
* Non-capturing and Named Groups::
* Lookahead Assertions::

Modifying Strings

* Splitting Strings::
* Search and Replace::

Common Problems

* Use String Methods::
* match() versus search(): match versus search.
* Greedy versus Non-Greedy::
* Using re.VERBOSE: Using re VERBOSE.

Socket Programming HOWTO

* Sockets::
* Creating a Socket::
* Using a Socket::
* Disconnecting::
* Non-blocking Sockets::

Sockets

* History::

Creating a Socket

* IPC::

Using a Socket

* Binary Data::

Disconnecting

* When Sockets Die::

Non-blocking Sockets

* Performance: Performance<2>.

Sorting HOW TO

* Sorting Basics::
* Key Functions::
* Operator Module Functions::
* Ascending and Descending::
* Sort Stability and Complex Sorts::
* The Old Way Using Decorate-Sort-Undecorate::
* The Old Way Using the cmp Parameter::
* Odd and Ends::

Unicode HOWTO

* Introduction to Unicode::
* Python 2.x's Unicode Support: Python 2 x's Unicode Support.
* Reading and Writing Unicode Data::
* Revision History and Acknowledgements: Revision History and Acknowledgements<2>.

Introduction to Unicode

* History of Character Codes::
* Definitions::
* Encodings::
* References: References<2>.

Python 2.x's Unicode Support

* The Unicode Type::
* Unicode Literals in Python Source Code::
* Unicode Properties::
* References: References<3>.

Reading and Writing Unicode Data

* Unicode filenames::
* Tips for Writing Unicode-aware Programs::
* References: References<4>.

HOWTO Fetch Internet Resources Using urllib2

* Introduction: Introduction<15>.
* Fetching URLs::
* Handling Exceptions: Handling Exceptions<2>.
* info and geturl::
* Openers and Handlers::
* Basic Authentication::
* Proxies::
* Sockets and Layers::
* Footnotes: Footnotes<2>.

Fetching URLs

* Data::
* Headers::

Handling Exceptions

* URLError::
* HTTPError::
* Wrapping it Up::

HTTPError

* Error Codes::

Wrapping it Up

* Number 1::
* Number 2::

HOWTO Use Python in the web

* The Low-Level View::
* Step back; WSGI: Step back WSGI.
* Model-View-Controller::
* Ingredients for Websites::
* Frameworks::

The Low-Level View

* Common Gateway Interface::
* mod_python::
* FastCGI and SCGI::
* mod_wsgi::

Common Gateway Interface

* Simple script for testing CGI::
* Setting up CGI on your own server::
* Common problems with CGI scripts::

FastCGI and SCGI

* Setting up FastCGI::

Step back: WSGI

* WSGI Servers::
* Case study; MoinMoin: Case study MoinMoin.

Ingredients for Websites

* Templates::
* Data persistence::

Frameworks

* Some notable frameworks::

Some notable frameworks

* Django::
* TurboGears::
* Zope::
* Other notable frameworks::


File: python.info,  Node: Python Advocacy HOWTO,  Next: Porting Extension Modules to 3 0,  Up: Python HOWTOs

11.1 Python Advocacy HOWTO
==========================

     Author: A.M. Kuchling

     Release: 0.03

Abstract
........

It's usually difficult to get your management to accept open source
software, and Python is no exception to this rule.  This document
discusses reasons to use Python, strategies for winning acceptance,
facts and arguments you can use, and cases where you _shouldn't_ try to
use Python.

* Menu:

* Reasons to Use Python::
* Arguments and Rebuttals::
* Useful Resources::

Reasons to Use Python

* Programmability::
* Prototyping::
* Simplicity and Ease of Understanding::
* Java Integration::


File: python.info,  Node: Reasons to Use Python,  Next: Arguments and Rebuttals,  Up: Python Advocacy HOWTO

11.1.1 Reasons to Use Python
----------------------------

There are several reasons to incorporate a scripting language into your
development process, and this section will discuss them, and why Python
has some properties that make it a particularly good choice.

* Menu:

* Programmability::
* Prototyping::
* Simplicity and Ease of Understanding::
* Java Integration::


File: python.info,  Node: Programmability,  Next: Prototyping,  Up: Reasons to Use Python

11.1.1.1 Programmability
........................

Programs are often organized in a modular fashion.  Lower-level
operations are grouped together, and called by higher-level functions,
which may in turn be used as basic operations by still further upper
levels.

  For example, the lowest level might define a very low-level set of
functions for accessing a hash table.  The next level might use hash
tables to store the headers of a mail message, mapping a header name
like `Date' to a value such as `Tue, 13 May 1997 20:00:54 -0400'.  A
yet higher level may operate on message objects, without knowing or
caring that message headers are stored in a hash table, and so forth.

  Often, the lowest levels do very simple things; they implement a data
structure such as a binary tree or hash table, or they perform some
simple computation, such as converting a date string to a number.  The
higher levels then contain logic connecting these primitive operations.
Using the approach, the primitives can be seen as basic building
blocks which are then glued together to produce the complete product.

  Why is this design approach relevant to Python?  Because Python is
well suited to functioning as such a glue language.  A common approach
is to write a Python module that implements the lower level operations;
for the sake of speed, the implementation might be in C, Java, or even
Fortran.  Once the primitives are available to Python programs, the
logic underlying higher level operations is written in the form of
Python code.  The high-level logic is then more understandable, and
easier to modify.

  John Ousterhout wrote a paper that explains this idea at greater
length, entitled "Scripting: Higher Level Programming for the 21st
Century".  I recommend that you read this paper; see the references for
the URL.  Ousterhout is the inventor of the Tcl language, and therefore
argues that Tcl should be used for this purpose; he only briefly refers
to other languages such as Python, Perl, and Lisp/Scheme, but in
reality, Ousterhout's argument applies to scripting languages in
general, since you could equally write extensions for any of the
languages mentioned above.


File: python.info,  Node: Prototyping,  Next: Simplicity and Ease of Understanding,  Prev: Programmability,  Up: Reasons to Use Python

11.1.1.2 Prototyping
....................

In _The Mythical Man-Month_, Fredrick Brooks suggests the following
rule when planning software projects: "Plan to throw one away; you will
anyway."  Brooks is saying that the first attempt at a software design
often turns out to be wrong; unless the problem is very simple or
you're an extremely good designer, you'll find that new requirements
and features become apparent once development has actually started.  If
these new requirements can't be cleanly incorporated into the program's
structure, you're presented with two unpleasant choices: hammer the new
features into the program somehow, or scrap everything and write a new
version of the program, taking the new features into account from the
beginning.

  Python provides you with a good environment for quickly developing an
initial prototype.  That lets you get the overall program structure and
logic right, and you can fine-tune small details in the fast
development cycle that Python provides.  Once you're satisfied with the
GUI interface or program output, you can translate the Python code into
C++, Fortran, Java, or some other compiled language.

  Prototyping means you have to be careful not to use too many Python
features that are hard to implement in your other language.  Using
`eval()', or regular expressions, or the *note pickle: 12e. module,
means that you're going to need C or Java libraries for formula
evaluation, regular expressions, and serialization, for example.  But
it's not hard to avoid such tricky code, and in the end the translation
usually isn't very difficult.  The resulting code can be rapidly
debugged, because any serious logical errors will have been removed
from the prototype, leaving only more minor slip-ups in the translation
to track down.

  This strategy builds on the earlier discussion of programmability.
Using Python as glue to connect lower-level components has obvious
relevance for constructing prototype systems.  In this way Python can
help you with development, even if end users never come in contact with
Python code at all.  If the performance of the Python version is
adequate and corporate politics allow it, you may not need to do a
translation into C or Java, but it can still be faster to develop a
prototype and then translate it, instead of attempting to produce the
final version immediately.

  One example of this development strategy is Microsoft Merchant
Server. Version 1.0 was written in pure Python, by a company that
subsequently was purchased by Microsoft.  Version 2.0 began to
translate the code into C++, shipping with some C++code and some Python
code.  Version 3.0 didn't contain any Python at all; all the code had
been translated into C++.  Even though the product doesn't contain a
Python interpreter, the Python language has still served a useful
purpose by speeding up development.

  This is a very common use for Python.  Past conference papers have
also described this approach for developing high-level numerical
algorithms; see David M. Beazley and Peter S. Lomdahl's paper "Feeding
a Large-scale Physics Application to Python" in the references for a
good example.  If an algorithm's basic operations are things like "Take
the inverse of this 4000x4000 matrix", and are implemented in some
lower-level language, then Python has almost no additional performance
cost; the extra time required for Python to evaluate an expression like
`m.invert()' is dwarfed by the cost of the actual computation.  It's
particularly good for applications where seemingly endless tweaking is
required to get things right. GUI interfaces and Web sites are prime
examples.

  The Python code is also shorter and faster to write (once you're
familiar with Python), so it's easier to throw it away if you decide
your approach was wrong; if you'd spent two weeks working on it instead
of just two hours, you might waste time trying to patch up what you've
got out of a natural reluctance to admit that those two weeks were
wasted.  Truthfully, those two weeks haven't been wasted, since you've
learnt something about the problem and the technology you're using to
solve it, but it's human nature to view this as a failure of some sort.


File: python.info,  Node: Simplicity and Ease of Understanding,  Next: Java Integration,  Prev: Prototyping,  Up: Reasons to Use Python

11.1.1.3 Simplicity and Ease of Understanding
.............................................

Python is definitely _not_ a toy language that's only usable for small
tasks.  The language features are general and powerful enough to enable
it to be used for many different purposes.  It's useful at the small
end, for 10- or 20-line scripts, but it also scales up to larger
systems that contain thousands of lines of code.

  However, this expressiveness doesn't come at the cost of an obscure
or tricky syntax.  While Python has some dark corners that can lead to
obscure code, there are relatively few such corners, and proper design
can isolate their use to only a few classes or modules.  It's certainly
possible to write confusing code by using too many features with too
little concern for clarity, but most Python code can look a lot like a
slightly-formalized version of human-understandable pseudocode.

  In _The New Hacker's Dictionary_, Eric S. Raymond gives the following
definition for "compact":

     Compact _adj._  Of a design, describes the valuable property that
     it can all be apprehended at once in one's head. This generally
     means the thing created from the design can be used with greater
     facility and fewer errors than an equivalent tool that is not
     compact. Compactness does not imply triviality or lack of power;
     for example, C is compact and FORTRAN is not, but C is more
     powerful than FORTRAN. Designs become non-compact through
     accreting features and cruft that don't merge cleanly into the
     overall design scheme (thus, some fans of Classic C maintain that
     ANSI C is no longer compact).

     (From <http://www.catb.org/~esr/jargon/html/C/compact.html>)

  In this sense of the word, Python is quite compact, because the
language has just a few ideas, which are used in lots of places.  Take
namespaces, for example.  Import a module with `import math', and you
create a new namespace called `math'.  Classes are also namespaces that
share many of the properties of modules, and have a few of their own;
for example, you can create instances of a class. Instances?  They're
yet another namespace.  Namespaces are currently implemented as Python
dictionaries, so they have the same methods as the standard dictionary
data type: .keys() returns all the keys, and so forth.

  This simplicity arises from Python's development history.  The
language syntax derives from different sources; ABC, a relatively
obscure teaching language, is one primary influence, and Modula-3 is
another.  (For more information about ABC and Modula-3, consult their
respective Web sites at <http://www.cwi.nl/~steven/abc/> and
<http://www.m3.org>.)  Other features have come from C, Icon, Algol-68,
and even Perl.  Python hasn't really innovated very much, but instead
has tried to keep the language small and easy to learn, building on
ideas that have been tried in other languages and found useful.

  Simplicity is a virtue that should not be underestimated.  It lets
you learn the language more quickly, and then rapidly write code - code
that often works the first time you run it.


File: python.info,  Node: Java Integration,  Prev: Simplicity and Ease of Understanding,  Up: Reasons to Use Python

11.1.1.4 Java Integration
.........................

If you're working with Java, Jython (<http://www.jython.org/>) is
definitely worth your attention.  Jython is a re-implementation of
Python in Java that compiles Python code into Java bytecodes.  The
resulting environment has very tight, almost seamless, integration with
Java.  It's trivial to access Java classes from Python, and you can
write Python classes that subclass Java classes.  Jython can be used
for prototyping Java applications in much the same way CPython is used,
and it can also be used for test suites for Java code, or embedded in a
Java application to add scripting capabilities.


File: python.info,  Node: Arguments and Rebuttals,  Next: Useful Resources,  Prev: Reasons to Use Python,  Up: Python Advocacy HOWTO

11.1.2 Arguments and Rebuttals
------------------------------

Let's say that you've decided upon Python as the best choice for your
application.  How can you convince your management, or your fellow
developers, to use Python?  This section lists some common arguments
against using Python, and provides some possible rebuttals.

  *Python is freely available software that doesn't cost anything. How
good can it be?*

  Very good, indeed.  These days Linux and Apache, two other pieces of
open source software, are becoming more respected as alternatives to
commercial software, but Python hasn't had all the publicity.

  Python has been around for several years, with many users and
developers.  Accordingly, the interpreter has been used by many people,
and has gotten most of the bugs shaken out of it.  While bugs are still
discovered at intervals, they're usually either quite obscure (they'd
have to be, for no one to have run into them before) or they involve
interfaces to external libraries.  The internals of the language itself
are quite stable.

  Having the source code should be viewed as making the software
available for peer review; people can examine the code, suggest (and
implement) improvements, and track down bugs.  To find out more about
the idea of open source code, along with arguments and case studies
supporting it, go to <http://www.opensource.org>.

  *Who's going to support it?*

  Python has a sizable community of developers, and the number is still
growing.  The Internet community surrounding the language is an active
one, and is worth being considered another one of Python's advantages.
Most questions posted to the comp.lang.python newsgroup are quickly
answered by someone.

  Should you need to dig into the source code, you'll find it's clear
and well-organized, so it's not very difficult to write extensions and
track down bugs yourself.  If you'd prefer to pay for support, there
are companies and individuals who offer commercial support for Python.

  *Who uses Python for serious work?*

  Lots of people; one interesting thing about Python is the surprising
diversity of applications that it's been used for.  People are using
Python to:

   * Run Web sites

   * Write GUI interfaces

   * Control number-crunching code on supercomputers

   * Make a commercial application scriptable by embedding the Python
     interpreter inside it

   * Process large XML data sets

   * Build test suites for C or Java code

  Whatever your application domain is, there's probably someone who's
used Python for something similar.  Yet, despite being useable for such
high-end applications, Python's still simple enough to use for little
jobs.

  See <http://wiki.python.org/moin/OrganizationsUsingPython> for a list
of some of the  organizations that use Python.

  *What are the restrictions on Python's use?*

  They're practically nonexistent.  Consult the `Misc/COPYRIGHT' file
in the source distribution, or the section *note History and License:
2f3d. for the full language, but it boils down to three conditions:

   * You have to leave the copyright notice on the software; if you
     don't include the source code in a product, you have to put the
     copyright notice in the supporting documentation.

   * Don't claim that the institutions that have developed Python
     endorse your product in any way.

   * If something goes wrong, you can't sue for damages.  Practically
     all software licenses contain this condition.

  Notice that you don't have to provide source code for anything that
contains Python or is built with it.  Also, the Python interpreter and
accompanying documentation can be modified and redistributed in any way
you like, and you don't have to pay anyone any licensing fees at all.

  *Why should we use an obscure language like Python instead of
well-known language X?*

  I hope this HOWTO, and the documents listed in the final section,
will help convince you that Python isn't obscure, and has a healthily
growing user base.  One word of advice: always present Python's
positive advantages, instead of concentrating on language X's failings.
People want to know why a solution is good, rather than why all the
other solutions are bad.  So instead of attacking a competing solution
on various grounds, simply show how Python's virtues can help.


File: python.info,  Node: Useful Resources,  Prev: Arguments and Rebuttals,  Up: Python Advocacy HOWTO

11.1.3 Useful Resources
-----------------------

<http://www.pythonology.com/success>
     The Python Success Stories are a collection of stories from
     successful users of Python, with the emphasis on business and
     corporate users.

<http://www.tcl.tk/doc/scripting.html>
     John Ousterhout's white paper on scripting is a good argument for
     the utility of scripting languages, though naturally enough, he
     emphasizes Tcl, the language he developed.  Most of the arguments
     would apply to any scripting language.

<http://www.python.org/workshops/1997-10/proceedings/beazley.html>
     The authors, David M. Beazley and Peter S. Lomdahl,  describe
     their use of Python at Los Alamos National Laboratory. It's
     another good example of how Python can help get real work done.
     This quotation from the paper has been echoed by many people:

          Originally developed as a large monolithic application for
          massively parallel processing systems, we have used Python to
          transform our application into a flexible, highly modular,
          and extremely powerful system for performing simulation, data
          analysis, and visualization. In addition, we describe how
          Python has solved a number of important problems related to
          the development, debugging, deployment, and maintenance of
          scientific software.

<http://pythonjournal.cognizor.com/pyj1/Everitt-Feit_interview98-V1.html>
     This interview with Andy Feit, discussing Infoseek's use of
     Python, can be used to show that choosing Python didn't introduce
     any difficulties into a company's development process, and
     provided some substantial benefits.

<http://www.python.org/workshops/1997-10/proceedings/stein.ps>
     For the 6th Python conference, Greg Stein presented a paper that
     traced Python's adoption and usage at a startup called eShop, and
     later at Microsoft.

<http://www.opensource.org>
     Management may be doubtful of the reliability and usefulness of
     software that wasn't written commercially.  This site presents
     arguments that show how open source software can have considerable
     advantages over closed-source software.

<http://www.faqs.org/docs/Linux-mini/Advocacy.html>
     The Linux Advocacy mini-HOWTO was the inspiration for this
     document, and is also well worth reading for general suggestions
     on winning acceptance for a new technology, such as Linux or
     Python.  In general, you won't make much progress by simply
     attacking existing systems and complaining about their
     inadequacies; this often ends up looking like unfocused whining.
     It's much better to point out some of the many areas where Python
     is an improvement over other systems.


File: python.info,  Node: Porting Extension Modules to 3 0,  Next: Curses Programming with Python,  Prev: Python Advocacy HOWTO,  Up: Python HOWTOs

11.2 Porting Extension Modules to 3.0
=====================================

     author: Benjamin Peterson

Abstract
........

Although changing the C-API was not one of Python 3.0's objectives, the
many Python level changes made leaving 2.x's API intact impossible.  In
fact, some changes such as *note int(): 1ef. and *note long(): 1f0.
unification are more obvious on the C level.  This document endeavors
to document incompatibilities and how they can be worked around.

* Menu:

* Conditional compilation::
* Changes to Object APIs::
* Module initialization and state::
* Other options: Other options<2>.


File: python.info,  Node: Conditional compilation,  Next: Changes to Object APIs,  Up: Porting Extension Modules to 3 0

11.2.1 Conditional compilation
------------------------------

The easiest way to compile only some code for 3.0 is to check if
`PY_MAJOR_VERSION' is greater than or equal to 3.

    #if PY_MAJOR_VERSION >= 3
    #define IS_PY3K
    #endif

API functions that are not present can be aliased to their equivalents
within conditional blocks.


File: python.info,  Node: Changes to Object APIs,  Next: Module initialization and state,  Prev: Conditional compilation,  Up: Porting Extension Modules to 3 0

11.2.2 Changes to Object APIs
-----------------------------

Python 3.0 merged together some types with similar functions while
cleanly separating others.

* Menu:

* str/unicode Unification::
* long/int Unification::


File: python.info,  Node: str/unicode Unification,  Next: long/int Unification,  Up: Changes to Object APIs

11.2.2.1 str/unicode Unification
................................

Python 3.0's *note str(): 1e7. (`PyString_*' functions in C) type is
equivalent to 2.x's *note unicode(): 1f2. (`PyUnicode_*').  The old
8-bit string type has become `bytes()'.  Python 2.6 and later provide a
compatibility header, `bytesobject.h', mapping `PyBytes' names to
`PyString' ones.  For best compatibility with 3.0, `PyUnicode' should
be used for textual data and `PyBytes' for binary data.  It's also
important to remember that `PyBytes' and `PyUnicode' in 3.0 are not
interchangeable like `PyString' and `PyUnicode' are in 2.x.  The
following example shows best practices with regards to `PyUnicode',
`PyString', and `PyBytes'.

    #include "stdlib.h"
    #include "Python.h"
    #include "bytesobject.h"

    /* text example */
    static PyObject *
    say_hello(PyObject *self, PyObject *args) {
        PyObject *name, *result;

        if (!PyArg_ParseTuple(args, "U:say_hello", &name))
            return NULL;

        result = PyUnicode_FromFormat("Hello, %S!", name);
        return result;
    }

    /* just a forward */
    static char * do_encode(PyObject *);

    /* bytes example */
    static PyObject *
    encode_object(PyObject *self, PyObject *args) {
        char *encoded;
        PyObject *result, *myobj;

        if (!PyArg_ParseTuple(args, "O:encode_object", &myobj))
            return NULL;

        encoded = do_encode(myobj);
        if (encoded == NULL)
            return NULL;
        result = PyBytes_FromString(encoded);
        free(encoded);
        return result;
    }



File: python.info,  Node: long/int Unification,  Prev: str/unicode Unification,  Up: Changes to Object APIs

11.2.2.2 long/int Unification
.............................

In Python 3.0, there is only one integer type.  It is called *note
int(): 1ef. on the Python level, but actually corresponds to 2.x's
*note long(): 1f0. type.  In the C-API, `PyInt_*' functions are
replaced by their `PyLong_*' neighbors.  The best course of action here
is using the `PyInt_*' functions aliased to `PyLong_*' found in
`intobject.h'.  The abstract `PyNumber_*' APIs can also be used in some
cases.

    #include "Python.h"
    #include "intobject.h"

    static PyObject *
    add_ints(PyObject *self, PyObject *args) {
        int one, two;
        PyObject *result;

        if (!PyArg_ParseTuple(args, "ii:add_ints", &one, &two))
            return NULL;

        return PyInt_FromLong(one + two);
    }



File: python.info,  Node: Module initialization and state,  Next: Other options<2>,  Prev: Changes to Object APIs,  Up: Porting Extension Modules to 3 0

11.2.3 Module initialization and state
--------------------------------------

Python 3.0 has a revamped extension module initialization system.  (See PEP
3121(1).)  Instead of storing module state in globals, they should be
stored in an interpreter specific structure.  Creating modules that act
correctly in both 2.x and 3.0 is tricky.  The following simple example
demonstrates how.

    #include "Python.h"

    struct module_state {
        PyObject *error;
    };

    #if PY_MAJOR_VERSION >= 3
    #define GETSTATE(m) ((struct module_state*)PyModule_GetState(m))
    #else
    #define GETSTATE(m) (&_state)
    static struct module_state _state;
    #endif

    static PyObject *
    error_out(PyObject *m) {
        struct module_state *st = GETSTATE(m);
        PyErr_SetString(st->error, "something bad happened");
        return NULL;
    }

    static PyMethodDef myextension_methods[] = {
        {"error_out", (PyCFunction)error_out, METH_NOARGS, NULL},
        {NULL, NULL}
    };

    #if PY_MAJOR_VERSION >= 3

    static int myextension_traverse(PyObject *m, visitproc visit, void *arg) {
        Py_VISIT(GETSTATE(m)->error);
        return 0;
    }

    static int myextension_clear(PyObject *m) {
        Py_CLEAR(GETSTATE(m)->error);
        return 0;
    }


    static struct PyModuleDef moduledef = {
            PyModuleDef_HEAD_INIT,
            "myextension",
            NULL,
            sizeof(struct module_state),
            myextension_methods,
            NULL,
            myextension_traverse,
            myextension_clear,
            NULL
    };

    #define INITERROR return NULL

    PyObject *
    PyInit_myextension(void)

    #else
    #define INITERROR return

    void
    initmyextension(void)
    #endif
    {
    #if PY_MAJOR_VERSION >= 3
        PyObject *module = PyModule_Create(&moduledef);
    #else
        PyObject *module = Py_InitModule("myextension", myextension_methods);
    #endif

        if (module == NULL)
            INITERROR;
        struct module_state *st = GETSTATE(module);

        st->error = PyErr_NewException("myextension.Error", NULL, NULL);
        if (st->error == NULL) {
            Py_DECREF(module);
            INITERROR;
        }

    #if PY_MAJOR_VERSION >= 3
        return module;
    #endif
    }


  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-3121


File: python.info,  Node: Other options<2>,  Prev: Module initialization and state,  Up: Porting Extension Modules to 3 0

11.2.4 Other options
--------------------

If you are writing a new extension module, you might consider
Cython(1).  It translates a Python-like language to C.  The extension
modules it creates are compatible with Python 3.x and 2.x.

  ---------- Footnotes ----------

  (1) http://www.cython.org


File: python.info,  Node: Curses Programming with Python,  Next: Descriptor HowTo Guide,  Prev: Porting Extension Modules to 3 0,  Up: Python HOWTOs

11.3 Curses Programming with Python
===================================

     Author: A.M. Kuchling, Eric S. Raymond

     Release: 2.03

Abstract
........

This document describes how to write text-mode programs with Python
2.x, using the *note curses: 79. extension module to control the
display.

* Menu:

* What is curses?::
* Starting and ending a curses application::
* Windows and Pads::
* Displaying Text::
* User Input::
* For More Information::

What is curses?

* The Python curses module::

Displaying Text

* Attributes and Color::


File: python.info,  Node: What is curses?,  Next: Starting and ending a curses application,  Up: Curses Programming with Python

11.3.1 What is curses?
----------------------

The curses library supplies a terminal-independent screen-painting and
keyboard-handling facility for text-based terminals; such terminals
include VT100s, the Linux console, and the simulated terminal provided
by X11 programs such as xterm and rxvt.  Display terminals support
various control codes to perform common operations such as moving the
cursor, scrolling the screen, and erasing areas.  Different terminals
use widely differing codes, and often have their own minor quirks.

  In a world of X displays, one might ask "why bother"?  It's true that
character-cell display terminals are an obsolete technology, but there
are niches in which being able to do fancy things with them are still
valuable.  One is on small-footprint or embedded Unixes that don't
carry an X server.  Another is for tools like OS installers and kernel
configurators that may have to run before X is available.

  The curses library hides all the details of different terminals, and
provides the programmer with an abstraction of a display, containing
multiple non-overlapping windows.  The contents of a window can be
changed in various ways- adding text, erasing it, changing its
appearance-and the curses library will automagically figure out what
control codes need to be sent to the terminal to produce the right
output.

  The curses library was originally written for BSD Unix; the later
System V versions of Unix from AT&T added many enhancements and new
functions. BSD curses is no longer maintained, having been replaced by
ncurses, which is an open-source implementation of the AT&T interface.
If you're using an open-source Unix such as Linux or FreeBSD, your
system almost certainly uses ncurses.  Since most current commercial
Unix versions are based on System V code, all the functions described
here will probably be available.  The older versions of curses carried
by some proprietary Unixes may not support everything, though.

  No one has made a Windows port of the curses module.  On a Windows
platform, try the Console module written by Fredrik Lundh.  The Console
module provides cursor-addressable text output, plus full support for
mouse and keyboard input, and is available from
<http://effbot.org/zone/console-index.htm>.

* Menu:

* The Python curses module::


File: python.info,  Node: The Python curses module,  Up: What is curses?

11.3.1.1 The Python curses module
.................................

Thy Python module is a fairly simple wrapper over the C functions
provided by curses; if you're already familiar with curses programming
in C, it's really easy to transfer that knowledge to Python.  The
biggest difference is that the Python interface makes things simpler,
by merging different C functions such as `addstr()', `mvaddstr()',
`mvwaddstr()', into a single `addstr()' method.  You'll see this
covered in more detail later.

  This HOWTO is simply an introduction to writing text-mode programs
with curses and Python. It doesn't attempt to be a complete guide to
the curses API; for that, see the Python library guide's section on
ncurses, and the C manual pages for ncurses.  It will, however, give
you the basic ideas.


File: python.info,  Node: Starting and ending a curses application,  Next: Windows and Pads,  Prev: What is curses?,  Up: Curses Programming with Python

11.3.2 Starting and ending a curses application
-----------------------------------------------

Before doing anything, curses must be initialized.  This is done by
calling the `initscr()' function, which will determine the terminal
type, send any required setup codes to the terminal, and create various
internal data structures.  If successful, `initscr()' returns a window
object representing the entire screen; this is usually called `stdscr',
after the name of the corresponding C variable.

    import curses
    stdscr = curses.initscr()

Usually curses applications turn off automatic echoing of keys to the
screen, in order to be able to read keys and only display them under
certain circumstances.  This requires calling the `noecho()' function.

    curses.noecho()

Applications will also commonly need to react to keys instantly, without
requiring the Enter key to be pressed; this is called cbreak mode, as
opposed to the usual buffered input mode.

    curses.cbreak()

Terminals usually return special keys, such as the cursor keys or
navigation keys such as Page Up and Home, as a multibyte escape
sequence.  While you could write your application to expect such
sequences and process them accordingly, curses can do it for you,
returning a special value such as `curses.KEY_LEFT'.  To get curses to
do the job, you'll have to enable keypad mode.

    stdscr.keypad(1)

Terminating a curses application is much easier than starting one.
You'll need to call

    curses.nocbreak(); stdscr.keypad(0); curses.echo()

to reverse the curses-friendly terminal settings. Then call the
`endwin()' function to restore the terminal to its original operating
mode.

    curses.endwin()

A common problem when debugging a curses application is to get your
terminal messed up when the application dies without restoring the
terminal to its previous state.  In Python this commonly happens when
your code is buggy and raises an uncaught exception.  Keys are no
longer be echoed to the screen when you type them, for example, which
makes using the shell difficult.

  In Python you can avoid these complications and make debugging much
easier by importing the module *note curses.wrapper: 7d.  It supplies a
`wrapper()' function that takes a callable.  It does the
initializations described above, and also initializes colors if color
support is present.  It then runs your provided callable and finally
deinitializes appropriately.  The callable is called inside a try-catch
clause which catches exceptions, performs curses deinitialization, and
then passes the exception upwards.  Thus, your terminal won't be left
in a funny state on exception.


File: python.info,  Node: Windows and Pads,  Next: Displaying Text,  Prev: Starting and ending a curses application,  Up: Curses Programming with Python

11.3.3 Windows and Pads
-----------------------

Windows are the basic abstraction in curses.  A window object
represents a rectangular area of the screen, and supports various
methods to display text, erase it, allow the user to input strings, and
so forth.

  The `stdscr' object returned by the `initscr()' function is a window
object that covers the entire screen.  Many programs may need only this
single window, but you might wish to divide the screen into smaller
windows, in order to redraw or clear them separately. The `newwin()'
function creates a new window of a given size, returning the new window
object.

    begin_x = 20 ; begin_y = 7
    height = 5 ; width = 40
    win = curses.newwin(height, width, begin_y, begin_x)

A word about the coordinate system used in curses: coordinates are
always passed in the order _y,x_, and the top-left corner of a window
is coordinate (0,0).  This breaks a common convention for handling
coordinates, where the _x_ coordinate usually comes first.  This is an
unfortunate difference from most other computer applications, but it's
been part of curses since it was first written, and it's too late to
change things now.

  When you call a method to display or erase text, the effect doesn't
immediately show up on the display.  This is because curses was
originally written with slow 300-baud terminal connections in mind;
with these terminals, minimizing the time required to redraw the screen
is very important.  This lets curses accumulate changes to the screen,
and display them in the most efficient manner.  For example, if your
program displays some characters in a window, and then clears the
window, there's no need to send the original characters because they'd
never be visible.

  Accordingly, curses requires that you explicitly tell it to redraw
windows, using the `refresh()' method of window objects.  In practice,
this doesn't really complicate programming with curses much. Most
programs go into a flurry of activity, and then pause waiting for a
keypress or some other action on the part of the user.  All you have to
do is to be sure that the screen has been redrawn before pausing to
wait for user input, by simply calling `stdscr.refresh()' or the
`refresh()' method of some other relevant window.

  A pad is a special case of a window; it can be larger than the actual
display screen, and only a portion of it displayed at a time. Creating
a pad simply requires the pad's height and width, while refreshing a
pad requires giving the coordinates of the on-screen area where a
subsection of the pad will be displayed.

    pad = curses.newpad(100, 100)
    #  These loops fill the pad with letters; this is
    # explained in the next section
    for y in range(0, 100):
        for x in range(0, 100):
            try: pad.addch(y,x, ord('a') + (x*x+y*y) % 26 )
            except curses.error: pass

    #  Displays a section of the pad in the middle of the screen
    pad.refresh( 0,0, 5,5, 20,75)

The `refresh()' call displays a section of the pad in the rectangle
extending from coordinate (5,5) to coordinate (20,75) on the screen;
the upper left corner of the displayed section is coordinate (0,0) on
the pad.  Beyond that difference, pads are exactly like ordinary
windows and support the same methods.

  If you have multiple windows and pads on screen there is a more
efficient way to go, which will prevent annoying screen flicker at
refresh time.  Use the `noutrefresh()' method of each window to update
the data structure representing the desired state of the screen; then
change the physical screen to match the desired state in one go with
the function `doupdate()'.  The normal `refresh()' method calls
`doupdate()' as its last act.


File: python.info,  Node: Displaying Text,  Next: User Input,  Prev: Windows and Pads,  Up: Curses Programming with Python

11.3.4 Displaying Text
----------------------

From a C programmer's point of view, curses may sometimes look like a
twisty maze of functions, all subtly different.  For example,
`addstr()' displays a string at the current cursor location in the
`stdscr' window, while `mvaddstr()' moves to a given y,x coordinate
first before displaying the string. `waddstr()' is just like
`addstr()', but allows specifying a window to use, instead of using
`stdscr' by default. `mvwaddstr()' follows similarly.

  Fortunately the Python interface hides all these details; `stdscr' is
a window object like any other, and methods like `addstr()' accept
multiple argument forms.  Usually there are four different forms.

Form                                  Description
------------------------------------------------------------------------------------------ 
_str_ or _ch_                         Display the string _str_ or character _ch_ at the
                                      current position
_str_ or _ch_, _attr_                 Display the string _str_ or character _ch_, using
                                      attribute _attr_ at the current position
_y_, _x_, _str_ or _ch_               Move to position _y,x_ within the window, and
                                      display _str_ or _ch_
_y_, _x_, _str_ or _ch_, _attr_       Move to position _y,x_ within the window, and
                                      display _str_ or _ch_, using attribute _attr_

  Attributes allow displaying text in highlighted forms, such as in
boldface, underline, reverse code, or in color.  They'll be explained
in more detail in the next subsection.

  The `addstr()' function takes a Python string as the value to be
displayed, while the `addch()' functions take a character, which can be
either a Python string of length 1 or an integer.  If it's a string,
you're limited to displaying characters between 0 and 255.  SVr4 curses
provides constants for extension characters; these constants are
integers greater than 255.  For example, `ACS_PLMINUS' is a +/- symbol,
and `ACS_ULCORNER' is the upper left corner of a box (handy for drawing
borders).

  Windows remember where the cursor was left after the last operation,
so if you leave out the _y,x_ coordinates, the string or character will
be displayed wherever the last operation left off.  You can also move
the cursor with the `move(y,x)' method.  Because some terminals always
display a flashing cursor, you may want to ensure that the cursor is
positioned in some location where it won't be distracting; it can be
confusing to have the cursor blinking at some apparently random
location.

  If your application doesn't need a blinking cursor at all, you can
call `curs_set(0)' to make it invisible.  Equivalently, and for
compatibility with older curses versions, there's a `leaveok(bool)'
function.  When _bool_ is true, the curses library will attempt to
suppress the flashing cursor, and you won't need to worry about leaving
it in odd locations.

* Menu:

* Attributes and Color::


File: python.info,  Node: Attributes and Color,  Up: Displaying Text

11.3.4.1 Attributes and Color
.............................

Characters can be displayed in different ways.  Status lines in a
text-based application are commonly shown in reverse video; a text
viewer may need to highlight certain words.  curses supports this by
allowing you to specify an attribute for each cell on the screen.

  An attribute is a integer, each bit representing a different
attribute.  You can try to display text with multiple attribute bits
set, but curses doesn't guarantee that all the possible combinations
are available, or that they're all visually distinct.  That depends on
the ability of the terminal being used, so it's safest to stick to the
most commonly available attributes, listed here.

Attribute                  Description
---------------------------------------------------------------------- 
`A_BLINK'                  Blinking text
`A_BOLD'                   Extra bright or bold text
`A_DIM'                    Half bright text
`A_REVERSE'                Reverse-video text
`A_STANDOUT'               The best highlighting mode available
`A_UNDERLINE'              Underlined text

  So, to display a reverse-video status line on the top line of the
screen, you could code:

    stdscr.addstr(0, 0, "Current mode: Typing mode",
                  curses.A_REVERSE)
    stdscr.refresh()

The curses library also supports color on those terminals that provide
it, The most common such terminal is probably the Linux console,
followed by color xterms.

  To use color, you must call the `start_color()' function soon after
calling `initscr()', to initialize the default color set (the *note
curses.wrapper.wrapper(): 13c2. function does this automatically).
Once that's done, the `has_colors()' function returns TRUE if the
terminal in use can actually display color.  (Note: curses uses the
American spelling 'color', instead of the Canadian/British spelling
'colour'.  If you're used to the British spelling, you'll have to
resign yourself to misspelling it for the sake of these functions.)

  The curses library maintains a finite number of color pairs,
containing a foreground (or text) color and a background color.  You
can get the attribute value corresponding to a color pair with the
`color_pair()' function; this can be bitwise-OR'ed with other
attributes such as `A_REVERSE', but again, such combinations are not
guaranteed to work on all terminals.

  An example, which displays a line of text using color pair 1:

    stdscr.addstr( "Pretty text", curses.color_pair(1) )
    stdscr.refresh()

As I said before, a color pair consists of a foreground and background
color.  `start_color()' initializes 8 basic colors when it activates
color mode.  They are: 0:black, 1:red, 2:green, 3:yellow, 4:blue,
5:magenta, 6:cyan, and 7:white.  The curses module defines named
constants for each of these colors: `curses.COLOR_BLACK',
`curses.COLOR_RED', and so forth.

  The `init_pair(n, f, b)' function changes the definition of color
pair _n_, to foreground color f and background color b.  Color pair 0
is hard-wired to white on black, and cannot be changed.

  Let's put all this together. To change color 1 to red text on a white
background, you would call:

    curses.init_pair(1, curses.COLOR_RED, curses.COLOR_WHITE)

When you change a color pair, any text already displayed using that
color pair will change to the new colors.  You can also display new
text in this color with:

    stdscr.addstr(0,0, "RED ALERT!", curses.color_pair(1) )

Very fancy terminals can change the definitions of the actual colors to
a given RGB value.  This lets you change color 1, which is usually red,
to purple or blue or any other color you like.  Unfortunately, the
Linux console doesn't support this, so I'm unable to try it out, and
can't provide any examples.  You can check if your terminal can do this
by calling `can_change_color()', which returns TRUE if the capability
is there.  If you're lucky enough to have such a talented terminal,
consult your system's man pages for more information.


File: python.info,  Node: User Input,  Next: For More Information,  Prev: Displaying Text,  Up: Curses Programming with Python

11.3.5 User Input
-----------------

The curses library itself offers only very simple input mechanisms.
Python's support adds a text-input widget that makes up some of the
lack.

  The most common way to get input to a window is to use its `getch()'
method.  `getch()' pauses and waits for the user to hit a key,
displaying it if `echo()' has been called earlier.  You can optionally
specify a coordinate to which the cursor should be moved before pausing.

  It's possible to change this behavior with the method `nodelay()'.
After `nodelay(1)', `getch()' for the window becomes non-blocking and
returns `curses.ERR' (a value of -1) when no input is ready.  There's
also a `halfdelay()' function, which can be used to (in effect) set a
timer on each `getch()'; if no input becomes available within a
specified delay (measured in tenths of a second), curses raises an
exception.

  The `getch()' method returns an integer; if it's between 0 and 255, it
represents the ASCII code of the key pressed.  Values greater than 255
are special keys such as Page Up, Home, or the cursor keys. You can
compare the value returned to constants such as `curses.KEY_PPAGE',
`curses.KEY_HOME', or `curses.KEY_LEFT'.  Usually the main loop of your
program will look something like this:

    while 1:
        c = stdscr.getch()
        if c == ord('p'): PrintDocument()
        elif c == ord('q'): break  # Exit the while()
        elif c == curses.KEY_HOME: x = y = 0

The *note curses.ascii: 7a. module supplies ASCII class membership
functions that take either integer or 1-character-string arguments;
these may be useful in writing more readable tests for your command
interpreters.  It also supplies conversion functions  that take either
integer or 1-character-string arguments and return the same type.  For
example, *note curses.ascii.ctrl(): 13d5. returns the control character
corresponding to its argument.

  There's also a method to retrieve an entire string, `getstr()'.  It
isn't used very often, because its functionality is quite limited; the
only editing keys available are the backspace key and the Enter key,
which terminates the string.  It can optionally be limited to a fixed
number of characters.

    curses.echo()            # Enable echoing of characters

    # Get a 15-character string, with the cursor on the top line
    s = stdscr.getstr(0,0, 15)

The Python *note curses.textpad: 7c. module supplies something better.
With it, you can turn a window into a text box that supports an
Emacs-like set of keybindings.  Various methods of `Textbox' class
support editing with input validation and gathering the edit results
either with or without trailing spaces.   See the library documentation
on *note curses.textpad: 7c. for the details.


File: python.info,  Node: For More Information,  Prev: User Input,  Up: Curses Programming with Python

11.3.6 For More Information
---------------------------

This HOWTO didn't cover some advanced topics, such as screen-scraping or
capturing mouse events from an xterm instance.  But the Python library
page for the curses modules is now pretty complete.  You should browse
it next.

  If you're in doubt about the detailed behavior of any of the ncurses
entry points, consult the manual pages for your curses implementation,
whether it's ncurses or a proprietary Unix vendor's.  The manual pages
will document any quirks, and provide complete lists of all the
functions, attributes, and `ACS_*' characters available to you.

  Because the curses API is so large, some functions aren't supported
in the Python interface, not because they're difficult to implement,
but because no one has needed them yet.  Feel free to add them and then
submit a patch.  Also, we don't yet have support for the menu library
associated with ncurses; feel free to add that.

  If you write an interesting little program, feel free to contribute
it as another demo.  We can always use more of them!

  The ncurses FAQ:
<http://invisible-island.net/ncurses/ncurses.faq.html>


File: python.info,  Node: Descriptor HowTo Guide,  Next: Idioms and Anti-Idioms in Python,  Prev: Curses Programming with Python,  Up: Python HOWTOs

11.4 Descriptor HowTo Guide
===========================

     Author: Raymond Hettinger

     Contact: <python at rcn dot com>

* Menu:

* Abstract::
* Definition and Introduction::
* Descriptor Protocol::
* Invoking Descriptors: Invoking Descriptors<2>.
* Descriptor Example::
* Properties::
* Functions and Methods::
* Static Methods and Class Methods::


File: python.info,  Node: Abstract,  Next: Definition and Introduction,  Up: Descriptor HowTo Guide

11.4.1 Abstract
---------------

Defines descriptors, summarizes the protocol, and shows how descriptors
are called.  Examines a custom descriptor and several built-in python
descriptors including functions, properties, static methods, and class
methods.  Shows how each works by giving a pure Python equivalent and a
sample application.

  Learning about descriptors not only provides access to a larger
toolset, it creates a deeper understanding of how Python works and an
appreciation for the elegance of its design.


File: python.info,  Node: Definition and Introduction,  Next: Descriptor Protocol,  Prev: Abstract,  Up: Descriptor HowTo Guide

11.4.2 Definition and Introduction
----------------------------------

In general, a descriptor is an object attribute with "binding
behavior", one whose attribute access has been overridden by methods in
the descriptor protocol.  Those methods are *note __get__(): 6ea, *note
__set__(): 6eb, and *note __delete__(): 6ec.  If any of those methods
are defined for an object, it is said to be a descriptor.

  The default behavior for attribute access is to get, set, or delete
the attribute from an object's dictionary.  For instance, `a.x' has a
lookup chain starting with `a.__dict__['x']', then
`type(a).__dict__['x']', and continuing through the base classes of
`type(a)' excluding metaclasses. If the looked-up value is an object
defining one of the descriptor methods, then Python may override the
default behavior and invoke the descriptor method instead.  Where this
occurs in the precedence chain depends on which descriptor methods were
defined.  Note that descriptors are only invoked for new style objects
or classes (a class is new style if it inherits from *note object: 1ee.
or *note type: 47d.).

  Descriptors are a powerful, general purpose protocol.  They are the
mechanism behind properties, methods, static methods, class methods,
and *note super(): 36c.  They are used used throughout Python itself to
implement the new style classes introduced in version 2.2.  Descriptors
simplify the underlying C-code and offer a flexible set of new tools
for everyday Python programs.


File: python.info,  Node: Descriptor Protocol,  Next: Invoking Descriptors<2>,  Prev: Definition and Introduction,  Up: Descriptor HowTo Guide

11.4.3 Descriptor Protocol
--------------------------

`descr.__get__(self, obj, type=None) --> value'

  `descr.__set__(self, obj, value) --> None'

  `descr.__delete__(self, obj) --> None'

  That is all there is to it.  Define any of these methods and an
object is considered a descriptor and can override default behavior
upon being looked up as an attribute.

  If an object defines both *note __get__(): 6ea. and *note __set__():
6eb, it is considered a data descriptor.  Descriptors that only define
*note __get__(): 6ea. are called non-data descriptors (they are
typically used for methods but other uses are possible).

  Data and non-data descriptors differ in how overrides are calculated
with respect to entries in an instance's dictionary.  If an instance's
dictionary has an entry with the same name as a data descriptor, the
data descriptor takes precedence.  If an instance's dictionary has an
entry with the same name as a non-data descriptor, the dictionary entry
takes precedence.

  To make a read-only data descriptor, define both *note __get__():
6ea. and *note __set__(): 6eb. with the *note __set__(): 6eb. raising
an *note AttributeError: 1f5. when called.  Defining the *note
__set__(): 6eb. method with an exception raising placeholder is enough
to make it a data descriptor.


File: python.info,  Node: Invoking Descriptors<2>,  Next: Descriptor Example,  Prev: Descriptor Protocol,  Up: Descriptor HowTo Guide

11.4.4 Invoking Descriptors
---------------------------

A descriptor can be called directly by its method name.  For example,
`d.__get__(obj)'.

  Alternatively, it is more common for a descriptor to be invoked
automatically upon attribute access.  For example, `obj.d' looks up `d'
in the dictionary of `obj'.  If `d' defines the method *note __get__():
6ea, then `d.__get__(obj)' is invoked according to the precedence rules
listed below.

  The details of invocation depend on whether `obj' is an object or a
class.  Either way, descriptors only work for new style objects and
classes.  A class is new style if it is a subclass of *note object: 1ee.

  For objects, the machinery is in *note object.__getattribute__():
32a. which transforms `b.x' into `type(b).__dict__['x'].__get__(b,
type(b))'.  The implementation works through a precedence chain that
gives data descriptors priority over instance variables, instance
variables priority over non-data descriptors, and assigns lowest
priority to *note __getattr__(): 320. if provided.  The full C
implementation can be found in *note PyObject_GenericGetAttr(): 2a72. in
Objects/object.c(1).

  For classes, the machinery is in `type.__getattribute__()' which
transforms `B.x' into `B.__dict__['x'].__get__(None, B)'.  In pure
Python, it looks like:

    def __getattribute__(self, key):
        "Emulate type_getattro() in Objects/typeobject.c"
        v = object.__getattribute__(self, key)
        if hasattr(v, '__get__'):
           return v.__get__(None, self)
        return v

The important points to remember are:

   * descriptors are invoked by the *note __getattribute__(): 32a.
     method

   * overriding *note __getattribute__(): 32a. prevents automatic
     descriptor calls

   * *note __getattribute__(): 32a. is only available with new style
     classes and objects

   * *note object.__getattribute__(): 32a. and
     `type.__getattribute__()' make different calls to *note __get__():
     6ea.

   * data descriptors always override instance dictionaries.

   * non-data descriptors may be overridden by instance dictionaries.

  The object returned by `super()' also has a custom *note
__getattribute__(): 32a.  method for invoking descriptors.  The call
`super(B, obj).m()' searches `obj.__class__.__mro__' for the base class
`A' immediately following `B' and then returns
`A.__dict__['m'].__get__(obj, A)'.  If not a descriptor, `m' is
returned unchanged.  If not in the dictionary, `m' reverts to a search
using *note object.__getattribute__(): 32a.

  Note, in Python 2.2, `super(B, obj).m()' would only invoke *note
__get__(): 6ea. if `m' was a data descriptor.  In Python 2.3, non-data
descriptors also get invoked unless an old-style class is involved.
The implementation details are in `super_getattro()' in
Objects/typeobject.c(2) and a pure Python equivalent can be found in
Guido's Tutorial(3).

  The details above show that the mechanism for descriptors is embedded
in the *note __getattribute__(): 32a. methods for *note object: 1ee,
*note type: 47d, and *note super(): 36c.  Classes inherit this
machinery when they derive from *note object: 1ee. or if they have a
meta-class providing similar functionality.  Likewise, classes can
turn-off descriptor invocation by overriding *note __getattribute__():
32a.

  ---------- Footnotes ----------

  (1)
http://svn.python.org/view/python/trunk/Objects/object.c?view=markup

  (2)
http://svn.python.org/view/python/trunk/Objects/typeobject.c?view=markup

  (3) http://www.python.org/2.2.3/descrintro.html#cooperation


File: python.info,  Node: Descriptor Example,  Next: Properties,  Prev: Invoking Descriptors<2>,  Up: Descriptor HowTo Guide

11.4.5 Descriptor Example
-------------------------

The following code creates a class whose objects are data descriptors
which print a message for each get or set.  Overriding *note
__getattribute__(): 32a. is alternate approach that could do this for
every attribute.  However, this descriptor is useful for monitoring
just a few chosen attributes:

    class RevealAccess(object):
        """A data descriptor that sets and returns values
           normally and prints a message logging their access.
        """

        def __init__(self, initval=None, name='var'):
            self.val = initval
            self.name = name

        def __get__(self, obj, objtype):
            print 'Retrieving', self.name
            return self.val

        def __set__(self, obj, val):
            print 'Updating' , self.name
            self.val = val

    >>> class MyClass(object):
        x = RevealAccess(10, 'var "x"')
        y = 5

    >>> m = MyClass()
    >>> m.x
    Retrieving var "x"
    10
    >>> m.x = 20
    Updating var "x"
    >>> m.x
    Retrieving var "x"
    20
    >>> m.y
    5

The protocol is simple and offers exciting possibilities.  Several use
cases are so common that they have been packaged into individual
function calls.  Properties, bound and unbound methods, static methods,
and class methods are all based on the descriptor protocol.


File: python.info,  Node: Properties,  Next: Functions and Methods,  Prev: Descriptor Example,  Up: Descriptor HowTo Guide

11.4.6 Properties
-----------------

Calling *note property(): 476. is a succinct way of building a data
descriptor that triggers function calls upon access to an attribute.
Its signature is:

    property(fget=None, fset=None, fdel=None, doc=None) -> property attribute

The documentation shows a typical use to define a managed attribute `x':

    class C(object):
        def getx(self): return self.__x
        def setx(self, value): self.__x = value
        def delx(self): del self.__x
        x = property(getx, setx, delx, "I'm the 'x' property.")

To see how *note property(): 476. is implemented in terms of the
descriptor protocol, here is a pure Python equivalent:

    class Property(object):
        "Emulate PyProperty_Type() in Objects/descrobject.c"

        def __init__(self, fget=None, fset=None, fdel=None, doc=None):
            self.fget = fget
            self.fset = fset
            self.fdel = fdel
            self.__doc__ = doc

        def __get__(self, obj, objtype=None):
            if obj is None:
                return self
            if self.fget is None:
                raise AttributeError, "unreadable attribute"
            return self.fget(obj)

        def __set__(self, obj, value):
            if self.fset is None:
                raise AttributeError, "can't set attribute"
            self.fset(obj, value)

        def __delete__(self, obj):
            if self.fdel is None:
                raise AttributeError, "can't delete attribute"
            self.fdel(obj)

The *note property(): 476. builtin helps whenever a user interface has
granted attribute access and then subsequent changes require the
intervention of a method.

  For instance, a spreadsheet class may grant access to a cell value
through `Cell('b10').value'. Subsequent improvements to the program
require the cell to be recalculated on every access; however, the
programmer does not want to affect existing client code accessing the
attribute directly.  The solution is to wrap access to the value
attribute in a property data descriptor:

    class Cell(object):
        . . .
        def getvalue(self, obj):
            "Recalculate cell before returning value"
            self.recalc()
            return obj._value
        value = property(getvalue)



File: python.info,  Node: Functions and Methods,  Next: Static Methods and Class Methods,  Prev: Properties,  Up: Descriptor HowTo Guide

11.4.7 Functions and Methods
----------------------------

Python's object oriented features are built upon a function based
environment.  Using non-data descriptors, the two are merged seamlessly.

  Class dictionaries store methods as functions.  In a class
definition, methods are written using *note def: 3e3. and *note lambda:
3f2, the usual tools for creating functions.  The only difference from
regular functions is that the first argument is reserved for the object
instance.  By Python convention, the instance reference is called
_self_ but may be called _this_ or any other variable name.

  To support method calls, functions include the *note __get__(): 6ea.
method for binding methods during attribute access.  This means that
all functions are non-data descriptors which return bound or unbound
methods depending whether they are invoked from an object or a class.
In pure python, it works like this:

    class Function(object):
        . . .
        def __get__(self, obj, objtype=None):
            "Simulate func_descr_get() in Objects/funcobject.c"
            return types.MethodType(self, obj, objtype)

Running the interpreter shows how the function descriptor works in
practice:

    >>> class D(object):
         def f(self, x):
              return x

    >>> d = D()
    >>> D.__dict__['f'] # Stored internally as a function
    <function f at 0x00C45070>
    >>> D.f             # Get from a class becomes an unbound method
    <unbound method D.f>
    >>> d.f             # Get from an instance becomes a bound method
    <bound method D.f of <__main__.D object at 0x00B18C90>>

The output suggests that bound and unbound methods are two different
types.  While they could have been implemented that way, the actual C
implementation of *note PyMethod_Type: 2c69. in Objects/classobject.c(1)
is a single object with two different representations depending on
whether the `im_self' field is set or is _NULL_ (the C equivalent of
_None_).

  Likewise, the effects of calling a method object depend on the
`im_self' field. If set (meaning bound), the original function (stored
in the `im_func' field) is called as expected with the first argument
set to the instance.  If unbound, all of the arguments are passed
unchanged to the original function. The actual C implementation of
`instancemethod_call()' is only slightly more complex in that it
includes some type checking.

  ---------- Footnotes ----------

  (1)
http://svn.python.org/view/python/trunk/Objects/classobject.c?view=markup


File: python.info,  Node: Static Methods and Class Methods,  Prev: Functions and Methods,  Up: Descriptor HowTo Guide

11.4.8 Static Methods and Class Methods
---------------------------------------

Non-data descriptors provide a simple mechanism for variations on the
usual patterns of binding functions into methods.

  To recap, functions have a *note __get__(): 6ea. method so that they
can be converted to a method when accessed as attributes.  The non-data
descriptor transforms a `obj.f(*args)' call into `f(obj, *args)'.
Calling `klass.f(*args)' becomes `f(*args)'.

  This chart summarizes the binding and its two most useful variants:

      Transformation        Called from an Object      Called from a Class
     ------------------------------------------------------------------------ 
     function              f(obj, *args)              f(*args)
     staticmethod          f(*args)                   f(*args)
     classmethod           f(type(obj), *args)        f(klass, *args)


  Static methods return the underlying function without changes.
Calling either `c.f' or `C.f' is the equivalent of a direct lookup into
`object.__getattribute__(c, "f")' or `object.__getattribute__(C, "f")'.
As a result, the function becomes identically accessible from either an
object or a class.

  Good candidates for static methods are methods that do not reference
the `self' variable.

  For instance, a statistics package may include a container class for
experimental data.  The class provides normal methods for computing the
average, mean, median, and other descriptive statistics that depend on
the data. However, there may be useful functions which are conceptually
related but do not depend on the data.  For instance, `erf(x)' is handy
conversion routine that comes up in statistical work but does not
directly depend on a particular dataset.  It can be called either from
an object or the class:  `s.erf(1.5) --> .9332' or `Sample.erf(1.5) -->
.9332'.

  Since staticmethods return the underlying function with no changes,
the example calls are unexciting:

    >>> class E(object):
         def f(x):
              print x
         f = staticmethod(f)

    >>> print E.f(3)
    3
    >>> print E().f(3)
    3

Using the non-data descriptor protocol, a pure Python version of *note
staticmethod(): 3e4. would look like this:

    class StaticMethod(object):
     "Emulate PyStaticMethod_Type() in Objects/funcobject.c"

     def __init__(self, f):
          self.f = f

     def __get__(self, obj, objtype=None):
          return self.f

Unlike static methods, class methods prepend the class reference to the
argument list before calling the function.  This format is the same for
whether the caller is an object or a class:

    >>> class E(object):
         def f(klass, x):
              return klass.__name__, x
         f = classmethod(f)

    >>> print E.f(3)
    ('E', 3)
    >>> print E().f(3)
    ('E', 3)

This behavior is useful whenever the function only needs to have a class
reference and does not care about any underlying data.  One use for
classmethods is to create alternate class constructors.  In Python 2.3,
the classmethod *note dict.fromkeys(): 8c4. creates a new dictionary
from a list of keys.  The pure Python equivalent is:

    class Dict:
        . . .
        def fromkeys(klass, iterable, value=None):
            "Emulate dict_fromkeys() in Objects/dictobject.c"
            d = klass()
            for key in iterable:
                d[key] = value
            return d
        fromkeys = classmethod(fromkeys)

Now a new dictionary of unique keys can be constructed like this:

    >>> Dict.fromkeys('abracadabra')
    {'a': None, 'r': None, 'b': None, 'c': None, 'd': None}

Using the non-data descriptor protocol, a pure Python version of *note
classmethod(): 3e5. would look like this:

    class ClassMethod(object):
         "Emulate PyClassMethod_Type() in Objects/funcobject.c"

         def __init__(self, f):
              self.f = f

         def __get__(self, obj, klass=None):
              if klass is None:
                   klass = type(obj)
              def newfunc(*args):
                   return self.f(klass, *args)
              return newfunc



File: python.info,  Node: Idioms and Anti-Idioms in Python,  Next: Functional Programming HOWTO,  Prev: Descriptor HowTo Guide,  Up: Python HOWTOs

11.5 Idioms and Anti-Idioms in Python
=====================================

     Author: Moshe Zadka

  This document is placed in the public domain.

Abstract
........

This document can be considered a companion to the tutorial. It shows
how to use Python, and even more importantly, how _not_ to use Python.

* Menu:

* Language Constructs You Should Not Use::
* Exceptions: Exceptions<8>.
* Using the Batteries::
* Using Backslash to Continue Statements::

Language Constructs You Should Not Use

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: Language Constructs You Should Not Use,  Next: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

11.5.1 Language Constructs You Should Not Use
---------------------------------------------

While Python has relatively few gotchas compared to other languages, it
still has some constructs which are only useful in corner cases, or are
plain dangerous.

* Menu:

* from module import *::
* Unadorned exec, execfile() and friends: Unadorned exec execfile and friends.
* from module import name1, name2: from module import name1 name2.
* except;: except.

from module import *

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: from module import *,  Next: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

11.5.1.1 from module import *
.............................

* Menu:

* Inside Function Definitions::
* At Module Level::
* When It Is Just Fine::


File: python.info,  Node: Inside Function Definitions,  Next: At Module Level,  Up: from module import *

11.5.1.2 Inside Function Definitions
....................................

`from module import *' is _invalid_ inside function definitions. While
many versions of Python do not check for the invalidity, it does not
make it more valid, no more than having a smart lawyer makes a man
innocent. Do not use it like that ever. Even in versions where it was
accepted, it made the function execution slower, because the compiler
could not be certain which names are local and which are global. In
Python 2.1 this construct causes warnings, and sometimes even errors.


File: python.info,  Node: At Module Level,  Next: When It Is Just Fine,  Prev: Inside Function Definitions,  Up: from module import *

11.5.1.3 At Module Level
........................

While it is valid to use `from module import *' at module level it is
usually a bad idea. For one, this loses an important property Python
otherwise has -- you can know where each toplevel name is defined by a
simple "search" function in your favourite editor. You also open
yourself to trouble in the future, if some module grows additional
functions or classes.

  One of the most awful question asked on the newsgroup is why this
code:

    f = open("www")
    f.read()

does not work. Of course, it works just fine (assuming you have a file
called "www".) But it does not work if somewhere in the module, the
statement `from os import *' is present. The *note os: 129. module has
a function called *note open(): 2cb. which returns an integer. While it
is very useful, shadowing a builtin is one of its least useful
properties.

  Remember, you can never know for sure what names a module exports, so
either take what you need -- `from module import name1, name2', or keep
them in the module and access on a per-need basis --  `import
module;print module.name'.


File: python.info,  Node: When It Is Just Fine,  Prev: At Module Level,  Up: from module import *

11.5.1.4 When It Is Just Fine
.............................

There are situations in which `from module import *' is just fine:

   * The interactive prompt. For example, `from math import *' makes
     Python an amazing scientific calculator.

   * When extending a module in C with a module in Python.

   * When the module advertises itself as `from import *' safe.


File: python.info,  Node: Unadorned exec execfile and friends,  Next: from module import name1 name2,  Prev: from module import *,  Up: Language Constructs You Should Not Use

11.5.1.5 Unadorned `exec', `execfile()' and friends
...................................................

The word "unadorned" refers to the use without an explicit dictionary,
in which case those constructs evaluate code in the _current_
environment. This is dangerous for the same reasons `from import *' is
dangerous -- it might step over variables you are counting on and mess
up things for the rest of your code.  Simply do not do that.

  Bad examples:

    >>> for name in sys.argv[1:]:
    >>>     exec "%s=1" % name
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         exec "s.%s=val" % var  # invalid!
    >>> execfile("handler.py")
    >>> handle()

Good examples:

    >>> d = {}
    >>> for name in sys.argv[1:]:
    >>>     d[name] = 1
    >>> def func(s, **kw):
    >>>     for var, val in kw.items():
    >>>         setattr(s, var, val)
    >>> d={}
    >>> execfile("handle.py", d, d)
    >>> handle = d['handle']
    >>> handle()



File: python.info,  Node: from module import name1 name2,  Next: except,  Prev: Unadorned exec execfile and friends,  Up: Language Constructs You Should Not Use

11.5.1.6 from module import name1, name2
........................................

This is a "don't" which is much weaker than the previous "don't"s but
is still something you should not do if you don't have good reasons to
do that. The reason it is usually bad idea is because you suddenly have
an object which lives in two separate namespaces. When the binding in
one namespace changes, the binding in the other will not, so there will
be a discrepancy between them. This happens when, for example, one
module is reloaded, or changes the definition of a function at runtime.

  Bad example:

    # foo.py
    a = 1

    # bar.py
    from foo import a
    if something():
        a = 2 # danger: foo.a != a

Good example:

    # foo.py
    a = 1

    # bar.py
    import foo
    if something():
        foo.a = 2



File: python.info,  Node: except,  Prev: from module import name1 name2,  Up: Language Constructs You Should Not Use

11.5.1.7 except:
................

Python has the `except:' clause, which catches all exceptions. Since
_every_ error in Python raises an exception, using `except:' can make
many programming errors look like runtime problems, which hinders the
debugging process.

  The following code shows a great example of why this is bad:

    try:
        foo = opne("file") # misspelled "open"
    except:
        sys.exit("could not open file!")

The second line triggers a *note NameError: 392, which is caught by the
except clause. The program will exit, and the error message the program
prints will make you think the problem is the readability of `"file"'
when in fact the real error has nothing to do with `"file"'.

  A better way to write the above is

    try:
        foo = opne("file")
    except IOError:
        sys.exit("could not open file")

When this is run, Python will produce a traceback showing the *note
NameError: 392, and it will be immediately apparent what needs to be
fixed.

  Because `except:' catches _all_ exceptions, including *note
SystemExit: 321, *note KeyboardInterrupt: 24e, and *note GeneratorExit:
326. (which is not an error and should not normally be caught by user
code), using a bare `except:' is almost never a good idea.  In
situations where you need to catch all "normal" errors, such as in a
framework that runs callbacks, you can catch the base class for all
normal exceptions, *note Exception: 328.  Unfortunately in Python 2.x
it is possible for third-party code to raise exceptions that do not
inherit from *note Exception: 328, so in Python 2.x there are some
cases where you may have to use a bare `except:' and manually re-raise
the exceptions you don't want to catch.


File: python.info,  Node: Exceptions<8>,  Next: Using the Batteries,  Prev: Language Constructs You Should Not Use,  Up: Idioms and Anti-Idioms in Python

11.5.2 Exceptions
-----------------

Exceptions are a useful feature of Python. You should learn to raise
them whenever something unexpected occurs, and catch them only where
you can do something about them.

  The following is a very popular anti-idiom

    def get_status(file):
        if not os.path.exists(file):
            print "file not found"
            sys.exit(1)
        return open(file).readline()

Consider the case where the file gets deleted between the time the call
to *note os.path.exists(): db5. is made and the time *note open(): 2cb.
is called. In that case the last line will raise an *note IOError: 1f7.
The same thing would happen if _file_ exists but has no read
permission.  Since testing this on a normal machine on existent and
non-existent files makes it seem bugless, the test results will seem
fine, and the code will get shipped.  Later an unhandled *note IOError:
1f7. (or perhaps some other *note EnvironmentError: 917.) escapes to the
user, who gets to watch the ugly traceback.

  Here is a somewhat better way to do it.

    def get_status(file):
        try:
            return open(file).readline()
        except EnvironmentError as err:
            print "Unable to open file: {}".format(err)
            sys.exit(1)

In this version, _either_ the file gets opened and the line is read (so
it works even on flaky NFS or SMB connections), or an error message is
printed that provides all the available information on why the open
failed, and the application is aborted.

  However, even this version of `get_status()' makes too many
assumptions -- that it will only be used in a short running script, and
not, say, in a long running server. Sure, the caller could do something
like

    try:
        status = get_status(log)
    except SystemExit:
        status = None

But there is a better way.  You should try to use as few `except'
clauses in your code as you can -- the ones you do use will usually be
inside calls which should always succeed, or a catch-all in a main
function.

  So, an even better version of `get_status()' is probably

    def get_status(file):
        return open(file).readline()

The caller can deal with the exception if it wants (for example, if it
tries several files in a loop), or just let the exception filter
upwards to _its_ caller.

  But the last version still has a serious problem -- due to
implementation details in CPython, the file would not be closed when an
exception is raised until the exception handler finishes; and, worse,
in other implementations (e.g., Jython) it might not be closed at all
regardless of whether or not an exception is raised.

  The best version of this function uses the `open()' call as a context
manager, which will ensure that the file gets closed as soon as the
function returns:

    def get_status(file):
        with open(file) as fp:
            return fp.readline()



File: python.info,  Node: Using the Batteries,  Next: Using Backslash to Continue Statements,  Prev: Exceptions<8>,  Up: Idioms and Anti-Idioms in Python

11.5.3 Using the Batteries
--------------------------

Every so often, people seem to be writing stuff in the Python library
again, usually poorly. While the occasional module has a poor
interface, it is usually much better to use the rich standard library
and data types that come with Python than inventing your own.

  A useful module very few people know about is *note os.path: 12a. It
always has the correct path arithmetic for your operating system, and
will usually be much better than whatever you come up with yourself.

  Compare:

    # ugh!
    return dir+"/"+file
    # better
    return os.path.join(dir, file)

More useful functions in *note os.path: 12a.: `basename()',
`dirname()' and `splitext()'.

  There are also many useful built-in functions people seem not to be
aware of for some reason: *note min(): 221. and *note max(): 222. can
find the minimum/maximum of any sequence with comparable semantics, for
example, yet many people write their own *note max(): 222./*note min():
221. Another highly useful function is *note reduce(): 2d9. which can
be used to repeatly apply a binary operation to a sequence, reducing it
to a single value.  For example, compute a factorial with a series of
multiply operations:

    >>> n = 4
    >>> import operator
    >>> reduce(operator.mul, range(1, n+1))
    24

When it comes to parsing numbers, note that *note float(): 1e8, *note
int(): 1ef. and *note long(): 1f0. all accept string arguments and will
reject ill-formed strings by raising an *note ValueError: 233.


File: python.info,  Node: Using Backslash to Continue Statements,  Prev: Using the Batteries,  Up: Idioms and Anti-Idioms in Python

11.5.4 Using Backslash to Continue Statements
---------------------------------------------

Since Python treats a newline as a statement terminator, and since
statements are often more than is comfortable to put in one line, many
people do:

    if foo.bar()['first'][0] == baz.quux(1, 2)[5:9] and \
       calculate_number(10, 20) != forbulate(500, 360):
          pass

You should realize that this is dangerous: a stray space after the `\'
would make this line wrong, and stray spaces are notoriously hard to
see in editors.  In this case, at least it would be a syntax error, but
if the code was:

    value = foo.bar()['first'][0]*baz.quux(1, 2)[5:9] \
            + calculate_number(10, 20)*forbulate(500, 360)

then it would just be subtly wrong.

  It is usually much better to use the implicit continuation inside
parenthesis:

  This version is bulletproof:

    value = (foo.bar()['first'][0]*baz.quux(1, 2)[5:9]
            + calculate_number(10, 20)*forbulate(500, 360))



File: python.info,  Node: Functional Programming HOWTO,  Next: Logging HOWTO,  Prev: Idioms and Anti-Idioms in Python,  Up: Python HOWTOs

11.6 Functional Programming HOWTO
=================================

     Author: A. M. Kuchling

     Release: 0.31

  In this document, we'll take a tour of Python's features suitable for
implementing programs in a functional style.  After an introduction to
the concepts of functional programming, we'll look at language features
such as *note iterator: 84a.s and *note generator: 5bb.s and relevant
library modules such as *note itertools: fb. and *note functools: da.

* Menu:

* Introduction: Introduction<13>.
* Iterators: Iterators<2>.
* Generator expressions and list comprehensions::
* Generators: Generators<2>.
* Built-in functions::
* Small functions and the lambda expression::
* The itertools module::
* The functools module::
* Revision History and Acknowledgements::
* References::

Introduction

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::

Iterators

* Data Types That Support Iterators::

Generators

* Passing values into a generator::

The itertools module

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::

The functools module

* The operator module::
* The functional module::

References

* General::
* Python-specific::
* Python documentation::


File: python.info,  Node: Introduction<13>,  Next: Iterators<2>,  Up: Functional Programming HOWTO

11.6.1 Introduction
-------------------

This section explains the basic concept of functional programming; if
you're just interested in learning about Python language features, skip
to the next section.

  Programming languages support decomposing problems in several
different ways:

   * Most programming languages are *procedural*: programs are lists of
     instructions that tell the computer what to do with the program's
     input.  C, Pascal, and even Unix shells are procedural languages.

   * In *declarative* languages, you write a specification that
     describes the problem to be solved, and the language
     implementation figures out how to perform the computation
     efficiently.  SQL is the declarative language you're most likely
     to be familiar with; a SQL query describes the data set you want
     to retrieve, and the SQL engine decides whether to scan tables or
     use indexes, which subclauses should be performed first, etc.

   * *Object-oriented* programs manipulate collections of objects.
     Objects have internal state and support methods that query or
     modify this internal state in some way. Smalltalk and Java are
     object-oriented languages.  C++ and Python are languages that
     support object-oriented programming, but don't force the use of
     object-oriented features.

   * *Functional* programming decomposes a problem into a set of
     functions.  Ideally, functions only take inputs and produce
     outputs, and don't have any internal state that affects the output
     produced for a given input.  Well-known functional languages
     include the ML family (Standard ML, OCaml, and other variants) and
     Haskell.

  The designers of some computer languages choose to emphasize one
particular approach to programming.  This often makes it difficult to
write programs that use a different approach.  Other languages are
multi-paradigm languages that support several different approaches.
Lisp, C++, and Python are multi-paradigm; you can write programs or
libraries that are largely procedural, object-oriented, or functional
in all of these languages.  In a large program, different sections
might be written using different approaches; the GUI might be
object-oriented while the processing logic is procedural or functional,
for example.

  In a functional program, input flows through a set of functions. Each
function operates on its input and produces some output.  Functional
style discourages functions with side effects that modify internal
state or make other changes that aren't visible in the function's
return value.  Functions that have no side effects at all are called
*purely functional*.  Avoiding side effects means not using data
structures that get updated as a program runs; every function's output
must only depend on its input.

  Some languages are very strict about purity and don't even have
assignment statements such as `a=3' or `c = a + b', but it's difficult
to avoid all side effects.  Printing to the screen or writing to a disk
file are side effects, for example.  For example, in Python a `print'
statement or a `time.sleep(1)' both return no useful value; they're
only called for their side effects of sending some text to the screen
or pausing execution for a second.

  Python programs written in functional style usually won't go to the
extreme of avoiding all I/O or all assignments; instead, they'll
provide a functional-appearing interface but will use non-functional
features internally.  For example, the implementation of a function
will still use assignments to local variables, but won't modify global
variables or have other side effects.

  Functional programming can be considered the opposite of
object-oriented programming.  Objects are little capsules containing
some internal state along with a collection of method calls that let
you modify this state, and programs consist of making the right set of
state changes.  Functional programming wants to avoid state changes as
much as possible and works with data flowing between functions.  In
Python you might combine the two approaches by writing functions that
take and return instances representing objects in your application
(e-mail messages, transactions, etc.).

  Functional design may seem like an odd constraint to work under.  Why
should you avoid objects and side effects?  There are theoretical and
practical advantages to the functional style:

   * Formal provability.

   * Modularity.

   * Composability.

   * Ease of debugging and testing.

* Menu:

* Formal provability::
* Modularity::
* Ease of debugging and testing::
* Composability::


File: python.info,  Node: Formal provability,  Next: Modularity,  Up: Introduction<13>

11.6.1.1 Formal provability
...........................

A theoretical benefit is that it's easier to construct a mathematical
proof that a functional program is correct.

  For a long time researchers have been interested in finding ways to
mathematically prove programs correct.  This is different from testing
a program on numerous inputs and concluding that its output is usually
correct, or reading a program's source code and concluding that the
code looks right; the goal is instead a rigorous proof that a program
produces the right result for all possible inputs.

  The technique used to prove programs correct is to write down
*invariants*, properties of the input data and of the program's
variables that are always true.  For each line of code, you then show
that if invariants X and Y are true *before* the line is executed, the
slightly different invariants X' and Y' are true *after* the line is
executed.  This continues until you reach the end of the program, at
which point the invariants should match the desired conditions on the
program's output.

  Functional programming's avoidance of assignments arose because
assignments are difficult to handle with this technique; assignments
can break invariants that were true before the assignment without
producing any new invariants that can be propagated onward.

  Unfortunately, proving programs correct is largely impractical and
not relevant to Python software. Even trivial programs require proofs
that are several pages long; the proof of correctness for a moderately
complicated program would be enormous, and few or none of the programs
you use daily (the Python interpreter, your XML parser, your web
browser) could be proven correct.  Even if you wrote down or generated
a proof, there would then be the question of verifying the proof; maybe
there's an error in it, and you wrongly believe you've proved the
program correct.


File: python.info,  Node: Modularity,  Next: Ease of debugging and testing,  Prev: Formal provability,  Up: Introduction<13>

11.6.1.2 Modularity
...................

A more practical benefit of functional programming is that it forces
you to break apart your problem into small pieces.  Programs are more
modular as a result.  It's easier to specify and write a small function
that does one thing than a large function that performs a complicated
transformation.  Small functions are also easier to read and to check
for errors.


File: python.info,  Node: Ease of debugging and testing,  Next: Composability,  Prev: Modularity,  Up: Introduction<13>

11.6.1.3 Ease of debugging and testing
......................................

Testing and debugging a functional-style program is easier.

  Debugging is simplified because functions are generally small and
clearly specified.  When a program doesn't work, each function is an
interface point where you can check that the data are correct.  You can
look at the intermediate inputs and outputs to quickly isolate the
function that's responsible for a bug.

  Testing is easier because each function is a potential subject for a
unit test.  Functions don't depend on system state that needs to be
replicated before running a test; instead you only have to synthesize
the right input and then check that the output matches expectations.


File: python.info,  Node: Composability,  Prev: Ease of debugging and testing,  Up: Introduction<13>

11.6.1.4 Composability
......................

As you work on a functional-style program, you'll write a number of
functions with varying inputs and outputs.  Some of these functions
will be unavoidably specialized to a particular application, but others
will be useful in a wide variety of programs.  For example, a function
that takes a directory path and returns all the XML files in the
directory, or a function that takes a filename and returns its
contents, can be applied to many different situations.

  Over time you'll form a personal library of utilities.  Often you'll
assemble new programs by arranging existing functions in a new
configuration and writing a few functions specialized for the current
task.


File: python.info,  Node: Iterators<2>,  Next: Generator expressions and list comprehensions,  Prev: Introduction<13>,  Up: Functional Programming HOWTO

11.6.2 Iterators
----------------

I'll start by looking at a Python language feature that's an important
foundation for writing functional-style programs: iterators.

  An iterator is an object representing a stream of data; this object
returns the data one element at a time.  A Python iterator must support
a method called `next()' that takes no arguments and always returns the
next element of the stream.  If there are no more elements in the
stream, `next()' must raise the `StopIteration' exception.  Iterators
don't have to be finite, though; it's perfectly reasonable to write an
iterator that produces an infinite stream of data.

  The built-in *note iter(): 2a8. function takes an arbitrary object
and tries to return an iterator that will return the object's contents
or elements, raising *note TypeError: 215. if the object doesn't
support iteration.  Several of Python's built-in data types support
iteration, the most common being lists and dictionaries.  An object is
called an *iterable* object if you can get an iterator for it.

  You can experiment with the iteration interface manually:

    >>> L = [1,2,3]
    >>> it = iter(L)
    >>> print it
    <...iterator object at ...>
    >>> it.next()
    1
    >>> it.next()
    2
    >>> it.next()
    3
    >>> it.next()
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    StopIteration
    >>>

Python expects iterable objects in several different contexts, the most
important being the `for' statement.  In the statement `for X in Y', Y
must be an iterator or some object for which `iter()' can create an
iterator.  These two statements are equivalent:

    for i in iter(obj):
        print i

    for i in obj:
        print i

Iterators can be materialized as lists or tuples by using the *note
list(): 3ab. or *note tuple(): 3f7. constructor functions:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> t = tuple(iterator)
    >>> t
    (1, 2, 3)

Sequence unpacking also supports iterators: if you know an iterator
will return N elements, you can unpack them into an N-tuple:

    >>> L = [1,2,3]
    >>> iterator = iter(L)
    >>> a,b,c = iterator
    >>> a,b,c
    (1, 2, 3)

Built-in functions such as *note max(): 222. and *note min(): 221. can
take a single iterator argument and will return the largest or smallest
element.  The `"in"' and `"not in"' operators also support iterators:
`X in iterator' is true if X is found in the stream returned by the
iterator.  You'll run into obvious problems if the iterator is
infinite; `max()', `min()', and `"not in"' will never return, and if
the element X never appears in the stream, the `"in"' operator won't
return either.

  Note that you can only go forward in an iterator; there's no way to
get the previous element, reset the iterator, or make a copy of it.
Iterator objects can optionally provide these additional capabilities,
but the iterator protocol only specifies the `next()' method.
Functions may therefore consume all of the iterator's output, and if
you need to do something different with the same stream, you'll have to
create a new iterator.

* Menu:

* Data Types That Support Iterators::


File: python.info,  Node: Data Types That Support Iterators,  Up: Iterators<2>

11.6.2.1 Data Types That Support Iterators
..........................................

We've already seen how lists and tuples support iterators.  In fact,
any Python sequence type, such as strings, will automatically support
creation of an iterator.

  Calling *note iter(): 2a8. on a dictionary returns an iterator that
will loop over the dictionary's keys:

    >>> m = {'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,
    ...      'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12}
    >>> for key in m:
    ...     print key, m[key]
    Mar 3
    Feb 2
    Aug 8
    Sep 9
    Apr 4
    Jun 6
    Jul 7
    Jan 1
    May 5
    Nov 11
    Dec 12
    Oct 10

Note that the order is essentially random, because it's based on the
hash ordering of the objects in the dictionary.

  Applying `iter()' to a dictionary always loops over the keys, but
dictionaries have methods that return other iterators.  If you want to
iterate over keys, values, or key/value pairs, you can explicitly call
the `iterkeys()', `itervalues()', or `iteritems()' methods to get an
appropriate iterator.

  The *note dict(): 2f6. constructor can accept an iterator that
returns a finite stream of `(key, value)' tuples:

    >>> L = [('Italy', 'Rome'), ('France', 'Paris'), ('US', 'Washington DC')]
    >>> dict(iter(L))
    {'Italy': 'Rome', 'US': 'Washington DC', 'France': 'Paris'}

Files also support iteration by calling the `readline()' method until
there are no more lines in the file.  This means you can read each line
of a file like this:

    for line in file:
        # do something for each line
        ...

Sets can take their contents from an iterable and let you iterate over
the set's elements:

    S = set((2, 3, 5, 7, 11, 13))
    for i in S:
        print i



File: python.info,  Node: Generator expressions and list comprehensions,  Next: Generators<2>,  Prev: Iterators<2>,  Up: Functional Programming HOWTO

11.6.3 Generator expressions and list comprehensions
----------------------------------------------------

Two common operations on an iterator's output are 1) performing some
operation for every element, 2) selecting a subset of elements that
meet some condition.  For example, given a list of strings, you might
want to strip off trailing whitespace from each line or extract all the
strings containing a given substring.

  List comprehensions and generator expressions (short form:
"listcomps" and "genexps") are a concise notation for such operations,
borrowed from the functional programming language Haskell
(<http://www.haskell.org/>).  You can strip all the whitespace from a
stream of strings with the following code:

    line_list = ['  line 1\n', 'line 2  \n', ...]

    # Generator expression -- returns iterator
    stripped_iter = (line.strip() for line in line_list)

    # List comprehension -- returns list
    stripped_list = [line.strip() for line in line_list]

You can select only certain elements by adding an `"if"' condition:

    stripped_list = [line.strip() for line in line_list
                     if line != ""]

With a list comprehension, you get back a Python list; `stripped_list'
is a list containing the resulting lines, not an iterator.  Generator
expressions return an iterator that computes the values as necessary,
not needing to materialize all the values at once.  This means that
list comprehensions aren't useful if you're working with iterators that
return an infinite stream or a very large amount of data.  Generator
expressions are preferable in these situations.

  Generator expressions are surrounded by parentheses ("()") and list
comprehensions are surrounded by square brackets ("[]").  Generator
expressions have the form:

    ( expression for expr in sequence1
                 if condition1
                 for expr2 in sequence2
                 if condition2
                 for expr3 in sequence3 ...
                 if condition3
                 for exprN in sequenceN
                 if conditionN )

Again, for a list comprehension only the outside brackets are different
(square brackets instead of parentheses).

  The elements of the generated output will be the successive values of
`expression'.  The `if' clauses are all optional; if present,
`expression' is only evaluated and added to the result when `condition'
is true.

  Generator expressions always have to be written inside parentheses,
but the parentheses signalling a function call also count.  If you want
to create an iterator that will be immediately passed to a function you
can write:

    obj_total = sum(obj.count for obj in list_all_objects())

The `for...in' clauses contain the sequences to be iterated over.  The
sequences do not have to be the same length, because they are iterated
over from left to right, *not* in parallel.  For each element in
`sequence1', `sequence2' is looped over from the beginning.
`sequence3' is then looped over for each resulting pair of elements
from `sequence1' and `sequence2'.

  To put it another way, a list comprehension or generator expression is
equivalent to the following Python code:

    for expr1 in sequence1:
        if not (condition1):
            continue   # Skip this element
        for expr2 in sequence2:
            if not (condition2):
                continue    # Skip this element
            ...
            for exprN in sequenceN:
                 if not (conditionN):
                     continue   # Skip this element

                 # Output the value of
                 # the expression.

This means that when there are multiple `for...in' clauses but no `if'
clauses, the length of the resulting output will be equal to the
product of the lengths of all the sequences.  If you have two lists of
length 3, the output list is 9 elements long:

    >>> seq1 = 'abc'
    >>> seq2 = (1,2,3)
    >>> [(x,y) for x in seq1 for y in seq2]
    [('a', 1), ('a', 2), ('a', 3),
     ('b', 1), ('b', 2), ('b', 3),
     ('c', 1), ('c', 2), ('c', 3)]

To avoid introducing an ambiguity into Python's grammar, if
`expression' is creating a tuple, it must be surrounded with
parentheses.  The first list comprehension below is a syntax error,
while the second one is correct:

    # Syntax error
    [ x,y for x in seq1 for y in seq2]
    # Correct
    [ (x,y) for x in seq1 for y in seq2]



File: python.info,  Node: Generators<2>,  Next: Built-in functions,  Prev: Generator expressions and list comprehensions,  Up: Functional Programming HOWTO

11.6.4 Generators
-----------------

Generators are a special class of functions that simplify the task of
writing iterators.  Regular functions compute a value and return it,
but generators return an iterator that returns a stream of values.

  You're doubtless familiar with how regular function calls work in
Python or C.  When you call a function, it gets a private namespace
where its local variables are created.  When the function reaches a
`return' statement, the local variables are destroyed and the value is
returned to the caller.  A later call to the same function creates a
new private namespace and a fresh set of local variables. But, what if
the local variables weren't thrown away on exiting a function?  What if
you could later resume the function where it left off?  This is what
generators provide; they can be thought of as resumable functions.

  Here's the simplest example of a generator function:

    def generate_ints(N):
        for i in range(N):
            yield i

Any function containing a `yield' keyword is a generator function; this
is detected by Python's *note bytecode: 567. compiler which compiles
the function specially as a result.

  When you call a generator function, it doesn't return a single value;
instead it returns a generator object that supports the iterator
protocol.  On executing the `yield' expression, the generator outputs
the value of `i', similar to a `return' statement.  The big difference
between `yield' and a `return' statement is that on reaching a `yield'
the generator's state of execution is suspended and local variables are
preserved.  On the next call to the generator's `.next()' method, the
function will resume executing.

  Here's a sample usage of the `generate_ints()' generator:

    >>> gen = generate_ints(3)
    >>> gen
    <generator object generate_ints at ...>
    >>> gen.next()
    0
    >>> gen.next()
    1
    >>> gen.next()
    2
    >>> gen.next()
    Traceback (most recent call last):
      File "stdin", line 1, in ?
      File "stdin", line 2, in generate_ints
    StopIteration

You could equally write `for i in generate_ints(5)', or `a,b,c =
generate_ints(3)'.

  Inside a generator function, the `return' statement can only be used
without a value, and signals the end of the procession of values; after
executing a `return' the generator cannot return any further values.
`return' with a value, such as `return 5', is a syntax error inside a
generator function.  The end of the generator's results can also be
indicated by raising `StopIteration' manually, or by just letting the
flow of execution fall off the bottom of the function.

  You could achieve the effect of generators manually by writing your
own class and storing all the local variables of the generator as
instance variables.  For example, returning a list of integers could be
done by setting `self.count' to 0, and having the `next()' method
increment `self.count' and return it.  However, for a moderately
complicated generator, writing a corresponding class can be much
messier.

  The test suite included with Python's library, `test_generators.py',
contains a number of more interesting examples.  Here's one generator
that implements an in-order traversal of a tree using generators
recursively.

    # A recursive generator that generates Tree leaves in in-order.
    def inorder(t):
        if t:
            for x in inorder(t.left):
                yield x

            yield t.label

            for x in inorder(t.right):
                yield x

Two other examples in `test_generators.py' produce solutions for the
N-Queens problem (placing N queens on an NxN chess board so that no
queen threatens another) and the Knight's Tour (finding a route that
takes a knight to every square of an NxN chessboard without visiting
any square twice).

* Menu:

* Passing values into a generator::


File: python.info,  Node: Passing values into a generator,  Up: Generators<2>

11.6.4.1 Passing values into a generator
........................................

In Python 2.4 and earlier, generators only produced output.  Once a
generator's code was invoked to create an iterator, there was no way to
pass any new information into the function when its execution is
resumed.  You could hack together this ability by making the generator
look at a global variable or by passing in some mutable object that
callers then modify, but these approaches are messy.

  In Python 2.5 there's a simple way to pass values into a generator.
*note yield: 2e8. became an expression, returning a value that can be
assigned to a variable or otherwise operated on:

    val = (yield i)

I recommend that you *always* put parentheses around a `yield'
expression when you're doing something with the returned value, as in
the above example.  The parentheses aren't always necessary, but it's
easier to always add them instead of having to remember when they're
needed.

  (PEP 342 explains the exact rules, which are that a
`yield'-expression must always be parenthesized except when it occurs
at the top-level expression on the right-hand side of an assignment.
This means you can write `val = yield i' but have to use parentheses
when there's an operation, as in `val = (yield i) + 12'.)

  Values are sent into a generator by calling its `send(value)' method.
This method resumes the generator's code and the `yield' expression
returns the specified value.  If the regular `next()' method is called,
the `yield' returns `None'.

  Here's a simple counter that increments by 1 and allows changing the
value of the internal counter.

    def counter (maximum):
        i = 0
        while i < maximum:
            val = (yield i)
            # If value provided, change counter
            if val is not None:
                i = val
            else:
                i += 1

And here's an example of changing the counter:

    >>> it = counter(10)
    >>> print it.next()
    0
    >>> print it.next()
    1
    >>> print it.send(8)
    8
    >>> print it.next()
    9
    >>> print it.next()
    Traceback (most recent call last):
      File "t.py", line 15, in ?
        print it.next()
    StopIteration

Because `yield' will often be returning `None', you should always check
for this case.  Don't just use its value in expressions unless you're
sure that the `send()' method will be the only method used resume your
generator function.

  In addition to `send()', there are two other new methods on
generators:

   * `throw(type, value=None, traceback=None)' is used to raise an
     exception inside the generator; the exception is raised by the
     `yield' expression where the generator's execution is paused.

   * `close()' raises a *note GeneratorExit: 326. exception inside the
     generator to terminate the iteration.  On receiving this
     exception, the generator's code must either raise *note
     GeneratorExit: 326. or *note StopIteration: 322.; catching the
     exception and doing anything else is illegal and will trigger a
     *note RuntimeError: 38a.  `close()' will also be called by
     Python's garbage collector when the generator is garbage-collected.

     If you need to run cleanup code when a *note GeneratorExit: 326.
     occurs, I suggest using a `try: ... finally:' suite instead of
     catching *note GeneratorExit: 326.

  The cumulative effect of these changes is to turn generators from
one-way producers of information into both producers and consumers.

  Generators also become *coroutines*, a more generalized form of
subroutines.  Subroutines are entered at one point and exited at
another point (the top of the function, and a `return' statement), but
coroutines can be entered, exited, and resumed at many different points
(the `yield' statements).


File: python.info,  Node: Built-in functions,  Next: Small functions and the lambda expression,  Prev: Generators<2>,  Up: Functional Programming HOWTO

11.6.5 Built-in functions
-------------------------

Let's look in more detail at built-in functions often used with
iterators.

  Two of Python's built-in functions, *note map(): 2f5. and *note
filter(): 3f8, are somewhat obsolete; they duplicate the features of
list comprehensions but return actual lists instead of iterators.

  `map(f, iterA, iterB, ...)' returns a list containing `f(iterA[0],
iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]), ...'.

    >>> def upper(s):
    ...     return s.upper()


    >>> map(upper, ['sentence', 'fragment'])
    ['SENTENCE', 'FRAGMENT']


    >>> [upper(s) for s in ['sentence', 'fragment']]
    ['SENTENCE', 'FRAGMENT']

As shown above, you can achieve the same effect with a list
comprehension.  The *note itertools.imap(): d1f. function does the same
thing but can handle infinite iterators; it'll be discussed later, in
the section on the *note itertools: fb. module.

  `filter(predicate, iter)' returns a list that contains all the
sequence elements that meet a certain condition, and is similarly
duplicated by list comprehensions.  A *predicate* is a function that
returns the truth value of some condition; for use with *note filter():
3f8, the predicate must take a single value.

    >>> def is_even(x):
    ...     return (x % 2) == 0


    >>> filter(is_even, range(10))
    [0, 2, 4, 6, 8]

This can also be written as a list comprehension:

    >>> [x for x in range(10) if is_even(x)]
    [0, 2, 4, 6, 8]

*note filter(): 3f8. also has a counterpart in the *note itertools: fb.
module, *note itertools.ifilter(): 84c, that returns an iterator and
can therefore handle infinite sequences just as *note itertools.imap():
d1f. can.

  `reduce(func, iter, [initial_value])' doesn't have a counterpart in
the *note itertools: fb. module because it cumulatively performs an
operation on all the iterable's elements and therefore can't be applied
to infinite iterables.  `func' must be a function that takes two
elements and returns a single value.  *note reduce(): 2d9. takes the
first two elements A and B returned by the iterator and calculates
`func(A, B)'.  It then requests the third element, C, calculates
`func(func(A, B), C)', combines this result with the fourth element
returned, and continues until the iterable is exhausted.  If the
iterable returns no values at all, a *note TypeError: 215. exception is
raised.  If the initial value is supplied, it's used as a starting
point and `func(initial_value, A)' is the first calculation.

    >>> import operator
    >>> reduce(operator.concat, ['A', 'BB', 'C'])
    'ABBC'
    >>> reduce(operator.concat, [])
    Traceback (most recent call last):
      ...
    TypeError: reduce() of empty sequence with no initial value
    >>> reduce(operator.mul, [1,2,3], 1)
    6
    >>> reduce(operator.mul, [], 1)
    1

If you use *note operator.add(): d4d. with *note reduce(): 2d9, you'll
add up all the elements of the iterable.  This case is so common that
there's a special built-in called *note sum(): 415. to compute it:

    >>> reduce(operator.add, [1,2,3,4], 0)
    10
    >>> sum([1,2,3,4])
    10
    >>> sum([])
    0

For many uses of *note reduce(): 2d9, though, it can be clearer to just
write the obvious *note for: 2e1. loop:

    # Instead of:
    product = reduce(operator.mul, [1,2,3], 1)

    # You can write:
    product = 1
    for i in [1,2,3]:
        product *= i

`enumerate(iter)' counts off the elements in the iterable, returning
2-tuples containing the count and each element.

    >>> for item in enumerate(['subject', 'verb', 'object']):
    ...     print item
    (0, 'subject')
    (1, 'verb')
    (2, 'object')

*note enumerate(): 416. is often used when looping through a list and
recording the indexes at which certain conditions are met:

    f = open('data.txt', 'r')
    for i, line in enumerate(f):
        if line.strip() == '':
            print 'Blank line at line #%i' % i

`sorted(iterable, [cmp=None], [key=None], [reverse=False])' collects
all the elements of the iterable into a list, sorts the list, and
returns the sorted result.  The `cmp', `key', and `reverse' arguments
are passed through to the constructed list's `.sort()' method.

    >>> import random
    >>> # Generate 8 random numbers between [0, 10000)
    >>> rand_list = random.sample(range(10000), 8)
    >>> rand_list
    [769, 7953, 9828, 6431, 8442, 9878, 6213, 2207]
    >>> sorted(rand_list)
    [769, 2207, 6213, 6431, 7953, 8442, 9828, 9878]
    >>> sorted(rand_list, reverse=True)
    [9878, 9828, 8442, 7953, 6431, 6213, 2207, 769]

(For a more detailed discussion of sorting, see the Sorting mini-HOWTO
in the Python wiki at <http://wiki.python.org/moin/HowTo/Sorting>.)

  The `any(iter)' and `all(iter)' built-ins look at the truth values of
an iterable's contents.  *note any(): 39d. returns True if any element
in the iterable is a true value, and *note all(): 39e. returns True if
all of the elements are true values:

    >>> any([0,1,0])
    True
    >>> any([0,0,0])
    False
    >>> any([1,1,1])
    True
    >>> all([0,1,0])
    False
    >>> all([0,0,0])
    False
    >>> all([1,1,1])
    True



File: python.info,  Node: Small functions and the lambda expression,  Next: The itertools module,  Prev: Built-in functions,  Up: Functional Programming HOWTO

11.6.6 Small functions and the lambda expression
------------------------------------------------

When writing functional-style programs, you'll often need little
functions that act as predicates or that combine elements in some way.

  If there's a Python built-in or a module function that's suitable,
you don't need to define a new function at all:

    stripped_lines = [line.strip() for line in lines]
    existing_files = filter(os.path.exists, file_list)

If the function you need doesn't exist, you need to write it.  One way
to write small functions is to use the `lambda' statement.  `lambda'
takes a number of parameters and an expression combining these
parameters, and creates a small function that returns the value of the
expression:

    lowercase = lambda x: x.lower()

    print_assign = lambda name, value: name + '=' + str(value)

    adder = lambda x, y: x+y

An alternative is to just use the `def' statement and define a function
in the usual way:

    def lowercase(x):
        return x.lower()

    def print_assign(name, value):
        return name + '=' + str(value)

    def adder(x,y):
        return x + y

Which alternative is preferable?  That's a style question; my usual
course is to avoid using `lambda'.

  One reason for my preference is that `lambda' is quite limited in the
functions it can define.  The result has to be computable as a single
expression, which means you can't have multiway `if... elif... else'
comparisons or `try... except' statements.  If you try to do too much
in a `lambda' statement, you'll end up with an overly complicated
expression that's hard to read.  Quick, what's the following code doing?

    total = reduce(lambda a, b: (0, a[1] + b[1]), items)[1]

You can figure it out, but it takes time to disentangle the expression
to figure out what's going on.  Using a short nested `def' statements
makes things a little bit better:

    def combine (a, b):
        return 0, a[1] + b[1]

    total = reduce(combine, items)[1]

But it would be best of all if I had simply used a `for' loop:

    total = 0
    for a, b in items:
        total += b

Or the *note sum(): 415. built-in and a generator expression:

    total = sum(b for a,b in items)

Many uses of *note reduce(): 2d9. are clearer when written as `for'
loops.

  Fredrik Lundh once suggested the following set of rules for
refactoring uses of `lambda':

  1. Write a lambda function.

  2. Write a comment explaining what the heck that lambda does.

  3. Study the comment for a while, and think of a name that captures
     the essence of the comment.

  4. Convert the lambda to a def statement, using that name.

  5. Remove the comment.

  I really like these rules, but you're free to disagree about whether
this lambda-free style is better.


File: python.info,  Node: The itertools module,  Next: The functools module,  Prev: Small functions and the lambda expression,  Up: Functional Programming HOWTO

11.6.7 The itertools module
---------------------------

The *note itertools: fb. module contains a number of commonly-used
iterators as well as functions for combining several iterators.  This
section will introduce the module's contents by showing small examples.

  The module's functions fall into a few broad classes:

   * Functions that create a new iterator based on an existing iterator.

   * Functions for treating an iterator's elements as function
     arguments.

   * Functions for selecting portions of an iterator's output.

   * A function for grouping an iterator's output.

* Menu:

* Creating new iterators::
* Calling functions on elements::
* Selecting elements::
* Grouping elements::


File: python.info,  Node: Creating new iterators,  Next: Calling functions on elements,  Up: The itertools module

11.6.7.1 Creating new iterators
...............................

`itertools.count(n)' returns an infinite stream of integers, increasing
by 1 each time.  You can optionally supply the starting number, which
defaults to 0:

    itertools.count() =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...
    itertools.count(10) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

`itertools.cycle(iter)' saves a copy of the contents of a provided
iterable and returns a new iterator that returns its elements from
first to last.  The new iterator will repeat these elements infinitely.

    itertools.cycle([1,2,3,4,5]) =>
      1, 2, 3, 4, 5, 1, 2, 3, 4, 5, ...

`itertools.repeat(elem, [n])' returns the provided element `n' times, or
returns the element endlessly if `n' is not provided.

    itertools.repeat('abc') =>
      abc, abc, abc, abc, abc, abc, abc, abc, abc, abc, ...
    itertools.repeat('abc', 5) =>
      abc, abc, abc, abc, abc

`itertools.chain(iterA, iterB, ...)' takes an arbitrary number of
iterables as input, and returns all the elements of the first iterator,
then all the elements of the second, and so on, until all of the
iterables have been exhausted.

    itertools.chain(['a', 'b', 'c'], (1, 2, 3)) =>
      a, b, c, 1, 2, 3

`itertools.izip(iterA, iterB, ...)' takes one element from each
iterable and returns them in a tuple:

    itertools.izip(['a', 'b', 'c'], (1, 2, 3)) =>
      ('a', 1), ('b', 2), ('c', 3)

It's similar to the built-in *note zip(): 3f4. function, but doesn't
construct an in-memory list and exhaust all the input iterators before
returning; instead tuples are constructed and returned only if they're
requested.  (The technical term for this behaviour is lazy
evaluation(1).)

  This iterator is intended to be used with iterables that are all of
the same length.  If the iterables are of different lengths, the
resulting stream will be the same length as the shortest iterable.

    itertools.izip(['a', 'b'], (1, 2, 3)) =>
      ('a', 1), ('b', 2)

You should avoid doing this, though, because an element may be taken
from the longer iterators and discarded.  This means you can't go on to
use the iterators further because you risk skipping a discarded element.

  `itertools.islice(iter, [start], stop, [step])' returns a stream
that's a slice of the iterator.  With a single `stop' argument, it will
return the first `stop' elements.  If you supply a starting index,
you'll get `stop-start' elements, and if you supply a value for `step',
elements will be skipped accordingly.  Unlike Python's string and list
slicing, you can't use negative values for `start', `stop', or `step'.

    itertools.islice(range(10), 8) =>
      0, 1, 2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8) =>
      2, 3, 4, 5, 6, 7
    itertools.islice(range(10), 2, 8, 2) =>
      2, 4, 6

`itertools.tee(iter, [n])' replicates an iterator; it returns `n'
independent iterators that will all return the contents of the source
iterator.  If you don't supply a value for `n', the default is 2.
Replicating iterators requires saving some of the contents of the
source iterator, so this can consume significant memory if the iterator
is large and one of the new iterators is consumed more than the others.

    itertools.tee( itertools.count() ) =>
       iterA, iterB

    where iterA ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...

    and   iterB ->
       0, 1, 2, 3, 4, 5, 6, 7, 8, 9, ...


  ---------- Footnotes ----------

  (1) http://en.wikipedia.org/wiki/Lazy_evaluation


File: python.info,  Node: Calling functions on elements,  Next: Selecting elements,  Prev: Creating new iterators,  Up: The itertools module

11.6.7.2 Calling functions on elements
......................................

Two functions are used for calling other functions on the contents of an
iterable.

  `itertools.imap(f, iterA, iterB, ...)' returns a stream containing
`f(iterA[0], iterB[0]), f(iterA[1], iterB[1]), f(iterA[2], iterB[2]),
...':

    itertools.imap(operator.add, [5, 6, 5], [1, 2, 3]) =>
      6, 8, 8

The `operator' module contains a set of functions corresponding to
Python's operators.  Some examples are `operator.add(a, b)' (adds two
values), `operator.ne(a, b)' (same as `a!=b'), and
`operator.attrgetter('id')' (returns a callable that fetches the `"id"'
attribute).

  `itertools.starmap(func, iter)' assumes that the iterable will return
a stream of tuples, and calls `f()' using these tuples as the arguments:

    itertools.starmap(os.path.join,
                      [('/usr', 'bin', 'java'), ('/bin', 'python'),
                       ('/usr', 'bin', 'perl'),('/usr', 'bin', 'ruby')])
    =>
      /usr/bin/java, /bin/python, /usr/bin/perl, /usr/bin/ruby



File: python.info,  Node: Selecting elements,  Next: Grouping elements,  Prev: Calling functions on elements,  Up: The itertools module

11.6.7.3 Selecting elements
...........................

Another group of functions chooses a subset of an iterator's elements
based on a predicate.

  `itertools.ifilter(predicate, iter)' returns all the elements for
which the predicate returns true:

    def is_even(x):
        return (x % 2) == 0

    itertools.ifilter(is_even, itertools.count()) =>
      0, 2, 4, 6, 8, 10, 12, 14, ...

`itertools.ifilterfalse(predicate, iter)' is the opposite, returning all
elements for which the predicate returns false:

    itertools.ifilterfalse(is_even, itertools.count()) =>
      1, 3, 5, 7, 9, 11, 13, 15, ...

`itertools.takewhile(predicate, iter)' returns elements for as long as
the predicate returns true.  Once the predicate returns false, the
iterator will signal the end of its results.

    def less_than_10(x):
        return (x < 10)

    itertools.takewhile(less_than_10, itertools.count()) =>
      0, 1, 2, 3, 4, 5, 6, 7, 8, 9

    itertools.takewhile(is_even, itertools.count()) =>
      0

`itertools.dropwhile(predicate, iter)' discards elements while the
predicate returns true, and then returns the rest of the iterable's
results.

    itertools.dropwhile(less_than_10, itertools.count()) =>
      10, 11, 12, 13, 14, 15, 16, 17, 18, 19, ...

    itertools.dropwhile(is_even, itertools.count()) =>
      1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ...



File: python.info,  Node: Grouping elements,  Prev: Selecting elements,  Up: The itertools module

11.6.7.4 Grouping elements
..........................

The last function I'll discuss, `itertools.groupby(iter,
key_func=None)', is the most complicated.  `key_func(elem)' is a
function that can compute a key value for each element returned by the
iterable.  If you don't supply a key function, the key is simply each
element itself.

  `groupby()' collects all the consecutive elements from the underlying
iterable that have the same key value, and returns a stream of 2-tuples
containing a key value and an iterator for the elements with that key.

    city_list = [('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL'),
                 ('Anchorage', 'AK'), ('Nome', 'AK'),
                 ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ'),
                 ...
                ]

    def get_state ((city, state)):
        return state

    itertools.groupby(city_list, get_state) =>
      ('AL', iterator-1),
      ('AK', iterator-2),
      ('AZ', iterator-3), ...

    where
    iterator-1 =>
      ('Decatur', 'AL'), ('Huntsville', 'AL'), ('Selma', 'AL')
    iterator-2 =>
      ('Anchorage', 'AK'), ('Nome', 'AK')
    iterator-3 =>
      ('Flagstaff', 'AZ'), ('Phoenix', 'AZ'), ('Tucson', 'AZ')

`groupby()' assumes that the underlying iterable's contents will
already be sorted based on the key.  Note that the returned iterators
also use the underlying iterable, so you have to consume the results of
iterator-1 before requesting iterator-2 and its corresponding key.


File: python.info,  Node: The functools module,  Next: Revision History and Acknowledgements,  Prev: The itertools module,  Up: Functional Programming HOWTO

11.6.8 The functools module
---------------------------

The *note functools: da. module in Python 2.5 contains some
higher-order functions.  A *higher-order function* takes one or more
functions as input and returns a new function.  The most useful tool in
this module is the *note functools.partial(): d30. function.

  For programs written in a functional style, you'll sometimes want to
construct variants of existing functions that have some of the
parameters filled in.  Consider a Python function `f(a, b, c)'; you may
wish to create a new function `g(b, c)' that's equivalent to `f(1, b,
c)'; you're filling in a value for one of `f()''s parameters.  This is
called "partial function application".

  The constructor for `partial' takes the arguments `(function, arg1,
arg2, ... kwarg1=value1, kwarg2=value2)'.  The resulting object is
callable, so you can just call it to invoke `function' with the
filled-in arguments.

  Here's a small but realistic example:

    import functools

    def log (message, subsystem):
        "Write the contents of 'message' to the specified subsystem."
        print '%s: %s' % (subsystem, message)
        ...

    server_log = functools.partial(log, subsystem='server')
    server_log('Unable to open socket')


* Menu:

* The operator module::
* The functional module::


File: python.info,  Node: The operator module,  Next: The functional module,  Up: The functools module

11.6.8.1 The operator module
............................

The *note operator: 127. module was mentioned earlier.  It contains a
set of functions corresponding to Python's operators.  These functions
are often useful in functional-style code because they save you from
writing trivial functions that perform a single operation.

  Some of the functions in this module are:

   * Math operations: `add()', `sub()', `mul()', `div()', `floordiv()',
     `abs()', ...

   * Logical operations: `not_()', `truth()'.

   * Bitwise operations: `and_()', `or_()', `invert()'.

   * Comparisons: `eq()', `ne()', `lt()', `le()', `gt()', and `ge()'.

   * Object identity: `is_()', `is_not()'.

  Consult the operator module's documentation for a complete list.


File: python.info,  Node: The functional module,  Prev: The operator module,  Up: The functools module

11.6.8.2 The functional module
..............................

Collin Winter's functional module(1) provides a number of more advanced
tools for functional programming. It also reimplements several Python
built-ins, trying to make them more intuitive to those used to
functional programming in other languages.

  This section contains an introduction to some of the most important
functions in `functional'; full documentation can be found at the
project's website(2).

  `compose(outer, inner, unpack=False)'

  The `compose()' function implements function composition.  In other
words, it returns a wrapper around the `outer' and `inner' callables,
such that the return value from `inner' is fed directly to `outer'.
That is,

    >>> def add(a, b):
    ...     return a + b
    ...
    >>> def double(a):
    ...     return 2 * a
    ...
    >>> compose(double, add)(5, 6)
    22

is equivalent to

    >>> double(add(5, 6))
    22

The `unpack' keyword is provided to work around the fact that Python
functions are not always fully curried(3).  By default, it is expected
that the `inner' function will return a single object and that the
`outer' function will take a single argument. Setting the `unpack'
argument causes `compose' to expect a tuple from `inner' which will be
expanded before being passed to `outer'. Put simply,

    compose(f, g)(5, 6)

is equivalent to:

    f(g(5, 6))

while

    compose(f, g, unpack=True)(5, 6)

is equivalent to:

    f(*g(5, 6))

Even though `compose()' only accepts two functions, it's trivial to
build up a version that will compose any number of functions. We'll use
`reduce()', `compose()' and `partial()' (the last of which is provided
by both `functional' and `functools').

    from functional import compose, partial

    multi_compose = partial(reduce, compose)

We can also use `map()', `compose()' and `partial()' to craft a version
of `"".join(...)' that converts its arguments to string:

    from functional import compose, partial

    join = compose("".join, partial(map, str))

`flip(func)'

  `flip()' wraps the callable in `func' and causes it to receive its
non-keyword arguments in reverse order.

    >>> def triple(a, b, c):
    ...     return (a, b, c)
    ...
    >>> triple(5, 6, 7)
    (5, 6, 7)
    >>>
    >>> flipped_triple = flip(triple)
    >>> flipped_triple(5, 6, 7)
    (7, 6, 5)

`foldl(func, start, iterable)'

  `foldl()' takes a binary function, a starting value (usually some
kind of 'zero'), and an iterable.  The function is applied to the
starting value and the first element of the list, then the result of
that and the second element of the list, then the result of that and
the third element of the list, and so on.

  This means that a call such as:

    foldl(f, 0, [1, 2, 3])

is equivalent to:

    f(f(f(0, 1), 2), 3)

`foldl()' is roughly equivalent to the following recursive function:

    def foldl(func, start, seq):
        if len(seq) == 0:
            return start

        return foldl(func, func(start, seq[0]), seq[1:])

Speaking of equivalence, the above `foldl' call can be expressed in
terms of the built-in `reduce' like so:

    reduce(f, [1, 2, 3], 0)

We can use `foldl()', `operator.concat()' and `partial()' to write a
cleaner, more aesthetically-pleasing version of Python's `"".join(...)'
idiom:

    from functional import foldl, partial from operator import concat

    join = partial(foldl, concat, "")


  ---------- Footnotes ----------

  (1) http://oakwinter.com/code/functional/

  (2) http://oakwinter.com/code/functional/documentation/

  (3) http://en.wikipedia.org/wiki/Currying


File: python.info,  Node: Revision History and Acknowledgements,  Next: References,  Prev: The functools module,  Up: Functional Programming HOWTO

11.6.9 Revision History and Acknowledgements
--------------------------------------------

The author would like to thank the following people for offering
suggestions, corrections and assistance with various drafts of this
article: Ian Bicking, Nick Coghlan, Nick Efford, Raymond Hettinger, Jim
Jewett, Mike Krell, Leandro Lameiro, Jussi Salmela, Collin Winter,
Blake Winton.

  Version 0.1: posted June 30 2006.

  Version 0.11: posted July 1 2006.  Typo fixes.

  Version 0.2: posted July 10 2006.  Merged genexp and listcomp
sections into one.  Typo fixes.

  Version 0.21: Added more references suggested on the tutor mailing
list.

  Version 0.30: Adds a section on the `functional' module written by
Collin Winter; adds short section on the operator module; a few other
edits.


File: python.info,  Node: References,  Prev: Revision History and Acknowledgements,  Up: Functional Programming HOWTO

11.6.10 References
------------------

* Menu:

* General::
* Python-specific::
* Python documentation::


File: python.info,  Node: General,  Next: Python-specific,  Up: References

11.6.10.1 General
.................

*Structure and Interpretation of Computer Programs*, by Harold Abelson
and Gerald Jay Sussman with Julie Sussman.  Full text at
<http://mitpress.mit.edu/sicp/>.  In this classic textbook of computer
science, chapters 2 and 3 discuss the use of sequences and streams to
organize the data flow inside a program.  The book uses Scheme for its
examples, but many of the design approaches described in these chapters
are applicable to functional-style Python code.

  <http://www.defmacro.org/ramblings/fp.html>: A general introduction
to functional programming that uses Java examples and has a lengthy
historical introduction.

  <http://en.wikipedia.org/wiki/Functional_programming>: General
Wikipedia entry describing functional programming.

  <http://en.wikipedia.org/wiki/Coroutine>: Entry for coroutines.

  <http://en.wikipedia.org/wiki/Currying>: Entry for the concept of
currying.


File: python.info,  Node: Python-specific,  Next: Python documentation,  Prev: General,  Up: References

11.6.10.2 Python-specific
.........................

<http://gnosis.cx/TPiP/>: The first chapter of David Mertz's book `Text
Processing in Python' discusses functional programming for text
processing, in the section titled "Utilizing Higher-Order Functions in
Text Processing".

  Mertz also wrote a 3-part series of articles on functional programming
for IBM's DeveloperWorks site; see part 1(1), part 2(2), and part 3(3),

  ---------- Footnotes ----------

  (1) http://www-128.ibm.com/developerworks/library/l-prog.html

  (2) http://www-128.ibm.com/developerworks/library/l-prog2.html

  (3) http://www-128.ibm.com/developerworks/linux/library/l-prog3.html


File: python.info,  Node: Python documentation,  Prev: Python-specific,  Up: References

11.6.10.3 Python documentation
..............................

Documentation for the *note itertools: fb. module.

  Documentation for the *note operator: 127. module.

  PEP 289(1): "Generator Expressions"

  PEP 342(2): "Coroutines via Enhanced Generators" describes the new
generator features in Python 2.5.

  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-0289

  (2) http://www.python.org/dev/peps/pep-0342


File: python.info,  Node: Logging HOWTO,  Next: Logging Cookbook,  Prev: Functional Programming HOWTO,  Up: Python HOWTOs

11.7 Logging HOWTO
==================

     Author: Vinay Sajip <vinay_sajip at red-dove dot com>

* Menu:

* Basic Logging Tutorial::
* Advanced Logging Tutorial::
* Logging Levels::
* Useful Handlers::
* Exceptions raised during logging::
* Using arbitrary objects as messages::
* Optimization::

Basic Logging Tutorial

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::

Advanced Logging Tutorial

* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::

Logging Levels

* Custom Levels::


File: python.info,  Node: Basic Logging Tutorial,  Next: Advanced Logging Tutorial,  Up: Logging HOWTO

11.7.1 Basic Logging Tutorial
-----------------------------

Logging is a means of tracking events that happen when some software
runs. The software's developer adds logging calls to their code to
indicate that certain events have occurred. An event is described by a
descriptive message which can optionally contain variable data (i.e.
data that is potentially different for each occurrence of the event).
Events also have an importance which the developer ascribes to the
event; the importance can also be called the _level_ or _severity_.

* Menu:

* When to use logging::
* A simple example::
* Logging to a file::
* Logging from multiple modules::
* Logging variable data::
* Changing the format of displayed messages::
* Displaying the date/time in messages::
* Next Steps::


File: python.info,  Node: When to use logging,  Next: A simple example,  Up: Basic Logging Tutorial

11.7.1.1 When to use logging
............................

Logging provides a set of convenience functions for simple logging
usage. These are *note debug(): 126b, *note info(): 1297, *note
warning(): 12a3, *note error(): 12a4. and *note critical(): 12a6. To
determine when to use logging, see the table below, which states, for
each of a set of common tasks, the best tool to use for it.

Task you want to perform                  The best tool for the task
------------------------------------------------------------------------------------- 
Display console output for ordinary       *note print(): 2fc.
usage of a command line script or program 
Report events that occur during normal    *note logging.info(): 1297. (or *note
operation of a program (e.g.  for status  logging.debug(): 126b. for very detailed
monitoring or fault investigation)        output for diagnostic purposes)
Issue a warning regarding a particular    *note warnings.warn(): 4ad. in library
runtime event                             code if the issue is avoidable and the
                                          client application should be modified to
                                          eliminate the warning
                                          
                                            *note logging.warning(): 12a3. if there
                                          is nothing the client application can do
                                          about the situation, but the event should
                                          still be noted
Report an error regarding a particular    Raise an exception
runtime event                             
Report suppression of an error without    *note logging.error(): 12a4, *note
raising an exception (e.g.  error         logging.exception(): 12a5. or *note
handler in a long-running server process) logging.critical(): 12a6. as appropriate
                                          for the specific error and application
                                          domain

  The logging functions are named after the level or severity of the
events they are used to track. The standard levels and their
applicability are described below (in increasing order of severity):

Level              When it's used
--------------------------------------------------------------------- 
`DEBUG'            Detailed information, typically of interest only
                   when diagnosing problems.
`INFO'             Confirmation that things are working as expected.
`WARNING'          An indication that something unexpected
                   happened, or indicative of some problem in the
                   near future (e.g. 'disk space low').  The
                   software is still working as expected.
`ERROR'            Due to a more serious problem, the software has
                   not been able to perform some function.
`CRITICAL'         A serious error, indicating that the program
                   itself may be unable to continue running.

  The default level is `WARNING', which means that only events of this
level and above will be tracked, unless the logging package is
configured to do otherwise.

  Events that are tracked can be handled in different ways. The
simplest way of handling tracked events is to print them to the
console. Another common way is to write them to a disk file.


File: python.info,  Node: A simple example,  Next: Logging to a file,  Prev: When to use logging,  Up: Basic Logging Tutorial

11.7.1.2 A simple example
.........................

A very simple example is:

    import logging
    logging.warning('Watch out!') # will print a message to the console
    logging.info('I told you so') # will not print anything

If you type these lines into a script and run it, you'll see:

    WARNING:root:Watch out!

printed out on the console. The `INFO' message doesn't appear because
the default level is `WARNING'. The printed message includes the
indication of the level and the description of the event provided in
the logging call, i.e.  'Watch out!'. Don't worry about the 'root' part
for now: it will be explained later. The actual output can be formatted
quite flexibly if you need that; formatting options will also be
explained later.


File: python.info,  Node: Logging to a file,  Next: Logging from multiple modules,  Prev: A simple example,  Up: Basic Logging Tutorial

11.7.1.3 Logging to a file
..........................

A very common situation is that of recording logging events in a file,
so let's look at that next:

    import logging
    logging.basicConfig(filename='example.log',level=logging.DEBUG)
    logging.debug('This message should go to the log file')
    logging.info('So should this')
    logging.warning('And this, too')

And now if we open the file and look at what we have, we should find
the log messages:

    DEBUG:root:This message should go to the log file
    INFO:root:So should this
    WARNING:root:And this, too

This example also shows how you can set the logging level which acts as
the threshold for tracking. In this case, because we set the threshold
to `DEBUG', all of the messages were printed.

  If you want to set the logging level from a command-line option such
as:

    --log=INFO

and you have the value of the parameter passed for `--log' in some
variable _loglevel_, you can use:

    getattr(logging, loglevel.upper())

to get the value which you'll pass to *note basicConfig(): 12ad. via
the _level_ argument. You may want to error check any user input value,
perhaps as in the following example:

    # assuming loglevel is bound to the string value obtained from the
    # command line argument. Convert to upper case to allow the user to
    # specify --log=DEBUG or --log=debug
    numeric_level = getattr(logging, loglevel.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError('Invalid log level: %s' % loglevel)
    logging.basicConfig(level=numeric_level, ...)

The call to *note basicConfig(): 12ad. should come _before_ any calls
to *note debug(): 126b, *note info(): 1297. etc. As it's intended as a
one-off simple configuration facility, only the first call will
actually do anything: subsequent calls are effectively no-ops.

  If you run the above script several times, the messages from
successive runs are appended to the file _example.log_. If you want
each run to start afresh, not remembering the messages from earlier
runs, you can specify the _filemode_ argument, by changing the call in
the above example to:

    logging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)

The output will be the same as before, but the log file is no longer
appended to, so the messages from earlier runs are lost.


File: python.info,  Node: Logging from multiple modules,  Next: Logging variable data,  Prev: Logging to a file,  Up: Basic Logging Tutorial

11.7.1.4 Logging from multiple modules
......................................

If your program consists of multiple modules, here's an example of how
you could organize logging in it:

    # myapp.py
    import logging
    import mylib

    def main():
        logging.basicConfig(filename='myapp.log', level=logging.INFO)
        logging.info('Started')
        mylib.do_something()
        logging.info('Finished')

    if __name__ == '__main__':
        main()


    # mylib.py
    import logging

    def do_something():
        logging.info('Doing something')

If you run _myapp.py_, you should see this in _myapp.log_:

    INFO:root:Started
    INFO:root:Doing something
    INFO:root:Finished

which is hopefully what you were expecting to see. You can generalize
this to multiple modules, using the pattern in _mylib.py_. Note that
for this simple usage pattern, you won't know, by looking in the log
file, _where_ in your application your messages came from, apart from
looking at the event description. If you want to track the location of
your messages, you'll need to refer to the documentation beyond the
tutorial level - see *note Advanced Logging Tutorial: 1260.


File: python.info,  Node: Logging variable data,  Next: Changing the format of displayed messages,  Prev: Logging from multiple modules,  Up: Basic Logging Tutorial

11.7.1.5 Logging variable data
..............................

To log variable data, use a format string for the event description
message and append the variable data as arguments. For example:

    import logging
    logging.warning('%s before you %s', 'Look', 'leap!')

will display:

    WARNING:root:Look before you leap!

As you can see, merging of variable data into the event description
message uses the old, %-style of string formatting. This is for
backwards compatibility: the logging package pre-dates newer formatting
options such as *note str.format(): 1cf. and *note string.Template:
577. These newer formatting options _are_ supported, but exploring them
is outside the scope of this tutorial.


File: python.info,  Node: Changing the format of displayed messages,  Next: Displaying the date/time in messages,  Prev: Logging variable data,  Up: Basic Logging Tutorial

11.7.1.6 Changing the format of displayed messages
..................................................

To change the format which is used to display messages, you need to
specify the format you want to use:

    import logging
    logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)
    logging.debug('This message should appear on the console')
    logging.info('So should this')
    logging.warning('And this, too')

which would print:

    DEBUG:This message should appear on the console
    INFO:So should this
    WARNING:And this, too

Notice that the 'root' which appeared in earlier examples has
disappeared. For a full set of things that can appear in format
strings, you can refer to the documentation for *note LogRecord
attributes: 128e, but for simple usage, you just need the _levelname_
(severity), _message_ (event description, including variable data) and
perhaps to display when the event occurred. This is described in the
next section.


File: python.info,  Node: Displaying the date/time in messages,  Next: Next Steps,  Prev: Changing the format of displayed messages,  Up: Basic Logging Tutorial

11.7.1.7 Displaying the date/time in messages
.............................................

To display the date and time of an event, you would place '%(asctime)s'
in your format string:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s')
    logging.warning('is when this event was logged.')

which should print something like this:

    2010-12-12 11:41:42,612 is when this event was logged.

The default format for date/time display (shown above) is ISO8601. If
you need more control over the formatting of the date/time, provide a
_datefmt_ argument to `basicConfig', as in this example:

    import logging
    logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
    logging.warning('is when this event was logged.')

which would display something like this:

    12/12/2010 11:46:36 AM is when this event was logged.

The format of the _datefmt_ argument is the same as supported by *note
time.strftime(): 3ad.


File: python.info,  Node: Next Steps,  Prev: Displaying the date/time in messages,  Up: Basic Logging Tutorial

11.7.1.8 Next Steps
...................

That concludes the basic tutorial. It should be enough to get you up and
running with logging. There's a lot more that the logging package
offers, but to get the best out of it, you'll need to invest a little
more of your time in reading the following sections. If you're ready
for that, grab some of your favourite beverage and carry on.

  If your logging needs are simple, then use the above examples to
incorporate logging into your own scripts, and if you run into problems
or don't understand something, please post a question on the
comp.lang.python Usenet group (available at
<http://groups.google.com/group/comp.lang.python>) and you should
receive help before too long.

  Still here? You can carry on reading the next few sections, which
provide a slightly more advanced/in-depth tutorial than the basic one
above. After that, you can take a look at the *note Logging Cookbook:
1261.


File: python.info,  Node: Advanced Logging Tutorial,  Next: Logging Levels,  Prev: Basic Logging Tutorial,  Up: Logging HOWTO

11.7.2 Advanced Logging Tutorial
--------------------------------

The logging library takes a modular approach and offers several
categories of components: loggers, handlers, filters, and formatters.

   * Loggers expose the interface that application code directly uses.

   * Handlers send the log records (created by loggers) to the
     appropriate destination.

   * Filters provide a finer grained facility for determining which log
     records to output.

   * Formatters specify the layout of log records in the final output.

  Logging is performed by calling methods on instances of the *note
Logger: 1da.  class (hereafter called _loggers_). Each instance has a
name, and they are conceptually arranged in a namespace hierarchy using
dots (periods) as separators. For example, a logger named 'scan' is the
parent of loggers 'scan.text', 'scan.html' and 'scan.pdf'. Logger names
can be anything you want, and indicate the area of an application in
which a logged message originates.

  A good convention to use when naming loggers is to use a module-level
logger, in each module which uses logging, named as follows:

    logger = logging.getLogger(__name__)

This means that logger names track the package/module hierarchy, and
it's intuitively obvious where events are logged just from the logger
name.

  The root of the hierarchy of loggers is called the root logger.
That's the logger used by the functions *note debug(): 126b, *note
info(): 1297, *note warning(): 12a3, *note error(): 12a4. and *note
critical(): 12a6, which just call the same-named method of the root
logger. The functions and the methods have the same signatures. The
root logger's name is printed as 'root' in the logged output.

  It is, of course, possible to log messages to different destinations.
Support is included in the package for writing log messages to files,
HTTP GET/POST locations, email via SMTP, generic sockets, or
OS-specific logging mechanisms such as syslog or the Windows NT event
log. Destinations are served by _handler_ classes. You can create your
own log destination class if you have special requirements not met by
any of the built-in handler classes.

  By default, no destination is set for any logging messages. You can
specify a destination (such as console or file) by using *note
basicConfig(): 12ad. as in the tutorial examples. If you call the
functions  *note debug(): 126b, *note info(): 1297, *note warning():
12a3, *note error(): 12a4. and *note critical(): 12a6, they will check
to see if no destination is set; and if one is not set, they will set a
destination of the console (`sys.stderr') and a default format for the
displayed message before delegating to the root logger to do the actual
message output.

  The default format set by *note basicConfig(): 12ad. for messages is:

    severity:logger name:message

You can change this by passing a format string to *note basicConfig():
12ad. with the _format_ keyword argument. For all options regarding how
a format string is constructed, see *note Formatter Objects: 128c.

* Menu:

* Loggers::
* Handlers::
* Formatters::
* Configuring Logging::
* What happens if no configuration is provided::
* Configuring Logging for a Library::


File: python.info,  Node: Loggers,  Next: Handlers,  Up: Advanced Logging Tutorial

11.7.2.1 Loggers
................

*note Logger: 1da. objects have a threefold job.  First, they expose
several methods to application code so that applications can log
messages at runtime.  Second, logger objects determine which log
messages to act upon based upon severity (the default filtering
facility) or filter objects.  Third, logger objects pass along relevant
log messages to all interested log handlers.

  The most widely used methods on logger objects fall into two
categories: configuration and message sending.

  These are the most common configuration methods:

   * *note Logger.setLevel(): 1265. specifies the lowest-severity log
     message a logger will handle, where debug is the lowest built-in
     severity level and critical is the highest built-in severity.  For
     example, if the severity level is INFO, the logger will handle
     only INFO, WARNING, ERROR, and CRITICAL messages and will ignore
     DEBUG messages.

   * *note Logger.addHandler(): 1274. and *note Logger.removeHandler():
     1275. add and remove handler objects from the logger object.
     Handlers are covered in more detail in *note Handlers: 2f91.

   * *note Logger.addFilter(): 1271. and *note Logger.removeFilter():
     1272. add and remove filter objects from the logger object.
     Filters are covered in more detail in *note Filter Objects: 1293.

  You don't need to always call these methods on every logger you
create. See the last two paragraphs in this section.

  With the logger object configured, the following methods create log
messages:

   * *note Logger.debug(): 1268, *note Logger.info(): 126a, *note
     Logger.warning(): 126c, *note Logger.error(): 126d, and *note
     Logger.critical(): 126e. all create log records with a message and
     a level that corresponds to their respective method names. The
     message is actually a format string, which may contain the
     standard string substitution syntax of `%s', `%d', `%f', and so
     on.  The rest of their arguments is a list of objects that
     correspond with the substitution fields in the message.  With
     regard to `**kwargs', the logging methods care only about a
     keyword of `exc_info' and use it to determine whether to log
     exception information.

   * *note Logger.exception(): 1270. creates a log message similar to
     *note Logger.error(): 126d.  The difference is that *note
     Logger.exception(): 1270. dumps a stack trace along with it.  Call
     this method only from an exception handler.

   * *note Logger.log(): 126f. takes a log level as an explicit
     argument.  This is a little more verbose for logging messages than
     using the log level convenience methods listed above, but this is
     how to log at custom log levels.

  *note getLogger(): 12aa. returns a reference to a logger instance
with the specified name if it is provided, or `root' if not.  The names
are period-separated hierarchical structures.  Multiple calls to *note
getLogger(): 12aa. with the same name will return a reference to the
same logger object.  Loggers that are further down in the hierarchical
list are children of loggers higher up in the list.  For example, given
a logger with a name of `foo', loggers with names of `foo.bar',
`foo.bar.baz', and `foo.bam' are all descendants of `foo'.

  Loggers have a concept of _effective level_. If a level is not
explicitly set on a logger, the level of its parent is used instead as
its effective level.  If the parent has no explicit level set, _its_
parent is examined, and so on - all ancestors are searched until an
explicitly set level is found. The root logger always has an explicit
level set (`WARNING' by default). When deciding whether to process an
event, the effective level of the logger is used to determine whether
the event is passed to the logger's handlers.

  Child loggers propagate messages up to the handlers associated with
their ancestor loggers. Because of this, it is unnecessary to define
and configure handlers for all the loggers an application uses. It is
sufficient to configure handlers for a top-level logger and create
child loggers as needed.  (You can, however, turn off propagation by
setting the _propagate_ attribute of a logger to _False_.)


File: python.info,  Node: Handlers,  Next: Formatters,  Prev: Loggers,  Up: Advanced Logging Tutorial

11.7.2.2 Handlers
.................

`Handler' objects are responsible for dispatching the appropriate log
messages (based on the log messages' severity) to the handler's
specified destination.  Logger objects can add zero or more handler
objects to themselves with an `addHandler()' method.  As an example
scenario, an application may want to send all log messages to a log
file, all log messages of error or higher to stdout, and all messages
of critical to an email address.  This scenario requires three
individual handlers where each handler is responsible for sending
messages of a specific severity to a specific location.

  The standard library includes quite a few handler types (see *note
Useful Handlers: 2f93.); the tutorials use mainly *note StreamHandler:
12b2. and *note FileHandler: 12d0. in its examples.

  There are very few methods in a handler for application developers to
concern themselves with.  The only handler methods that seem relevant
for application developers who are using the built-in handler objects
(that is, not creating custom handlers) are the following configuration
methods:

   * The *note Handler.setLevel(): 1280. method, just as in logger
     objects, specifies the lowest severity that will be dispatched to
     the appropriate destination.  Why are there two `setLevel()'
     methods?  The level set in the logger determines which severity of
     messages it will pass to its handlers.  The level set in each
     handler determines which messages that handler will send on.

   * `setFormatter()' selects a Formatter object for this handler to
     use.

   * `addFilter()' and `removeFilter()' respectively configure and
     deconfigure filter objects on handlers.

  Application code should not directly instantiate and use instances of
`Handler'.  Instead, the `Handler' class is a base class that defines
the interface that all handlers should have and establishes some
default behavior that child classes can use (or override).


File: python.info,  Node: Formatters,  Next: Configuring Logging,  Prev: Handlers,  Up: Advanced Logging Tutorial

11.7.2.3 Formatters
...................

Formatter objects configure the final order, structure, and contents of
the log message.  Unlike the base `logging.Handler' class, application
code may instantiate formatter classes, although you could likely
subclass the formatter if your application needs special behavior.  The
constructor takes two optional arguments - a message format string and
a date format string.

 -- Method: logging.Formatter.__init__ (fmt=None, datefmt=None)

  If there is no message format string, the default is to use the raw
message.  If there is no date format string, the default date format is:

    %Y-%m-%d %H:%M:%S

with the milliseconds tacked on at the end.

  The message format string uses `%(<dictionary key>)s' styled string
substitution; the possible keys are documented in *note LogRecord
attributes: 128e.

  The following message format string will log the time in a
human-readable format, the severity of the message, and the contents of
the message, in that order:

    '%(asctime)s - %(levelname)s - %(message)s'

Formatters use a user-configurable function to convert the creation
time of a record to a tuple. By default, *note time.localtime(): a9c.
is used; to change this for a particular formatter instance, set the
`converter' attribute of the instance to a function with the same
signature as *note time.localtime(): a9c. or *note time.gmtime(): b0b.
To change it for all formatters, for example if you want all logging
times to be shown in GMT, set the `converter' attribute in the
Formatter class (to `time.gmtime' for GMT display).


File: python.info,  Node: Configuring Logging,  Next: What happens if no configuration is provided,  Prev: Formatters,  Up: Advanced Logging Tutorial

11.7.2.4 Configuring Logging
............................

Programmers can configure logging in three ways:

  1. Creating loggers, handlers, and formatters explicitly using Python
     code that calls the configuration methods listed above.

  2. Creating a logging config file and reading it using the *note
     fileConfig(): 12bb.  function.

  3. Creating a dictionary of configuration information and passing it
     to the *note dictConfig(): 12b9. function.

  For the reference documentation on the last two options, see *note
Configuration functions: 1d6.  The following example configures a very
simple logger, a console handler, and a simple formatter using Python
code:

    import logging

    # create logger
    logger = logging.getLogger('simple_example')
    logger.setLevel(logging.DEBUG)

    # create console handler and set level to debug
    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)

    # create formatter
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

    # add formatter to ch
    ch.setFormatter(formatter)

    # add ch to logger
    logger.addHandler(ch)

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Running this module from the command line produces the following output:

    $ python simple_logging_module.py
    2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message
    2005-03-19 15:10:26,620 - simple_example - INFO - info message
    2005-03-19 15:10:26,695 - simple_example - WARNING - warn message
    2005-03-19 15:10:26,697 - simple_example - ERROR - error message
    2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message

The following Python module creates a logger, handler, and formatter
nearly identical to those in the example listed above, with the only
difference being the names of the objects:

    import logging
    import logging.config

    logging.config.fileConfig('logging.conf')

    # create logger
    logger = logging.getLogger('simpleExample')

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Here is the logging.conf file:

    [loggers]
    keys=root,simpleExample

    [handlers]
    keys=consoleHandler

    [formatters]
    keys=simpleFormatter

    [logger_root]
    level=DEBUG
    handlers=consoleHandler

    [logger_simpleExample]
    level=DEBUG
    handlers=consoleHandler
    qualname=simpleExample
    propagate=0

    [handler_consoleHandler]
    class=StreamHandler
    level=DEBUG
    formatter=simpleFormatter
    args=(sys.stdout,)

    [formatter_simpleFormatter]
    format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
    datefmt=

The output is nearly identical to that of the non-config-file-based
example:

    $ python simple_logging_config.py
    2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message
    2005-03-19 15:38:55,979 - simpleExample - INFO - info message
    2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message
    2005-03-19 15:38:56,055 - simpleExample - ERROR - error message
    2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message

You can see that the config file approach has a few advantages over the
Python code approach, mainly separation of configuration and code and
the ability of noncoders to easily modify the logging properties.

  Note that the class names referenced in config files need to be
either relative to the logging module, or absolute values which can be
resolved using normal import mechanisms. Thus, you could use either
*note WatchedFileHandler: 12e2. (relative to the logging module) or
`mypackage.mymodule.MyHandler' (for a class defined in package
`mypackage' and module `mymodule', where `mypackage' is available on
the Python import path).

  In Python 2.7, a new means of configuring logging has been
introduced, using dictionaries to hold configuration information. This
provides a superset of the functionality of the config-file-based
approach outlined above, and is the recommended configuration method
for new applications and deployments. Because a Python dictionary is
used to hold configuration information, and since you can populate that
dictionary using different means, you have more options for
configuration. For example, you can use a configuration file in JSON
format, or, if you have access to YAML processing functionality, a file
in YAML format, to populate the configuration dictionary. Or, of
course, you can construct the dictionary in Python code, receive it in
pickled form over a socket, or use whatever approach makes sense for
your application.

  Here's an example of the same configuration as above, in YAML format
for the new dictionary-based approach:

    version: 1
    formatters:
      simple:
        format: format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
    handlers:
      console:
        class: logging.StreamHandler
        level: DEBUG
        formatter: simple
        stream: ext://sys.stdout
    loggers:
      simpleExample:
        level: DEBUG
        handlers: [console]
        propagate: no
    root:
      level: DEBUG
      handlers: [console]

For more information about logging using a dictionary, see *note
Configuration functions: 1d6.


File: python.info,  Node: What happens if no configuration is provided,  Next: Configuring Logging for a Library,  Prev: Configuring Logging,  Up: Advanced Logging Tutorial

11.7.2.5 What happens if no configuration is provided
.....................................................

If no logging configuration is provided, it is possible to have a
situation where a logging event needs to be output, but no handlers can
be found to output the event. The behaviour of the logging package in
these circumstances is dependent on the Python version.

  For Python 2.x, the behaviour is as follows:

   * If _logging.raiseExceptions_ is _False_ (production mode), the
     event is silently dropped.

   * If _logging.raiseExceptions_ is _True_ (development mode), a
     message 'No handlers could be found for logger X.Y.Z' is printed
     once.


File: python.info,  Node: Configuring Logging for a Library,  Prev: What happens if no configuration is provided,  Up: Advanced Logging Tutorial

11.7.2.6 Configuring Logging for a Library
..........................................

When developing a library which uses logging, you should take care to
document how the library uses logging - for example, the names of
loggers used. Some consideration also needs to be given to its logging
configuration.  If the using application does not use logging, and
library code makes logging calls, then (as described in the previous
section) events of severity `WARNING' and greater will be printed to
`sys.stderr'. This is regarded as the best default behaviour.

  If for some reason you _don't_ want these messages printed in the
absence of any logging configuration, you can attach a do-nothing
handler to the top-level logger for your library. This avoids the
message being printed, since a handler will be always be found for the
library's events: it just doesn't produce any output. If the library
user configures logging for application use, presumably that
configuration will add some handlers, and if levels are suitably
configured then logging calls made in library code will send output to
those handlers, as normal.

  A do-nothing handler is included in the logging package: *note
NullHandler: 12d1. (since Python 2.7). An instance of this handler
could be added to the top-level logger of the logging namespace used by
the library (_if_ you want to prevent your library's logged events
being output to `sys.stderr' in the absence of logging configuration).
If all logging by a library _foo_ is done using loggers with names
matching 'foo.x', 'foo.x.y', etc. then the code:

    import logging
    logging.getLogger('foo').addHandler(logging.NullHandler())

should have the desired effect. If an organisation produces a number of
libraries, then the logger name specified can be 'orgname.foo' rather
than just 'foo'.

  *PLEASE NOTE:* It is strongly advised that you _do not add any
handlers other than_ *note NullHandler: 12d1. _to your library's
loggers_. This is because the configuration of handlers is the
prerogative of the application developer who uses your library. The
application developer knows their target audience and what handlers are
most appropriate for their application: if you add handlers 'under the
hood', you might well interfere with their ability to carry out unit
tests and deliver logs which suit their requirements.


File: python.info,  Node: Logging Levels,  Next: Useful Handlers,  Prev: Advanced Logging Tutorial,  Up: Logging HOWTO

11.7.3 Logging Levels
---------------------

The numeric values of logging levels are given in the following table.
These are primarily of interest if you want to define your own levels,
and need them to have specific values relative to the predefined
levels. If you define a level with the same numeric value, it
overwrites the predefined value; the predefined name is lost.

Level              Numeric value
--------------------------------------- 
`CRITICAL'         50
`ERROR'            40
`WARNING'          30
`INFO'             20
`DEBUG'            10
`NOTSET'           0

  Levels can also be associated with loggers, being set either by the
developer or through loading a saved logging configuration. When a
logging method is called on a logger, the logger compares its own level
with the level associated with the method call. If the logger's level
is higher than the method call's, no logging message is actually
generated. This is the basic mechanism controlling the verbosity of
logging output.

  Logging messages are encoded as instances of the *note LogRecord:
1279.  class. When a logger decides to actually log an event, a *note
LogRecord: 1279. instance is created from the logging message.

  Logging messages are subjected to a dispatch mechanism through the
use of _handlers_, which are instances of subclasses of the `Handler'
class. Handlers are responsible for ensuring that a logged message (in
the form of a *note LogRecord: 1279.) ends up in a particular location
(or set of locations) which is useful for the target audience for that
message (such as end users, support desk staff, system administrators,
developers). Handlers are passed *note LogRecord: 1279. instances
intended for particular destinations. Each logger can have zero, one or
more handlers associated with it (via the *note addHandler(): 1274.
method of *note Logger: 1da.). In addition to any handlers directly
associated with a logger, _all handlers associated with all ancestors
of the logger_ are called to dispatch the message (unless the
_propagate_ flag for a logger is set to a false value, at which point
the passing to ancestor handlers stops).

  Just as for loggers, handlers can have levels associated with them. A
handler's level acts as a filter in the same way as a logger's level
does. If a handler decides to actually dispatch an event, the *note
emit(): 128a. method is used to send the message to its destination.
Most user-defined subclasses of `Handler' will need to override this
*note emit(): 128a.

* Menu:

* Custom Levels::


File: python.info,  Node: Custom Levels,  Up: Logging Levels

11.7.3.1 Custom Levels
......................

Defining your own levels is possible, but should not be necessary, as
the existing levels have been chosen on the basis of practical
experience.  However, if you are convinced that you need custom levels,
great care should be exercised when doing this, and it is possibly _a
very bad idea to define custom levels if you are developing a library_.
That's because if multiple library authors all define their own custom
levels, there is a chance that the logging output from such multiple
libraries used together will be difficult for the using developer to
control and/or interpret, because a given numeric value might mean
different things for different libraries.


File: python.info,  Node: Useful Handlers,  Next: Exceptions raised during logging,  Prev: Logging Levels,  Up: Logging HOWTO

11.7.4 Useful Handlers
----------------------

In addition to the base `Handler' class, many useful subclasses are
provided:

  1. *note StreamHandler: 12b2. instances send messages to streams
     (file-like objects).

  2. *note FileHandler: 12d0. instances send messages to disk files.

  3. `BaseRotatingHandler' is the base class for handlers that rotate
     log files at a certain point. It is not meant to be  instantiated
     directly. Instead, use *note RotatingFileHandler: 12c2. or *note
     TimedRotatingFileHandler: 12ea.

  4. *note RotatingFileHandler: 12c2. instances send messages to disk
     files, with support for maximum log file sizes and log file
     rotation.

  5. *note TimedRotatingFileHandler: 12ea. instances send messages to
     disk files, rotating the log file at certain timed intervals.

  6. *note SocketHandler: 12ef. instances send messages to TCP/IP
     sockets.

  7. *note DatagramHandler: 12f9. instances send messages to UDP
     sockets.

  8. *note SMTPHandler: 130d. instances send messages to a designated
     email address.

  9. *note SysLogHandler: 1d7. instances send messages to a Unix syslog
     daemon, possibly on a remote machine.

 10. *note NTEventLogHandler: 1305. instances send messages to a
     Windows NT/2000/XP event log.

 11. *note MemoryHandler: 12cb. instances send messages to a buffer in
     memory, which is flushed whenever specific criteria are met.

 12. *note HTTPHandler: 131c. instances send messages to an HTTP server
     using either `GET' or `POST' semantics.

 13. *note WatchedFileHandler: 12e2. instances watch the file they are
     logging to. If the file changes, it is closed and reopened using
     the file name. This handler is only useful on Unix-like systems;
     Windows does not support the underlying mechanism used.

 14. *note NullHandler: 12d1. instances do nothing with error messages.
     They are used by library developers who want to use logging, but
     want to avoid the 'No handlers could be found for logger XXX'
     message which can be displayed if the library user has not
     configured logging. See *note Configuring Logging for a Library:
     12df. for more information.

  New in version 2.7: The *note NullHandler: 12d1. class.

  The *note NullHandler: 12d1, *note StreamHandler: 12b2. and *note
FileHandler: 12d0.  classes are defined in the core logging package.
The other handlers are defined in a sub- module, *note
logging.handlers: 104. (There is also another sub-module, *note
logging.config: 103, for configuration functionality.)

  Logged messages are formatted for presentation through instances of
the *note Formatter: 1269. class. They are initialized with a format
string suitable for use with the % operator and a dictionary.

  For formatting multiple messages in a batch, instances of
`BufferingFormatter' can be used. In addition to the format string
(which is applied to each message in the batch), there is provision for
header and trailer format strings.

  When filtering based on logger level and/or handler level is not
enough, instances of *note Filter: 1295. can be added to both *note
Logger: 1da. and `Handler' instances (through their `addFilter()'
method). Before deciding to process a message further, both loggers and
handlers consult all their filters for permission. If any filter
returns a false value, the message is not processed further.

  The basic *note Filter: 1295. functionality allows filtering by
specific logger name. If this feature is used, messages sent to the
named logger and its children are allowed through the filter, and all
others dropped.


File: python.info,  Node: Exceptions raised during logging,  Next: Using arbitrary objects as messages,  Prev: Useful Handlers,  Up: Logging HOWTO

11.7.5 Exceptions raised during logging
---------------------------------------

The logging package is designed to swallow exceptions which occur while
logging in production. This is so that errors which occur while
handling logging events - such as logging misconfiguration, network or
other similar errors - do not cause the application using logging to
terminate prematurely.

  `SystemExit' and `KeyboardInterrupt' exceptions are never swallowed.
Other exceptions which occur during the `emit()' method of a `Handler'
subclass are passed to its `handleError()' method.

  The default implementation of `handleError()' in `Handler' checks to
see if a module-level variable, `raiseExceptions', is set. If set, a
traceback is printed to *note sys.stderr: 620. If not set, the
exception is swallowed.

  *Note_* The default value of `raiseExceptions' is `True'. This is
because during development, you typically want to be notified of any
exceptions that occur. It's advised that you set `raiseExceptions' to
`False' for production usage.


File: python.info,  Node: Using arbitrary objects as messages,  Next: Optimization,  Prev: Exceptions raised during logging,  Up: Logging HOWTO

11.7.6 Using arbitrary objects as messages
------------------------------------------

In the preceding sections and examples, it has been assumed that the
message passed when logging the event is a string. However, this is not
the only possibility. You can pass an arbitrary object as a message,
and its *note __str__(): 483. method will be called when the logging
system needs to convert it to a string representation. In fact, if you
want to, you can avoid computing a string representation altogether -
for example, the `SocketHandler' emits an event by pickling it and
sending it over the wire.


File: python.info,  Node: Optimization,  Prev: Using arbitrary objects as messages,  Up: Logging HOWTO

11.7.7 Optimization
-------------------

Formatting of message arguments is deferred until it cannot be avoided.
However, computing the arguments passed to the logging method can also
be expensive, and you may want to avoid doing it if the logger will
just throw away your event. To decide what to do, you can call the
`isEnabledFor()' method which takes a level argument and returns true
if the event would be created by the Logger for that level of call. You
can write code like this:

    if logger.isEnabledFor(logging.DEBUG):
        logger.debug('Message with %s, %s', expensive_func1(),
                                            expensive_func2())

so that if the logger's threshold is set above `DEBUG', the calls to
`expensive_func1()' and `expensive_func2()' are never made.

  There are other optimizations which can be made for specific
applications which need more precise control over what logging
information is collected. Here's a list of things you can do to avoid
processing during logging which you don't need:

What you don't want to collect                      How to avoid collecting it
------------------------------------------------------------------------------------------------- 
Information about where calls were made from.       Set `logging._srcfile' to `None'.
Threading information.                              Set `logging.logThreads' to `0'.
Process information.                                Set `logging.logProcesses' to `0'.

  Also note that the core logging module only includes the basic
handlers. If you don't import *note logging.handlers: 104. and *note
logging.config: 103, they won't take up any memory.

See also
........

Module *note logging: 102.
     API reference for the logging module.

Module *note logging.config: 103.
     Configuration API for the logging module.

Module *note logging.handlers: 104.
     Useful handlers included with the logging module.

  *note A logging cookbook: 1261.


File: python.info,  Node: Logging Cookbook,  Next: Regular Expression HOWTO,  Prev: Logging HOWTO,  Up: Python HOWTOs

11.8 Logging Cookbook
=====================

     Author: Vinay Sajip <vinay_sajip at red-dove dot com>

  This page contains a number of recipes related to logging, which have
been found useful in the past.

* Menu:

* Using logging in multiple modules::
* Multiple handlers and formatters::
* Logging to multiple destinations::
* Configuration server example::
* Sending and receiving logging events across a network::
* Adding contextual information to your logging output::
* Logging to a single file from multiple processes::
* Using file rotation::


File: python.info,  Node: Using logging in multiple modules,  Next: Multiple handlers and formatters,  Up: Logging Cookbook

11.8.1 Using logging in multiple modules
----------------------------------------

Multiple calls to `logging.getLogger('someLogger')' return a reference
to the same logger object.  This is true not only within the same
module, but also across modules as long as it is in the same Python
interpreter process.  It is true for references to the same object;
additionally, application code can define and configure a parent logger
in one module and create (but not configure) a child logger in a
separate module, and all logger calls to the child will pass up to the
parent.  Here is a main module:

    import logging
    import auxiliary_module

    # create logger with 'spam_application'
    logger = logging.getLogger('spam_application')
    logger.setLevel(logging.DEBUG)
    # create file handler which logs even debug messages
    fh = logging.FileHandler('spam.log')
    fh.setLevel(logging.DEBUG)
    # create console handler with a higher log level
    ch = logging.StreamHandler()
    ch.setLevel(logging.ERROR)
    # create formatter and add it to the handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    # add the handlers to the logger
    logger.addHandler(fh)
    logger.addHandler(ch)

    logger.info('creating an instance of auxiliary_module.Auxiliary')
    a = auxiliary_module.Auxiliary()
    logger.info('created an instance of auxiliary_module.Auxiliary')
    logger.info('calling auxiliary_module.Auxiliary.do_something')
    a.do_something()
    logger.info('finished auxiliary_module.Auxiliary.do_something')
    logger.info('calling auxiliary_module.some_function()')
    auxiliary_module.some_function()
    logger.info('done with auxiliary_module.some_function()')

Here is the auxiliary module:

    import logging

    # create logger
    module_logger = logging.getLogger('spam_application.auxiliary')

    class Auxiliary:
        def __init__(self):
            self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')
            self.logger.info('creating an instance of Auxiliary')
        def do_something(self):
            self.logger.info('doing something')
            a = 1 + 1
            self.logger.info('done doing something')

    def some_function():
        module_logger.info('received a call to "some_function"')

The output looks like this:

    2005-03-23 23:47:11,663 - spam_application - INFO -
       creating an instance of auxiliary_module.Auxiliary
    2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -
       creating an instance of Auxiliary
    2005-03-23 23:47:11,665 - spam_application - INFO -
       created an instance of auxiliary_module.Auxiliary
    2005-03-23 23:47:11,668 - spam_application - INFO -
       calling auxiliary_module.Auxiliary.do_something
    2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -
       doing something
    2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -
       done doing something
    2005-03-23 23:47:11,670 - spam_application - INFO -
       finished auxiliary_module.Auxiliary.do_something
    2005-03-23 23:47:11,671 - spam_application - INFO -
       calling auxiliary_module.some_function()
    2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -
       received a call to 'some_function'
    2005-03-23 23:47:11,673 - spam_application - INFO -
       done with auxiliary_module.some_function()



File: python.info,  Node: Multiple handlers and formatters,  Next: Logging to multiple destinations,  Prev: Using logging in multiple modules,  Up: Logging Cookbook

11.8.2 Multiple handlers and formatters
---------------------------------------

Loggers are plain Python objects.  The `addHandler()' method has no
minimum or maximum quota for the number of handlers you may add.
Sometimes it will be beneficial for an application to log all messages
of all severities to a text file while simultaneously logging errors or
above to the console.  To set this up, simply configure the appropriate
handlers.  The logging calls in the application code will remain
unchanged.  Here is a slight modification to the previous simple
module-based configuration example:

    import logging

    logger = logging.getLogger('simple_example')
    logger.setLevel(logging.DEBUG)
    # create file handler which logs even debug messages
    fh = logging.FileHandler('spam.log')
    fh.setLevel(logging.DEBUG)
    # create console handler with a higher log level
    ch = logging.StreamHandler()
    ch.setLevel(logging.ERROR)
    # create formatter and add it to the handlers
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)
    # add the handlers to logger
    logger.addHandler(ch)
    logger.addHandler(fh)

    # 'application' code
    logger.debug('debug message')
    logger.info('info message')
    logger.warn('warn message')
    logger.error('error message')
    logger.critical('critical message')

Notice that the 'application' code does not care about multiple
handlers.  All that changed was the addition and configuration of a new
handler named _fh_.

  The ability to create new handlers with higher- or lower-severity
filters can be very helpful when writing and testing an application.
Instead of using many `print' statements for debugging, use
`logger.debug': Unlike the print statements, which you will have to
delete or comment out later, the logger.debug statements can remain
intact in the source code and remain dormant until you need them again.
At that time, the only change that needs to happen is to modify the
severity level of the logger and/or handler to debug.


File: python.info,  Node: Logging to multiple destinations,  Next: Configuration server example,  Prev: Multiple handlers and formatters,  Up: Logging Cookbook

11.8.3 Logging to multiple destinations
---------------------------------------

Let's say you want to log to console and file with different message
formats and in differing circumstances. Say you want to log messages
with levels of DEBUG and higher to file, and those messages at level
INFO and higher to the console.  Let's also assume that the file should
contain timestamps, but the console messages should not. Here's how you
can achieve this:

    import logging

    # set up logging to file - see previous section for more details
    logging.basicConfig(level=logging.DEBUG,
                        format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                        datefmt='%m-%d %H:%M',
                        filename='/temp/myapp.log',
                        filemode='w')
    # define a Handler which writes INFO messages or higher to the sys.stderr
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    # set a format which is simpler for console use
    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
    # tell the handler to use this format
    console.setFormatter(formatter)
    # add the handler to the root logger
    logging.getLogger('').addHandler(console)

    # Now, we can log to the root logger, or any other logger. First the root...
    logging.info('Jackdaws love my big sphinx of quartz.')

    # Now, define a couple of other loggers which might represent areas in your
    # application:

    logger1 = logging.getLogger('myapp.area1')
    logger2 = logging.getLogger('myapp.area2')

    logger1.debug('Quick zephyrs blow, vexing daft Jim.')
    logger1.info('How quickly daft jumping zebras vex.')
    logger2.warning('Jail zesty vixen who grabbed pay from quack.')
    logger2.error('The five boxing wizards jump quickly.')

When you run this, on the console you will see

    root        : INFO     Jackdaws love my big sphinx of quartz.
    myapp.area1 : INFO     How quickly daft jumping zebras vex.
    myapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.
    myapp.area2 : ERROR    The five boxing wizards jump quickly.

and in the file you will see something like

    10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.
    10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.
    10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.
    10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.
    10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.

As you can see, the DEBUG message only shows up in the file. The other
messages are sent to both destinations.

  This example uses console and file handlers, but you can use any
number and combination of handlers you choose.


File: python.info,  Node: Configuration server example,  Next: Sending and receiving logging events across a network,  Prev: Logging to multiple destinations,  Up: Logging Cookbook

11.8.4 Configuration server example
-----------------------------------

Here is an example of a module using the logging configuration server:

    import logging
    import logging.config
    import time
    import os

    # read initial config file
    logging.config.fileConfig('logging.conf')

    # create and start listener on port 9999
    t = logging.config.listen(9999)
    t.start()

    logger = logging.getLogger('simpleExample')

    try:
        # loop through logging calls to see the difference
        # new configurations make, until Ctrl+C is pressed
        while True:
            logger.debug('debug message')
            logger.info('info message')
            logger.warn('warn message')
            logger.error('error message')
            logger.critical('critical message')
            time.sleep(5)
    except KeyboardInterrupt:
        # cleanup
        logging.config.stopListening()
        t.join()

And here is a script that takes a filename and sends that file to the
server, properly preceded with the binary-encoded length, as the new
logging configuration:

    #!/usr/bin/env python
    import socket, sys, struct

    with open(sys.argv[1], 'rb') as f:
        data_to_send = f.read()

    HOST = 'localhost'
    PORT = 9999
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    print('connecting...')
    s.connect((HOST, PORT))
    print('sending config...')
    s.send(struct.pack('>L', len(data_to_send)))
    s.send(data_to_send)
    s.close()
    print('complete')



File: python.info,  Node: Sending and receiving logging events across a network,  Next: Adding contextual information to your logging output,  Prev: Configuration server example,  Up: Logging Cookbook

11.8.5 Sending and receiving logging events across a network
------------------------------------------------------------

Let's say you want to send logging events across a network, and handle
them at the receiving end. A simple way of doing this is attaching a
`SocketHandler' instance to the root logger at the sending end:

    import logging, logging.handlers

    rootLogger = logging.getLogger('')
    rootLogger.setLevel(logging.DEBUG)
    socketHandler = logging.handlers.SocketHandler('localhost',
                        logging.handlers.DEFAULT_TCP_LOGGING_PORT)
    # don't bother with a formatter, since a socket handler sends the event as
    # an unformatted pickle
    rootLogger.addHandler(socketHandler)

    # Now, we can log to the root logger, or any other logger. First the root...
    logging.info('Jackdaws love my big sphinx of quartz.')

    # Now, define a couple of other loggers which might represent areas in your
    # application:

    logger1 = logging.getLogger('myapp.area1')
    logger2 = logging.getLogger('myapp.area2')

    logger1.debug('Quick zephyrs blow, vexing daft Jim.')
    logger1.info('How quickly daft jumping zebras vex.')
    logger2.warning('Jail zesty vixen who grabbed pay from quack.')
    logger2.error('The five boxing wizards jump quickly.')

At the receiving end, you can set up a receiver using the `socketserver'
module. Here is a basic working example:

    import pickle
    import logging
    import logging.handlers
    import socketserver
    import struct


    class LogRecordStreamHandler(socketserver.StreamRequestHandler):
        """Handler for a streaming logging request.

        This basically logs the record using whatever logging policy is
        configured locally.
        """

        def handle(self):
            """
            Handle multiple requests - each expected to be a 4-byte length,
            followed by the LogRecord in pickle format. Logs the record
            according to whatever policy is configured locally.
            """
            while True:
                chunk = self.connection.recv(4)
                if len(chunk) < 4:
                    break
                slen = struct.unpack('>L', chunk)[0]
                chunk = self.connection.recv(slen)
                while len(chunk) < slen:
                    chunk = chunk + self.connection.recv(slen - len(chunk))
                obj = self.unPickle(chunk)
                record = logging.makeLogRecord(obj)
                self.handleLogRecord(record)

        def unPickle(self, data):
            return pickle.loads(data)

        def handleLogRecord(self, record):
            # if a name is specified, we use the named logger rather than the one
            # implied by the record.
            if self.server.logname is not None:
                name = self.server.logname
            else:
                name = record.name
            logger = logging.getLogger(name)
            # N.B. EVERY record gets logged. This is because Logger.handle
            # is normally called AFTER logger-level filtering. If you want
            # to do filtering, do it at the client end to save wasting
            # cycles and network bandwidth!
            logger.handle(record)

    class LogRecordSocketReceiver(socketserver.ThreadingTCPServer):
        """
        Simple TCP socket-based logging receiver suitable for testing.
        """

        allow_reuse_address = 1

        def __init__(self, host='localhost',
                     port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,
                     handler=LogRecordStreamHandler):
            socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)
            self.abort = 0
            self.timeout = 1
            self.logname = None

        def serve_until_stopped(self):
            import select
            abort = 0
            while not abort:
                rd, wr, ex = select.select([self.socket.fileno()],
                                           [], [],
                                           self.timeout)
                if rd:
                    self.handle_request()
                abort = self.abort

    def main():
        logging.basicConfig(
            format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')
        tcpserver = LogRecordSocketReceiver()
        print('About to start TCP server...')
        tcpserver.serve_until_stopped()

    if __name__ == '__main__':
        main()

First run the server, and then the client. On the client side, nothing
is printed on the console; on the server side, you should see something
like:

    About to start TCP server...
       59 root            INFO     Jackdaws love my big sphinx of quartz.
       59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.
       69 myapp.area1     INFO     How quickly daft jumping zebras vex.
       69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.
       69 myapp.area2     ERROR    The five boxing wizards jump quickly.

Note that there are some security issues with pickle in some scenarios.
If these affect you, you can use an alternative serialization scheme by
overriding the `makePickle()' method and implementing your alternative
there, as well as adapting the above script to use your alternative
serialization.


File: python.info,  Node: Adding contextual information to your logging output,  Next: Logging to a single file from multiple processes,  Prev: Sending and receiving logging events across a network,  Up: Logging Cookbook

11.8.6 Adding contextual information to your logging output
-----------------------------------------------------------

Sometimes you want logging output to contain contextual information in
addition to the parameters passed to the logging call. For example, in a
networked application, it may be desirable to log client-specific
information in the log (e.g. remote client's username, or IP address).
Although you could use the _extra_ parameter to achieve this, it's not
always convenient to pass the information in this way. While it might
be tempting to create *note Logger: 1da. instances on a per-connection
basis, this is not a good idea because these instances are not garbage
collected. While this is not a problem in practice, when the number of
*note Logger: 1da. instances is dependent on the level of granularity
you want to use in logging an application, it could be hard to manage
if the number of *note Logger: 1da. instances becomes effectively
unbounded.

* Menu:

* Using LoggerAdapters to impart contextual information::
* Using Filters to impart contextual information::


File: python.info,  Node: Using LoggerAdapters to impart contextual information,  Next: Using Filters to impart contextual information,  Up: Adding contextual information to your logging output

11.8.6.1 Using LoggerAdapters to impart contextual information
..............................................................

An easy way in which you can pass contextual information to be output
along with logging event information is to use the *note LoggerAdapter:
1dc. class.  This class is designed to look like a *note Logger: 1da,
so that you can call *note debug(): 126b, *note info(): 1297, *note
warning(): 12a3, *note error(): 12a4, *note exception(): 12a5, *note
critical(): 12a6. and *note log(): 12a7. These methods have the same
signatures as their counterparts in *note Logger: 1da, so you can use
the two types of instances interchangeably.

  When you create an instance of *note LoggerAdapter: 1dc, you pass it a
*note Logger: 1da. instance and a dict-like object which contains your
contextual information. When you call one of the logging methods on an
instance of *note LoggerAdapter: 1dc, it delegates the call to the
underlying instance of *note Logger: 1da. passed to its constructor,
and arranges to pass the contextual information in the delegated call.
Here's a snippet from the code of *note LoggerAdapter: 1dc.:

    def debug(self, msg, *args, **kwargs):
        """
        Delegate a debug call to the underlying logger, after adding
        contextual information from this adapter instance.
        """
        msg, kwargs = self.process(msg, kwargs)
        self.logger.debug(msg, *args, **kwargs)

The `process()' method of *note LoggerAdapter: 1dc. is where the
contextual information is added to the logging output. It's passed the
message and keyword arguments of the logging call, and it passes back
(potentially) modified versions of these to use in the call to the
underlying logger. The default implementation of this method leaves the
message alone, but inserts an 'extra' key in the keyword argument whose
value is the dict-like object passed to the constructor. Of course, if
you had passed an 'extra' keyword argument in the call to the adapter,
it will be silently overwritten.

  The advantage of using 'extra' is that the values in the dict-like
object are merged into the *note LogRecord: 1279. instance's __dict__,
allowing you to use customized strings with your *note Formatter: 1269.
instances which know about the keys of the dict-like object. If you
need a different method, e.g. if you want to prepend or append the
contextual information to the message string, you just need to subclass
*note LoggerAdapter: 1dc. and override `process()' to do what you need.
Here's an example script which uses this class, which also illustrates
what dict-like behaviour is needed from an arbitrary 'dict-like' object
for use in the constructor:

    import logging

    class ConnInfo:
        """
        An example class which shows how an arbitrary class can be used as
        the 'extra' context information repository passed to a LoggerAdapter.
        """

        def __getitem__(self, name):
            """
            To allow this instance to look like a dict.
            """
            from random import choice
            if name == 'ip':
                result = choice(['127.0.0.1', '192.168.0.1'])
            elif name == 'user':
                result = choice(['jim', 'fred', 'sheila'])
            else:
                result = self.__dict__.get(name, '?')
            return result

        def __iter__(self):
            """
            To allow iteration over keys, which will be merged into
            the LogRecord dict before formatting and output.
            """
            keys = ['ip', 'user']
            keys.extend(self.__dict__.keys())
            return keys.__iter__()

    if __name__ == '__main__':
        from random import choice
        levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)
        a1 = logging.LoggerAdapter(logging.getLogger('a.b.c'),
                                   { 'ip' : '123.231.231.123', 'user' : 'sheila' })
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')
        a1.debug('A debug message')
        a1.info('An info message with %s', 'some parameters')
        a2 = logging.LoggerAdapter(logging.getLogger('d.e.f'), ConnInfo())
        for x in range(10):
            lvl = choice(levels)
            lvlname = logging.getLevelName(lvl)
            a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')

When this script is run, the output should look something like this:

    2008-01-18 14:49:54,023 a.b.c DEBUG    IP: 123.231.231.123 User: sheila   A debug message
    2008-01-18 14:49:54,023 a.b.c INFO     IP: 123.231.231.123 User: sheila   An info message with some parameters
    2008-01-18 14:49:54,023 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f INFO     IP: 192.168.0.1     User: jim      A message at INFO level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f WARNING  IP: 192.168.0.1     User: sheila   A message at WARNING level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f ERROR    IP: 127.0.0.1       User: fred     A message at ERROR level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f WARNING  IP: 192.168.0.1     User: sheila   A message at WARNING level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f WARNING  IP: 192.168.0.1     User: jim      A message at WARNING level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f INFO     IP: 192.168.0.1     User: fred     A message at INFO level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f WARNING  IP: 192.168.0.1     User: sheila   A message at WARNING level with 2 parameters
    2008-01-18 14:49:54,033 d.e.f WARNING  IP: 127.0.0.1       User: jim      A message at WARNING level with 2 parameters



File: python.info,  Node: Using Filters to impart contextual information,  Prev: Using LoggerAdapters to impart contextual information,  Up: Adding contextual information to your logging output

11.8.6.2 Using Filters to impart contextual information
.......................................................

You can also add contextual information to log output using a
user-defined *note Filter: 1295. `Filter' instances are allowed to
modify the `LogRecords' passed to them, including adding additional
attributes which can then be output using a suitable format string, or
if needed a custom *note Formatter: 1269.

  For example in a web application, the request being processed (or at
least, the interesting parts of it) can be stored in a threadlocal
(*note threading.local: 155e.) variable, and then accessed from a
`Filter' to add, say, information from the request - say, the remote IP
address and remote user's username - to the `LogRecord', using the
attribute names 'ip' and 'user' as in the `LoggerAdapter' example
above. In that case, the same format string can be used to get similar
output to that shown above. Here's an example script:

    import logging
    from random import choice

    class ContextFilter(logging.Filter):
        """
        This is a filter which injects contextual information into the log.

        Rather than use actual contextual information, we just use random
        data in this demo.
        """

        USERS = ['jim', 'fred', 'sheila']
        IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']

        def filter(self, record):

            record.ip = choice(ContextFilter.IPS)
            record.user = choice(ContextFilter.USERS)
            return True

    if __name__ == '__main__':
       levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)
       logging.basicConfig(level=logging.DEBUG,
                           format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')
       a1 = logging.getLogger('a.b.c')
       a2 = logging.getLogger('d.e.f')

       f = ContextFilter()
       a1.addFilter(f)
       a2.addFilter(f)
       a1.debug('A debug message')
       a1.info('An info message with %s', 'some parameters')
       for x in range(10):
           lvl = choice(levels)
           lvlname = logging.getLevelName(lvl)
           a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')

which, when run, produces something like:

    2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message
    2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters
    2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters
    2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters



File: python.info,  Node: Logging to a single file from multiple processes,  Next: Using file rotation,  Prev: Adding contextual information to your logging output,  Up: Logging Cookbook

11.8.7 Logging to a single file from multiple processes
-------------------------------------------------------

Although logging is thread-safe, and logging to a single file from
multiple threads in a single process _is_ supported, logging to a
single file from _multiple processes_ is _not_ supported, because there
is no standard way to serialize access to a single file across multiple
processes in Python. If you need to log to a single file from multiple
processes, one way of doing this is to have all the processes log to a
`SocketHandler', and have a separate process which implements a socket
server which reads from the socket and logs to file. (If you prefer,
you can dedicate one thread in one of the existing processes to perform
this function.) The following section documents this approach in more
detail and includes a working socket receiver which can be used as a
starting point for you to adapt in your own applications.

  If you are using a recent version of Python which includes the *note
multiprocessing: 11a. module, you could write your own handler which
uses the `Lock' class from this module to serialize access to the file
from your processes. The existing *note FileHandler: 12d0. and
subclasses do not make use of *note multiprocessing: 11a. at present,
though they may do so in the future.  Note that at present, the *note
multiprocessing: 11a. module does not provide working lock
functionality on all platforms (see <http://bugs.python.org/issue3770>).


File: python.info,  Node: Using file rotation,  Prev: Logging to a single file from multiple processes,  Up: Logging Cookbook

11.8.8 Using file rotation
--------------------------

Sometimes you want to let a log file grow to a certain size, then open
a new file and log to that. You may want to keep a certain number of
these files, and when that many files have been created, rotate the
files so that the number of files and the size of the files both remain
bounded. For this usage pattern, the logging package provides a *note
RotatingFileHandler: 12c2.:

    import glob
    import logging
    import logging.handlers

    LOG_FILENAME = 'logging_rotatingfile_example.out'

    # Set up a specific logger with our desired output level
    my_logger = logging.getLogger('MyLogger')
    my_logger.setLevel(logging.DEBUG)

    # Add the log message handler to the logger
    handler = logging.handlers.RotatingFileHandler(
                  LOG_FILENAME, maxBytes=20, backupCount=5)

    my_logger.addHandler(handler)

    # Log some messages
    for i in range(20):
        my_logger.debug('i = %d' % i)

    # See what files are created
    logfiles = glob.glob('%s*' % LOG_FILENAME)

    for filename in logfiles:
        print(filename)

The result should be 6 separate files, each with part of the log
history for the application:

    logging_rotatingfile_example.out
    logging_rotatingfile_example.out.1
    logging_rotatingfile_example.out.2
    logging_rotatingfile_example.out.3
    logging_rotatingfile_example.out.4
    logging_rotatingfile_example.out.5

The most current file is always `logging_rotatingfile_example.out', and
each time it reaches the size limit it is renamed with the suffix `.1'.
Each of the existing backup files is renamed to increment the suffix
(`.1' becomes `.2', etc.)  and the `.6' file is erased.

  Obviously this example sets the log length much much too small as an
extreme example.  You would want to set _maxBytes_ to an appropriate
value.


File: python.info,  Node: Regular Expression HOWTO,  Next: Socket Programming HOWTO,  Prev: Logging Cookbook,  Up: Python HOWTOs

11.9 Regular Expression HOWTO
=============================

     Author: A.M. Kuchling <<amk@amk.ca>>

Abstract
........

This document is an introductory tutorial to using regular expressions
in Python with the *note re: 144. module.  It provides a gentler
introduction than the corresponding section in the Library Reference.

* Menu:

* Introduction: Introduction<14>.
* Simple Patterns::
* Using Regular Expressions::
* More Pattern Power::
* Modifying Strings::
* Common Problems::
* Feedback::


File: python.info,  Node: Introduction<14>,  Next: Simple Patterns,  Up: Regular Expression HOWTO

11.9.1 Introduction
-------------------

The *note re: 144. module was added in Python 1.5, and provides
Perl-style regular expression patterns.  Earlier versions of Python
came with the `regex' module, which provided Emacs-style patterns.  The
`regex' module was removed completely in Python 2.5.

  Regular expressions (called REs, or regexes, or regex patterns) are
essentially a tiny, highly specialized programming language embedded
inside Python and made available through the *note re: 144. module.
Using this little language, you specify the rules for the set of
possible strings that you want to match; this set might contain English
sentences, or e-mail addresses, or TeX commands, or anything you like.
You can then ask questions such as "Does this string match the
pattern?", or "Is there a match for the pattern anywhere in this
string?".  You can also use REs to modify a string or to split it apart
in various ways.

  Regular expression patterns are compiled into a series of bytecodes
which are then executed by a matching engine written in C.  For
advanced use, it may be necessary to pay careful attention to how the
engine will execute a given RE, and write the RE in a certain way in
order to produce bytecode that runs faster.  Optimization isn't covered
in this document, because it requires that you have a good
understanding of the matching engine's internals.

  The regular expression language is relatively small and restricted,
so not all possible string processing tasks can be done using regular
expressions.  There are also tasks that _can_ be done with regular
expressions, but the expressions turn out to be very complicated.  In
these cases, you may be better off writing Python code to do the
processing; while Python code will be slower than an elaborate regular
expression, it will also probably be more understandable.


File: python.info,  Node: Simple Patterns,  Next: Using Regular Expressions,  Prev: Introduction<14>,  Up: Regular Expression HOWTO

11.9.2 Simple Patterns
----------------------

We'll start by learning about the simplest possible regular
expressions.  Since regular expressions are used to operate on strings,
we'll begin with the most common task: matching characters.

  For a detailed explanation of the computer science underlying regular
expressions (deterministic and non-deterministic finite automata), you
can refer to almost any textbook on writing compilers.

* Menu:

* Matching Characters::
* Repeating Things::


File: python.info,  Node: Matching Characters,  Next: Repeating Things,  Up: Simple Patterns

11.9.2.1 Matching Characters
............................

Most letters and characters will simply match themselves.  For example,
the regular expression `test' will match the string `test' exactly.
(You can enable a case-insensitive mode that would let this RE match
`Test' or `TEST' as well; more about this later.)

  There are exceptions to this rule; some characters are special
_metacharacters_, and don't match themselves.  Instead, they signal that
some out-of-the-ordinary thing should be matched, or they affect other
portions of the RE by repeating them or changing their meaning.  Much
of this document is devoted to discussing various metacharacters and
what they do.

  Here's a complete list of the metacharacters; their meanings will be
discussed in the rest of this HOWTO.

    . ^ $ * + ? { } [ ] \ | ( )

The first metacharacters we'll look at are `[' and `]'. They're used for
specifying a character class, which is a set of characters that you
wish to match.  Characters can be listed individually, or a range of
characters can be indicated by giving two characters and separating
them by a `'-''.  For example, `[abc]' will match any of the characters
`a', `b', or `c'; this is the same as `[a-c]', which uses a range to
express the same set of characters.  If you wanted to match only
lowercase letters, your RE would be `[a-z]'.

  Metacharacters are not active inside classes.  For example, `[akm$]'
will match any of the characters `'a'', `'k'', `'m'', or `'$''; `'$'' is
usually a metacharacter, but inside a character class it's stripped of
its special nature.

  You can match the characters not listed within the class by
_complementing_ the set.  This is indicated by including a `'^'' as the
first character of the class; `'^'' outside a character class will
simply match the `'^'' character.  For example, `[^5]' will match any
character except `'5''.

  Perhaps the most important metacharacter is the backslash, `\'.   As
in Python string literals, the backslash can be followed by various
characters to signal various special sequences.  It's also used to
escape all the metacharacters so you can still match them in patterns;
for example, if you need to match a `[' or  `\', you can precede them
with a backslash to remove their special meaning: `\[' or `\\'.

  Some of the special sequences beginning with `'\'' represent
predefined sets of characters that are often useful, such as the set of
digits, the set of letters, or the set of anything that isn't
whitespace.  The following predefined special sequences are a subset of
those available. The equivalent classes are for byte string patterns.
For a complete list of sequences and expanded class definitions for
Unicode string patterns, see the last part of *note Regular Expression
Syntax: 97a.

`\d'
     Matches any decimal digit; this is equivalent to the class `[0-9]'.

`\D'
     Matches any non-digit character; this is equivalent to the class
     `[^0-9]'.

`\s'
     Matches any whitespace character; this is equivalent to the class
     `[ \t\n\r\f\v]'.

`\S'
     Matches any non-whitespace character; this is equivalent to the
     class `[^ \t\n\r\f\v]'.

`\w'
     Matches any alphanumeric character; this is equivalent to the class
     `[a-zA-Z0-9_]'.

`\W'
     Matches any non-alphanumeric character; this is equivalent to the
     class `[^a-zA-Z0-9_]'.

  These sequences can be included inside a character class.  For
example, `[\s,.]' is a character class that will match any whitespace
character, or `','' or `'.''.

  The final metacharacter in this section is `.'.  It matches anything
except a newline character, and there's an alternate mode (`re.DOTALL')
where it will match even a newline.  `'.'' is often used where you want
to match "any character".


File: python.info,  Node: Repeating Things,  Prev: Matching Characters,  Up: Simple Patterns

11.9.2.2 Repeating Things
.........................

Being able to match varying sets of characters is the first thing
regular expressions can do that isn't already possible with the methods
available on strings.  However, if that was the only additional
capability of regexes, they wouldn't be much of an advance. Another
capability is that you can specify that portions of the RE must be
repeated a certain number of times.

  The first metacharacter for repeating things that we'll look at is
`*'.  `*' doesn't match the literal character `*'; instead, it
specifies that the previous character can be matched zero or more
times, instead of exactly once.

  For example, `ca*t' will match `ct' (0 `a' characters), `cat' (1 `a'),
`caaat' (3 `a' characters), and so forth.  The RE engine has various
internal limitations stemming from the size of C's `int' type that will
prevent it from matching over 2 billion `a' characters; you probably
don't have enough memory to construct a string that large, so you
shouldn't run into that limit.

  Repetitions such as `*' are _greedy_; when repeating a RE, the
matching engine will try to repeat it as many times as possible. If
later portions of the pattern don't match, the matching engine will
then back up and try again with few repetitions.

  A step-by-step example will make this more obvious.  Let's consider
the expression `a[bcd]*b'.  This matches the letter `'a'', zero or more
letters from the class `[bcd]', and finally ends with a `'b''.  Now
imagine matching this RE against the string `abcbd'.

Step       Matched         Explanation
----------------------------------------------------------------- 
1          `a'             The `a' in the RE matches.
2          `abcbd'         The engine matches `[bcd]*', going
                           as far as it can, which is to the
                           end of the string.
3          _Failure_       The engine tries to match `b', but
                           the current position is at the end
                           of the string, so it fails.
4          `abcb'          Back up, so that  `[bcd]*' matches
                           one less character.
5          _Failure_       Try `b' again, but the current
                           position is at the last character,
                           which is a `'d''.
6          `abc'           Back up again, so that `[bcd]*' is
                           only matching `bc'.
6          `abcb'          Try `b' again.  This time the
                           character at the current position is
                           `'b'', so it succeeds.

  The end of the RE has now been reached, and it has matched `abcb'.
This demonstrates how the matching engine goes as far as it can at
first, and if no match is found it will then progressively back up and
retry the rest of the RE again and again.  It will back up until it has
tried zero matches for `[bcd]*', and if that subsequently fails, the
engine will conclude that the string doesn't match the RE at all.

  Another repeating metacharacter is `+', which matches one or more
times.  Pay careful attention to the difference between `*' and `+';
`*' matches _zero_ or more times, so whatever's being repeated may not
be present at all, while `+' requires at least _one_ occurrence.  To
use a similar example, `ca+t' will match `cat' (1 `a'), `caaat' (3
`a''s), but won't match `ct'.

  There are two more repeating qualifiers.  The question mark
character, `?', matches either once or zero times; you can think of it
as marking something as being optional.  For example, `home-?brew'
matches either `homebrew' or `home-brew'.

  The most complicated repeated qualifier is `{m,n}', where _m_ and _n_
are decimal integers.  This qualifier means there must be at least _m_
repetitions, and at most _n_.  For example, `a/{1,3}b' will match
`a/b', `a//b', and `a///b'.  It won't match `ab', which has no slashes,
or `a////b', which has four.

  You can omit either _m_ or _n_; in that case, a reasonable value is
assumed for the missing value.  Omitting _m_ is interpreted as a lower
limit of 0, while omitting _n_ results in an upper bound of infinity --
actually, the upper bound is the 2-billion limit mentioned earlier, but
that might as well be infinity.

  Readers of a reductionist bent may notice that the three other
qualifiers can all be expressed using this notation.  `{0,}' is the
same as `*', `{1,}' is equivalent to `+', and `{0,1}' is the same as
`?'.  It's better to use `*', `+', or `?' when you can, simply because
they're shorter and easier to read.


File: python.info,  Node: Using Regular Expressions,  Next: More Pattern Power,  Prev: Simple Patterns,  Up: Regular Expression HOWTO

11.9.3 Using Regular Expressions
--------------------------------

Now that we've looked at some simple regular expressions, how do we
actually use them in Python?  The *note re: 144. module provides an
interface to the regular expression engine, allowing you to compile REs
into objects and then perform matches with them.

* Menu:

* Compiling Regular Expressions::
* The Backslash Plague::
* Performing Matches::
* Module-Level Functions: Module-Level Functions<2>.
* Compilation Flags::


File: python.info,  Node: Compiling Regular Expressions,  Next: The Backslash Plague,  Up: Using Regular Expressions

11.9.3.1 Compiling Regular Expressions
......................................

Regular expressions are compiled into pattern objects, which have
methods for various operations such as searching for pattern matches or
performing string substitutions.

    >>> import re
    >>> p = re.compile('ab*')
    >>> print p
    <_sre.SRE_Pattern object at 0x...>

*note re.compile(): 987. also accepts an optional _flags_ argument,
used to enable various special features and syntax variations.  We'll
go over the available settings later, but for now a single example will
do:

    >>> p = re.compile('ab*', re.IGNORECASE)

The RE is passed to *note re.compile(): 987. as a string.  REs are
handled as strings because regular expressions aren't part of the core
Python language, and no special syntax was created for expressing them.
(There are applications that don't need REs at all, so there's no need
to bloat the language specification by including them.) Instead, the
*note re: 144. module is simply a C extension module included with
Python, just like the *note socket: 15d. or *note zlib: 1ad. modules.

  Putting REs in strings keeps the Python language simpler, but has one
disadvantage which is the topic of the next section.


File: python.info,  Node: The Backslash Plague,  Next: Performing Matches,  Prev: Compiling Regular Expressions,  Up: Using Regular Expressions

11.9.3.2 The Backslash Plague
.............................

As stated earlier, regular expressions use the backslash character
(`'\'') to indicate special forms or to allow special characters to be
used without invoking their special meaning. This conflicts with
Python's usage of the same character for the same purpose in string
literals.

  Let's say you want to write a RE that matches the string `\section',
which might be found in a LaTeX file.  To figure out what to write in
the program code, start with the desired string to be matched.  Next,
you must escape any backslashes and other metacharacters by preceding
them with a backslash, resulting in the string `\\section'.  The
resulting string that must be passed to *note re.compile(): 987. must
be `\\section'.  However, to express this as a Python string literal,
both backslashes must be escaped _again_.

Characters              Stage
----------------------------------------------------------------------- 
`\section'              Text string to be matched
`\\section'             Escaped backslash for *note re.compile(): 987.
`"\\\\section"'         Escaped backslashes for a string literal

  In short, to match a literal backslash, one has to write `'\\\\'' as
the RE string, because the regular expression must be `\\', and each
backslash must be expressed as `\\' inside a regular Python string
literal.  In REs that feature backslashes repeatedly, this leads to
lots of repeated backslashes and makes the resulting strings difficult
to understand.

  The solution is to use Python's raw string notation for regular
expressions; backslashes are not handled in any special way in a string
literal prefixed with `'r'', so `r"\n"' is a two-character string
containing `'\'' and `'n'', while `"\n"' is a one-character string
containing a newline. Regular expressions will often be written in
Python code using this raw string notation.

Regular String          Raw string
----------------------------------------------- 
`"ab*"'                 `r"ab*"'
`"\\\\section"'         `r"\\section"'
`"\\w+\\s+\\1"'         `r"\w+\s+\1"'


File: python.info,  Node: Performing Matches,  Next: Module-Level Functions<2>,  Prev: The Backslash Plague,  Up: Using Regular Expressions

11.9.3.3 Performing Matches
...........................

Once you have an object representing a compiled regular expression,
what do you do with it?  Pattern objects have several methods and
attributes.  Only the most significant ones will be covered here;
consult the *note re: 144. docs for a complete listing.

Method/Attribute       Purpose
--------------------------------------------------------------------------- 
`match()'              Determine if the RE matches at the beginning of
                       the string.
`search()'             Scan through a string, looking for any location
                       where this RE matches.
`findall()'            Find all substrings where the RE matches, and
                       returns them as a list.
`finditer()'           Find all substrings where the RE matches, and
                       returns them as an *note iterator: 84a.

  `match()' and `search()' return `None' if no match can be found.  If
they're successful, a `MatchObject' instance is returned, containing
information about the match: where it starts and ends, the substring it
matched, and more.

  You can learn about this by interactively experimenting with the
*note re: 144.  module.  If you have Tkinter available, you may also
want to look at `Tools/scripts/redemo.py', a demonstration program
included with the Python distribution.  It allows you to enter REs and
strings, and displays whether the RE matches or fails. `redemo.py' can
be quite useful when trying to debug a complicated RE.  Phil Schwartz's
Kodos(1) is also an interactive tool for developing and testing RE
patterns.

  This HOWTO uses the standard Python interpreter for its examples.
First, run the Python interpreter, import the *note re: 144. module,
and compile a RE:

    Python 2.2.2 (#1, Feb 10 2003, 12:57:01)
    >>> import re
    >>> p = re.compile('[a-z]+')
    >>> p
    <_sre.SRE_Pattern object at 0x...>

Now, you can try matching various strings against the RE `[a-z]+'.  An
empty string shouldn't match at all, since `+' means 'one or more
repetitions'.  `match()' should return `None' in this case, which will
cause the interpreter to print no output.  You can explicitly print the
result of `match()' to make this clear.

    >>> p.match("")
    >>> print p.match("")
    None

Now, let's try it on a string that it should match, such as `tempo'.
In this case, `match()' will return a `MatchObject', so you should
store the result in a variable for later use.

    >>> m = p.match('tempo')
    >>> print m
    <_sre.SRE_Match object at 0x...>

Now you can query the `MatchObject' for information about the matching
string.   `MatchObject' instances also have several methods and
attributes; the most important ones are:

Method/Attribute       Purpose
------------------------------------------------------------------------ 
`group()'              Return the string matched by the RE
`start()'              Return the starting position of the match
`end()'                Return the ending position of the match
`span()'               Return a tuple containing the (start, end)
                       positions  of the match

  Trying these methods will soon clarify their meaning:

    >>> m.group()
    'tempo'
    >>> m.start(), m.end()
    (0, 5)
    >>> m.span()
    (0, 5)

`group()' returns the substring that was matched by the RE.  `start()'
and `end()' return the starting and ending index of the match. `span()'
returns both start and end indexes in a single tuple.  Since the
`match()' method only checks if the RE matches at the start of a
string, `start()' will always be zero.  However, the `search()' method
of patterns scans through the string, so  the match may not start at
zero in that case.

    >>> print p.match('::: message')
    None
    >>> m = p.search('::: message') ; print m
    <_sre.SRE_Match object at 0x...>
    >>> m.group()
    'message'
    >>> m.span()
    (4, 11)

In actual programs, the most common style is to store the `MatchObject'
in a variable, and then check if it was `None'.  This usually looks
like:

    p = re.compile( ... )
    m = p.match( 'string goes here' )
    if m:
        print 'Match found: ', m.group()
    else:
        print 'No match'

Two pattern methods return all of the matches for a pattern.
`findall()' returns a list of matching strings:

    >>> p = re.compile('\d+')
    >>> p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')
    ['12', '11', '10']

`findall()' has to create the entire list before it can be returned as
the result.  The `finditer()' method returns a sequence of `MatchObject'
instances as an *note iterator: 84a. (2)

    >>> iterator = p.finditer('12 drummers drumming, 11 ... 10 ...')
    >>> iterator
    <callable-iterator object at 0x401833ac>
    >>> for match in iterator:
    ...     print match.span()
    ...
    (0, 2)
    (22, 24)
    (29, 31)


  ---------- Footnotes ----------

  (1) http://kodos.sourceforge.net/

  (2) Introduced in Python 2.2.2.


File: python.info,  Node: Module-Level Functions<2>,  Next: Compilation Flags,  Prev: Performing Matches,  Up: Using Regular Expressions

11.9.3.4 Module-Level Functions
...............................

You don't have to create a pattern object and call its methods; the
*note re: 144. module also provides top-level functions called
`match()', `search()', `findall()', `sub()', and so forth.  These
functions take the same arguments as the corresponding pattern method,
with the RE string added as the first argument, and still return either
`None' or a `MatchObject' instance.

    >>> print re.match(r'From\s+', 'Fromage amk')
    None
    >>> re.match(r'From\s+', 'From amk Thu May 14 19:12:10 1998')
    <_sre.SRE_Match object at 0x...>

Under the hood, these functions simply create a pattern object for you
and call the appropriate method on it.  They also store the compiled
object in a cache, so future calls using the same RE are faster.

  Should you use these module-level functions, or should you get the
pattern and call its methods yourself?  That choice depends on how
frequently the RE will be used, and on your personal coding style.  If
the RE is being used at only one point in the code, then the module
functions are probably more convenient.  If a program contains a lot of
regular expressions, or re-uses the same ones in several locations,
then it might be worthwhile to collect all the definitions in one
place, in a section of code that compiles all the REs ahead of time.
To take an example from the standard library, here's an extract from
`xmllib.py':

    ref = re.compile( ... )
    entityref = re.compile( ... )
    charref = re.compile( ... )
    starttagopen = re.compile( ... )

I generally prefer to work with the compiled object, even for one-time
uses, but few people will be as much of a purist about this as I am.


File: python.info,  Node: Compilation Flags,  Prev: Module-Level Functions<2>,  Up: Using Regular Expressions

11.9.3.5 Compilation Flags
..........................

Compilation flags let you modify some aspects of how regular
expressions work.  Flags are available in the *note re: 144. module
under two names, a long name such as `IGNORECASE' and a short,
one-letter form such as `I'.  (If you're familiar with Perl's pattern
modifiers, the one-letter forms use the same letters; the short form of
*note re.VERBOSE: 98e. is *note re.X: 985, for example.)  Multiple
flags can be specified by bitwise OR-ing them; `re.I | re.M' sets both
the `I' and `M' flags, for example.

  Here's a table of the available flags, followed by a more detailed
explanation of each one.

Flag                                  Meaning
--------------------------------------------------------------------------------------- 
`DOTALL', `S'                         Make `.' match any character, including newlines
`IGNORECASE', `I'                     Do case-insensitive matches
`LOCALE', `L'                         Do a locale-aware match
`MULTILINE', `M'                      Multi-line matching, affecting `^' and `$'
`VERBOSE', `X'                        Enable verbose REs, which can be organized more
                                      cleanly and understandably.
`UNICODE', `U'                        Makes several escapes like `\w', `\b', `\s' and
                                      `\d' dependent on the Unicode character
                                      database.

 -- Data: I
 -- Data: IGNORECASE
     Perform case-insensitive matching; character class and literal
     strings will match letters by ignoring case.  For example, `[A-Z]'
     will match lowercase letters, too, and `Spam' will match `Spam',
     `spam', or `spAM'. This lowercasing doesn't take the current
     locale into account; it will if you also set the `LOCALE' flag.

 -- Data: L
 -- Data: LOCALE
     Make `\w', `\W', `\b', and `\B', dependent on the current locale.

     Locales are a feature of the C library intended to help in writing
     programs that take account of language differences.  For example,
     if you're processing French text, you'd want to be able to write
     `\w+' to match words, but `\w' only matches the character class
     `[A-Za-z]'; it won't match `''' or `'''.  If your system is
     configured properly and a French locale is selected, certain C
     functions will tell the program that `''' should also be
     considered a letter.  Setting the `LOCALE' flag when compiling a
     regular expression will cause the resulting compiled object to use
     these C functions for `\w'; this is slower, but also enables `\w+'
     to match French words as you'd expect.

 -- Data: M
 -- Data: MULTILINE
     (`^' and `$' haven't been explained yet;  they'll be introduced in
     section *note More Metacharacters: 2fbb.)

     Usually `^' matches only at the beginning of the string, and `$'
     matches only at the end of the string and immediately before the
     newline (if any) at the end of the string. When this flag is
     specified, `^' matches at the beginning of the string and at the
     beginning of each line within the string, immediately following
     each newline.  Similarly, the `$' metacharacter matches either at
     the end of the string and at the end of each line (immediately
     preceding each newline).

 -- Data: S
 -- Data: DOTALL
     Makes the `'.'' special character match any character at all,
     including a newline; without this flag, `'.'' will match anything
     _except_ a newline.

 -- Data: U
 -- Data: UNICODE
     Make `\w', `\W', `\b', `\B', `\d', `\D', `\s' and `\S' dependent
     on the Unicode character properties database.

 -- Data: X
 -- Data: VERBOSE
     This flag allows you to write regular expressions that are more
     readable by granting you more flexibility in how you can format
     them.  When this flag has been specified, whitespace within the RE
     string is ignored, except when the whitespace is in a character
     class or preceded by an unescaped backslash; this lets you
     organize and indent the RE more clearly.  This flag also lets you
     put comments within a RE that will be ignored by the engine;
     comments are marked by a `'#'' that's neither in a character class
     or preceded by an unescaped backslash.

     For example, here's a RE that uses *note re.VERBOSE: 98e.; see how
     much easier it is to read?

         charref = re.compile(r"""
          &[#]                # Start of a numeric entity reference
          (
              0[0-7]+         # Octal form
            | [0-9]+          # Decimal form
            | x[0-9a-fA-F]+   # Hexadecimal form
          )
          ;                   # Trailing semicolon
         """, re.VERBOSE)

     Without the verbose setting, the RE would look like this:

         charref = re.compile("&#(0[0-7]+"
                              "|[0-9]+"
                              "|x[0-9a-fA-F]+);")

     In the above example, Python's automatic concatenation of string
     literals has been used to break up the RE into smaller pieces, but
     it's still more difficult to understand than the version using
     *note re.VERBOSE: 98e.


File: python.info,  Node: More Pattern Power,  Next: Modifying Strings,  Prev: Using Regular Expressions,  Up: Regular Expression HOWTO

11.9.4 More Pattern Power
-------------------------

So far we've only covered a part of the features of regular
expressions.  In this section, we'll cover some new metacharacters, and
how to use groups to retrieve portions of the text that was matched.

* Menu:

* More Metacharacters::
* Grouping::
* Non-capturing and Named Groups::
* Lookahead Assertions::


File: python.info,  Node: More Metacharacters,  Next: Grouping,  Up: More Pattern Power

11.9.4.1 More Metacharacters
............................

There are some metacharacters that we haven't covered yet.  Most of
them will be covered in this section.

  Some of the remaining metacharacters to be discussed are _zero-width
assertions_.  They don't cause the engine to advance through the string;
instead, they consume no characters at all, and simply succeed or fail.
For example, `\b' is an assertion that the current position is located
at a word boundary; the position isn't changed by the `\b' at all.
This means that zero-width assertions should never be repeated, because
if they match once at a given location, they can obviously be matched
an infinite number of times.

`|'
     Alternation, or the "or" operator.   If A and B are regular
     expressions, `A|B' will match any string that matches either `A'
     or `B'. `|' has very low precedence in order to make it work
     reasonably when you're alternating multi-character strings.
     `Crow|Servo' will match either `Crow' or `Servo', not `Cro', a
     `'w'' or an `'S'', and `ervo'.

     To match a literal `'|'', use `\|', or enclose it inside a
     character class, as in `[|]'.

`^'
     Matches at the beginning of lines.  Unless the `MULTILINE' flag
     has been set, this will only match at the beginning of the string.
     In `MULTILINE' mode, this also matches immediately after each
     newline within the string.

     For example, if you wish to match the word `From' only at the
     beginning of a line, the RE to use is `^From'.

         >>> print re.search('^From', 'From Here to Eternity')
         <_sre.SRE_Match object at 0x...>
         >>> print re.search('^From', 'Reciting From Memory')
         None


`$'
     Matches at the end of a line, which is defined as either the end
     of the string, or any location followed by a newline character.

         >>> print re.search('}$', '{block}')
         <_sre.SRE_Match object at 0x...>
         >>> print re.search('}$', '{block} ')
         None
         >>> print re.search('}$', '{block}\n')
         <_sre.SRE_Match object at 0x...>

     To match a literal `'$'', use `\$' or enclose it inside a
     character class, as in  `[$]'.

`\A'
     Matches only at the start of the string.  When not in `MULTILINE'
     mode, `\A' and `^' are effectively the same.  In `MULTILINE' mode,
     they're different: `\A' still matches only at the beginning of the
     string, but `^' may match at any location inside the string that
     follows a newline character.

`\Z'
     Matches only at the end of the string.

`\b'
     Word boundary.  This is a zero-width assertion that matches only
     at the beginning or end of a word.  A word is defined as a
     sequence of alphanumeric characters, so the end of a word is
     indicated by whitespace or a non-alphanumeric character.

     The following example matches `class' only when it's a complete
     word; it won't match when it's contained inside another word.

         >>> p = re.compile(r'\bclass\b')
         >>> print p.search('no class at all')
         <_sre.SRE_Match object at 0x...>
         >>> print p.search('the declassified algorithm')
         None
         >>> print p.search('one subclass is')
         None

     There are two subtleties you should remember when using this
     special sequence.  First, this is the worst collision between
     Python's string literals and regular expression sequences.  In
     Python's string literals, `\b' is the backspace character, ASCII
     value 8.  If you're not using raw strings, then Python will
     convert the `\b' to a backspace, and your RE won't match as you
     expect it to.  The following example looks the same as our
     previous RE, but omits the `'r'' in front of the RE string.

         >>> p = re.compile('\bclass\b')
         >>> print p.search('no class at all')
         None
         >>> print p.search('\b' + 'class' + '\b')
         <_sre.SRE_Match object at 0x...>

     Second, inside a character class, where there's no use for this
     assertion, `\b' represents the backspace character, for
     compatibility with Python's string literals.

`\B'
     Another zero-width assertion, this is the opposite of `\b', only
     matching when the current position is not at a word boundary.


File: python.info,  Node: Grouping,  Next: Non-capturing and Named Groups,  Prev: More Metacharacters,  Up: More Pattern Power

11.9.4.2 Grouping
.................

Frequently you need to obtain more information than just whether the RE
matched or not.  Regular expressions are often used to dissect strings
by writing a RE divided into several subgroups which match different
components of interest.  For example, an RFC-822 header line is divided
into a header name and a value, separated by a `':'', like this:

    From: author@example.com
    User-Agent: Thunderbird 1.5.0.9 (X11/20061227)
    MIME-Version: 1.0
    To: editor@example.com

This can be handled by writing a regular expression which matches an
entire header line, and has one group which matches the header name,
and another group which matches the header's value.

  Groups are marked by the `'('', `')'' metacharacters. `'('' and `')''
have much the same meaning as they do in mathematical expressions; they
group together the expressions contained inside them, and you can
repeat the contents of a group with a repeating qualifier, such as `*',
`+', `?', or `{m,n}'.  For example, `(ab)*' will match zero or more
repetitions of `ab'.

    >>> p = re.compile('(ab)*')
    >>> print p.match('ababababab').span()
    (0, 10)

Groups indicated with `'('', `')'' also capture the starting and ending
index of the text that they match; this can be retrieved by passing an
argument to `group()', `start()', `end()', and `span()'.  Groups are
numbered starting with 0.  Group 0 is always present; it's the whole
RE, so `MatchObject' methods all have group 0 as their default
argument.  Later we'll see how to express groups that don't capture the
span of text that they match.

    >>> p = re.compile('(a)b')
    >>> m = p.match('ab')
    >>> m.group()
    'ab'
    >>> m.group(0)
    'ab'

Subgroups are numbered from left to right, from 1 upward.  Groups can
be nested; to determine the number, just count the opening parenthesis
characters, going from left to right.

    >>> p = re.compile('(a(b)c)d')
    >>> m = p.match('abcd')
    >>> m.group(0)
    'abcd'
    >>> m.group(1)
    'abc'
    >>> m.group(2)
    'b'

`group()' can be passed multiple group numbers at a time, in which case
it will return a tuple containing the corresponding values for those
groups.

    >>> m.group(2,1,2)
    ('b', 'abc', 'b')

The `groups()' method returns a tuple containing the strings for all the
subgroups, from 1 up to however many there are.

    >>> m.groups()
    ('abc', 'b')

Backreferences in a pattern allow you to specify that the contents of
an earlier capturing group must also be found at the current location
in the string.  For example, `\1' will succeed if the exact contents of
group 1 can be found at the current position, and fails otherwise.
Remember that Python's string literals also use a backslash followed by
numbers to allow including arbitrary characters in a string, so be sure
to use a raw string when incorporating backreferences in a RE.

  For example, the following RE detects doubled words in a string.

    >>> p = re.compile(r'(\b\w+)\s+\1')
    >>> p.search('Paris in the the spring').group()
    'the the'

Backreferences like this aren't often useful for just searching through
a string -- there are few text formats which repeat data in this way --
but you'll soon find out that they're _very_ useful when performing
string substitutions.


File: python.info,  Node: Non-capturing and Named Groups,  Next: Lookahead Assertions,  Prev: Grouping,  Up: More Pattern Power

11.9.4.3 Non-capturing and Named Groups
.......................................

Elaborate REs may use many groups, both to capture substrings of
interest, and to group and structure the RE itself.  In complex REs, it
becomes difficult to keep track of the group numbers.  There are two
features which help with this problem.  Both of them use a common
syntax for regular expression extensions, so we'll look at that first.

  Perl 5 added several additional features to standard regular
expressions, and the Python *note re: 144. module supports most of
them.   It would have been difficult to choose new single-keystroke
metacharacters or new special sequences beginning with `\' to represent
the new features without making Perl's regular expressions confusingly
different from standard REs.  If you chose `&' as a new metacharacter,
for example, old expressions would be assuming that `&' was a regular
character and wouldn't have escaped it by writing `\&' or `[&]'.

  The solution chosen by the Perl developers was to use `(?...)' as the
extension syntax.  `?' immediately after a parenthesis was a syntax
error because the `?' would have nothing to repeat, so this didn't
introduce any compatibility problems.  The characters immediately after
the `?'  indicate what extension is being used, so `(?=foo)' is one
thing (a positive lookahead assertion) and `(?:foo)' is something else
(a non-capturing group containing the subexpression `foo').

  Python adds an extension syntax to Perl's extension syntax.  If the
first character after the question mark is a `P', you know that it's an
extension that's specific to Python.  Currently there are two such
extensions: `(?P<name>...)' defines a named group, and `(?P=name)' is a
backreference to a named group.  If future versions of Perl 5 add
similar features using a different syntax, the *note re: 144. module
will be changed to support the new syntax, while preserving the
Python-specific syntax for compatibility's sake.

  Now that we've looked at the general extension syntax, we can return
to the features that simplify working with groups in complex REs. Since
groups are numbered from left to right and a complex expression may use
many groups, it can become difficult to keep track of the correct
numbering.  Modifying such a complex RE is annoying, too: insert a new
group near the beginning and you change the numbers of everything that
follows it.

  Sometimes you'll want to use a group to collect a part of a regular
expression, but aren't interested in retrieving the group's contents.
You can make this fact explicit by using a non-capturing group:
`(?:...)', where you can replace the `...' with any other regular
expression.

    >>> m = re.match("([abc])+", "abc")
    >>> m.groups()
    ('c',)
    >>> m = re.match("(?:[abc])+", "abc")
    >>> m.groups()
    ()

Except for the fact that you can't retrieve the contents of what the
group matched, a non-capturing group behaves exactly the same as a
capturing group; you can put anything inside it, repeat it with a
repetition metacharacter such as `*', and nest it within other groups
(capturing or non-capturing).  `(?:...)' is particularly useful when
modifying an existing pattern, since you can add new groups without
changing how all the other groups are numbered.  It should be mentioned
that there's no performance difference in searching between capturing
and non-capturing groups; neither form is any faster than the other.

  A more significant feature is named groups: instead of referring to
them by numbers, groups can be referenced by a name.

  The syntax for a named group is one of the Python-specific extensions:
`(?P<name>...)'.  _name_ is, obviously, the name of the group.  Named
groups also behave exactly like capturing groups, and additionally
associate a name with a group.  The `MatchObject' methods that deal
with capturing groups all accept either integers that refer to the
group by number or strings that contain the desired group's name.
Named groups are still given numbers, so you can retrieve information
about a group in two ways:

    >>> p = re.compile(r'(?P<word>\b\w+\b)')
    >>> m = p.search( '(((( Lots of punctuation )))' )
    >>> m.group('word')
    'Lots'
    >>> m.group(1)
    'Lots'

Named groups are handy because they let you use easily-remembered
names, instead of having to remember numbers.  Here's an example RE
from the *note imaplib: f3.  module:

    InternalDate = re.compile(r'INTERNALDATE "'
            r'(?P<day>[ 123][0-9])-(?P<mon>[A-Z][a-z][a-z])-'
            r'(?P<year>[0-9][0-9][0-9][0-9])'
            r' (?P<hour>[0-9][0-9]):(?P<min>[0-9][0-9]):(?P<sec>[0-9][0-9])'
            r' (?P<zonen>[-+])(?P<zoneh>[0-9][0-9])(?P<zonem>[0-9][0-9])'
            r'"')

It's obviously much easier to retrieve `m.group('zonem')', instead of
having to remember to retrieve group 9.

  The syntax for backreferences in an expression such as `(...)\1'
refers to the number of the group.  There's naturally a variant that
uses the group name instead of the number. This is another Python
extension: `(?P=name)' indicates that the contents of the group called
_name_ should again be matched at the current point.  The regular
expression for finding doubled words, `(\b\w+)\s+\1' can also be
written as `(?P<word>\b\w+)\s+(?P=word)':

    >>> p = re.compile(r'(?P<word>\b\w+)\s+(?P=word)')
    >>> p.search('Paris in the the spring').group()
    'the the'



File: python.info,  Node: Lookahead Assertions,  Prev: Non-capturing and Named Groups,  Up: More Pattern Power

11.9.4.4 Lookahead Assertions
.............................

Another zero-width assertion is the lookahead assertion.  Lookahead
assertions are available in both positive and negative form, and  look
like this:

`(?=...)'
     Positive lookahead assertion.  This succeeds if the contained
     regular expression, represented here by `...', successfully
     matches at the current location, and fails otherwise. But, once
     the contained expression has been tried, the matching engine
     doesn't advance at all; the rest of the pattern is tried right
     where the assertion started.

`(?!...)'
     Negative lookahead assertion.  This is the opposite of the
     positive assertion; it succeeds if the contained expression
     _doesn't_ match at the current position in the string.

  To make this concrete, let's look at a case where a lookahead is
useful.  Consider a simple pattern to match a filename and split it
apart into a base name and an extension, separated by a `.'.  For
example, in `news.rc', `news' is the base name, and `rc' is the
filename's extension.

  The pattern to match this is quite simple:

  `.*[.].*$'

  Notice that the `.' needs to be treated specially because it's a
metacharacter; I've put it inside a character class.  Also notice the
trailing `$'; this is added to ensure that all the rest of the string
must be included in the extension.  This regular expression matches
`foo.bar' and `autoexec.bat' and `sendmail.cf' and `printers.conf'.

  Now, consider complicating the problem a bit; what if you want to
match filenames where the extension is not `bat'? Some incorrect
attempts:

  `.*[.][^b].*$'  The first attempt above tries to exclude `bat' by
requiring that the first character of the extension is not a `b'.  This
is wrong, because the pattern also doesn't match `foo.bar'.

  `.*[.]([^b]..|.[^a].|..[^t])$'

  The expression gets messier when you try to patch up the first
solution by requiring one of the following cases to match: the first
character of the extension isn't `b'; the second character isn't `a';
or the third character isn't `t'.  This accepts `foo.bar' and rejects
`autoexec.bat', but it requires a three-letter extension and won't
accept a filename with a two-letter extension such as `sendmail.cf'.
We'll complicate the pattern again in an effort to fix it.

  `.*[.]([^b].?.?|.[^a]?.?|..?[^t]?)$'

  In the third attempt, the second and third letters are all made
optional in order to allow matching extensions shorter than three
characters, such as `sendmail.cf'.

  The pattern's getting really complicated now, which makes it hard to
read and understand.  Worse, if the problem changes and you want to
exclude both `bat' and `exe' as extensions, the pattern would get even
more complicated and confusing.

  A negative lookahead cuts through all this confusion:

  `.*[.](?!bat$).*$'  The negative lookahead means: if the expression
`bat' doesn't match at this point, try the rest of the pattern; if
`bat$' does match, the whole pattern will fail.  The trailing `$' is
required to ensure that something like `sample.batch', where the
extension only starts with `bat', will be allowed.

  Excluding another filename extension is now easy; simply add it as an
alternative inside the assertion.  The following pattern excludes
filenames that end in either `bat' or `exe':

  `.*[.](?!bat$|exe$).*$'


File: python.info,  Node: Modifying Strings,  Next: Common Problems,  Prev: More Pattern Power,  Up: Regular Expression HOWTO

11.9.5 Modifying Strings
------------------------

Up to this point, we've simply performed searches against a static
string.  Regular expressions are also commonly used to modify strings
in various ways, using the following pattern methods:

Method/Attribute       Purpose
--------------------------------------------------------------------------- 
`split()'              Split the string into a list, splitting it
                       wherever the RE matches
`sub()'                Find all substrings where the RE matches, and
                       replace them with a different string
`subn()'               Does the same thing as `sub()',  but returns the
                       new string and the number of replacements

* Menu:

* Splitting Strings::
* Search and Replace::


File: python.info,  Node: Splitting Strings,  Next: Search and Replace,  Up: Modifying Strings

11.9.5.1 Splitting Strings
..........................

The `split()' method of a pattern splits a string apart wherever the RE
matches, returning a list of the pieces. It's similar to the `split()'
method of strings but provides much more generality in the delimiters
that you can split by; `split()' only supports splitting by whitespace
or by a fixed string.  As you'd expect, there's a module-level *note
re.split(): 244. function, too.

 -- Method: .split (string[, maxsplit=0])
     Split _string_ by the matches of the regular expression.  If
     capturing parentheses are used in the RE, then their contents will
     also be returned as part of the resulting list.  If _maxsplit_ is
     nonzero, at most _maxsplit_ splits are performed.

  You can limit the number of splits made, by passing a value for
_maxsplit_.  When _maxsplit_ is nonzero, at most _maxsplit_ splits will
be made, and the remainder of the string is returned as the final
element of the list.  In the following example, the delimiter is any
sequence of non-alphanumeric characters.

    >>> p = re.compile(r'\W+')
    >>> p.split('This is a test, short and sweet, of split().')
    ['This', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']
    >>> p.split('This is a test, short and sweet, of split().', 3)
    ['This', 'is', 'a', 'test, short and sweet, of split().']

Sometimes you're not only interested in what the text between
delimiters is, but also need to know what the delimiter was.  If
capturing parentheses are used in the RE, then their values are also
returned as part of the list.  Compare the following calls:

    >>> p = re.compile(r'\W+')
    >>> p2 = re.compile(r'(\W+)')
    >>> p.split('This... is a test.')
    ['This', 'is', 'a', 'test', '']
    >>> p2.split('This... is a test.')
    ['This', '... ', 'is', ' ', 'a', ' ', 'test', '.', '']

The module-level function *note re.split(): 244. adds the RE to be used
as the first argument, but is otherwise the same.

    >>> re.split('[\W]+', 'Words, words, words.')
    ['Words', 'words', 'words', '']
    >>> re.split('([\W]+)', 'Words, words, words.')
    ['Words', ', ', 'words', ', ', 'words', '.', '']
    >>> re.split('[\W]+', 'Words, words, words.', 1)
    ['Words', 'words, words.']


