This is python.info, produced by makeinfo version 4.8 from
build/texinfo/python.texi.

Generated by Sphinx 1.1pre.
INFO-DIR-SECTION Documentation tools
START-INFO-DIR-ENTRY
* Python: (python.info). The Python Programming Language
END-INFO-DIR-ENTRY

     Python 2.7.2, April 02, 2012

     Georg Brandl

     Copyright (C) 1990-2012, Python Software Foundation


File: python.info,  Node: Miscellaneous<2>,  Next: Connection Objects<2>,  Prev: Pipes and Queues,  Up: Reference

5.16.6.10 Miscellaneous
.......................

 -- Function: multiprocessing.active_children ()
     Return list of all live children of the current process.

     Calling this has the side affect of "joining" any processes which
     have already finished.

 -- Function: multiprocessing.cpu_count ()
     Return the number of CPUs in the system.  May raise *note
     NotImplementedError: 919.

 -- Function: multiprocessing.current_process ()
     Return the *note Process: 15ab. object corresponding to the
     current process.

     An analogue of *note threading.current_thread(): 155a.

 -- Function: multiprocessing.freeze_support ()
     Add support for when a program which uses *note multiprocessing:
     11a. has been frozen to produce a Windows executable.  (Has been
     tested with *py2exe*, *PyInstaller* and *cx_Freeze*.)

     One needs to call this function straight after the `if __name__ ==
     '__main__'' line of the main module.  For example:

         from multiprocessing import Process, freeze_support

         def f():
             print 'hello world!'

         if __name__ == '__main__':
             freeze_support()
             Process(target=f).start()

     If the `freeze_support()' line is omitted then trying to run the
     frozen executable will raise *note RuntimeError: 38a.

     If the module is being run normally by the Python interpreter then
     *note freeze_support(): 15e1. has no effect.

 -- Function: multiprocessing.set_executable ()
     Sets the path of the Python interpreter to use when starting a
     child process.  (By default *note sys.executable: 15e3. is used).
     Embedders will probably need to do some thing like

         setExecutable(os.path.join(sys.exec_prefix, 'pythonw.exe'))

     before they can create child processes.  (Windows only)

     Note: *note multiprocessing: 11a. contains no analogues of *note
     threading.active_count(): 1555, *note threading.enumerate(): 1558,
     *note threading.settrace(): 1567, *note threading.setprofile():
     1568, *note threading.Timer: b80, or *note threading.local: 155e.


File: python.info,  Node: Connection Objects<2>,  Next: Synchronization primitives,  Prev: Miscellaneous<2>,  Up: Reference

5.16.6.11 Connection Objects
............................

Connection objects allow the sending and receiving of picklable objects
or strings.  They can be thought of as message oriented connected
sockets.

  Connection objects usually created using *note Pipe(): 15ae. - see
also *note Listeners and Clients: 15e5.

 -- Class: multiprocessing.Connection
      -- Method: send (obj)
          Send an object to the other end of the connection which
          should be read using *note recv(): 15b0.

          The object must be picklable.  Very large pickles
          (approximately 32 MB+, though it depends on the OS) may raise
          a ValueError exception.

      -- Method: recv ()
          Return an object sent from the other end of the connection
          using *note send(): 15af.  Raises *note EOFError: 854. if
          there is nothing left to receive and the other end was closed.

      -- Method: fileno ()
          Returns the file descriptor or handle used by the connection.

      -- Method: close ()
          Close the connection.

          This is called automatically when the connection is garbage
          collected.

      -- Method: poll ([timeout])
          Return whether there is any data available to be read.

          If _timeout_ is not specified then it will return
          immediately.  If _timeout_ is a number then this specifies
          the maximum time in seconds to block.  If _timeout_ is `None'
          then an infinite timeout is used.

      -- Method: send_bytes (buffer[, offset[, size]])
          Send byte data from an object supporting the buffer interface
          as a complete message.

          If _offset_ is given then data is read from that position in
          _buffer_.  If _size_ is given then that many bytes will be
          read from buffer.  Very large buffers (approximately 32 MB+,
          though it depends on the OS) may raise a ValueError exception

      -- Method: recv_bytes ([maxlength])
          Return a complete message of byte data sent from the other
          end of the connection as a string.  Raises *note EOFError:
          854. if there is nothing left to receive and the other end
          has closed.

          If _maxlength_ is specified and the message is longer than
          _maxlength_ then *note IOError: 1f7. is raised and the
          connection will no longer be readable.

      -- Method: recv_bytes_into (buffer[, offset])
          Read into _buffer_ a complete message of byte data sent from
          the other end of the connection and return the number of
          bytes in the message.  Raises *note EOFError: 854. if there
          is nothing left to receive and the other end was closed.

          _buffer_ must be an object satisfying the writable buffer
          interface.  If _offset_ is given then the message will be
          written into the buffer from that position.  Offset must be a
          non-negative integer less than the length of _buffer_ (in
          bytes).

          If the buffer is too short then a *note BufferTooShort: 15c8.
          exception is raised and the complete message is available as
          `e.args[0]' where `e' is the exception instance.

  For example:

    >>> from multiprocessing import Pipe
    >>> a, b = Pipe()
    >>> a.send([1, 'hello', None])
    >>> b.recv()
    [1, 'hello', None]
    >>> b.send_bytes('thank you')
    >>> a.recv_bytes()
    'thank you'
    >>> import array
    >>> arr1 = array.array('i', range(5))
    >>> arr2 = array.array('i', [0] * 10)
    >>> a.send_bytes(arr1)
    >>> count = b.recv_bytes_into(arr2)
    >>> assert count == len(arr1) * arr1.itemsize
    >>> arr2
    array('i', [0, 1, 2, 3, 4, 0, 0, 0, 0, 0])


     Warning: The *note Connection.recv(): 15b0. method automatically
     unpickles the data it receives, which can be a security risk
     unless you can trust the process which sent the message.

     Therefore, unless the connection object was produced using *note
     Pipe(): 15ae. you should only use the *note recv(): 15b0. and
     *note send(): 15af.  methods after performing some sort of
     authentication.  See *note Authentication keys: 15c6.

     Warning: If a process is killed while it is trying to read or
     write to a pipe then the data in the pipe is likely to become
     corrupted, because it may become impossible to be sure where the
     message boundaries lie.


File: python.info,  Node: Synchronization primitives,  Next: Shared ctypes Objects,  Prev: Connection Objects<2>,  Up: Reference

5.16.6.12 Synchronization primitives
....................................

Generally synchronization primitives are not as necessary in a
multiprocess program as they are in a multithreaded program.  See the
documentation for *note threading: 17a. module.

  Note that one can also create synchronization primitives by using a
manager object - see *note Managers: 15cd.

 -- Class: multiprocessing.BoundedSemaphore ([value])
     A bounded semaphore object: a clone of *note
     threading.BoundedSemaphore: 1564.

     (On Mac OS X, this is indistinguishable from *note Semaphore:
     15b7. because `sem_getvalue()' is not implemented on that
     platform).

 -- Class: multiprocessing.Condition ([lock])
     A condition variable: a clone of *note threading.Condition: 157f.

     If _lock_ is specified then it should be a *note Lock: 15b5. or
     *note RLock: 15b6.  object from *note multiprocessing: 11a.

 -- Class: multiprocessing.Event
     A clone of *note threading.Event: 265.  This method returns the
     state of the internal semaphore on exit, so it will always return
     `True' except if a timeout is given and the operation times out.

     Changed in version 2.7: Previously, the method always returned
     `None'.

 -- Class: multiprocessing.Lock
     A non-recursive lock object: a clone of *note threading.Lock: 155f.

 -- Class: multiprocessing.RLock
     A recursive lock object: a clone of *note threading.RLock: 1561.

 -- Class: multiprocessing.Semaphore ([value])
     A semaphore object: a clone of *note threading.Semaphore: 1587.

     Note: The `acquire()' method of *note BoundedSemaphore: 15b8,
     *note Lock: 15b5, *note RLock: 15b6. and *note Semaphore: 15b7.
     has a timeout parameter not supported by the equivalents in *note
     threading: 17a.  The signature is `acquire(block=True,
     timeout=None)' with keyword parameters being acceptable.  If
     _block_ is `True' and _timeout_ is not `None' then it specifies a
     timeout in seconds.  If _block_ is `False' then _timeout_ is
     ignored.

     On Mac OS X, `sem_timedwait' is unsupported, so calling
     `acquire()' with a timeout will emulate that function's behavior
     using a sleeping loop.

     Note: If the SIGINT signal generated by Ctrl-C arrives while the
     main thread is blocked by a call to `BoundedSemaphore.acquire()',
     `Lock.acquire()', `RLock.acquire()', `Semaphore.acquire()',
     `Condition.acquire()' or `Condition.wait()' then the call will be
     immediately interrupted and *note KeyboardInterrupt: 24e. will be
     raised.

     This differs from the behaviour of *note threading: 17a. where
     SIGINT will be ignored while the equivalent blocking calls are in
     progress.


File: python.info,  Node: Shared ctypes Objects,  Next: Managers,  Prev: Synchronization primitives,  Up: Reference

5.16.6.13 Shared `ctypes' Objects
.................................

It is possible to create shared objects using shared memory which can be
inherited by child processes.

 -- Function: multiprocessing.Value (typecode_or_type, *args[, lock])
     Return a *note ctypes: 78. object allocated from shared memory.
     By default the return value is actually a synchronized wrapper for
     the object.

     _typecode_or_type_ determines the type of the returned object: it
     is either a ctypes type or a one character typecode of the kind
     used by the *note array: e.  module.  _*args_ is passed on to the
     constructor for the type.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a *note
     Lock: 15b5. or *note RLock: 15b6. object then that will be used to
     synchronize access to the value.  If _lock_ is `False' then access
     to the returned object will not be automatically protected by a
     lock, so it will not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.Array (typecode_or_type,
          size_or_initializer, *, lock=True)
     Return a ctypes array allocated from shared memory.  By default
     the return value is actually a synchronized wrapper for the array.

     _typecode_or_type_ determines the type of the elements of the
     returned array: it is either a ctypes type or a one character
     typecode of the kind used by the *note array: e. module.  If
     _size_or_initializer_ is an integer, then it determines the length
     of the array, and the array will be initially zeroed.  Otherwise,
     _size_or_initializer_ is a sequence which is used to initialize
     the array and whose length determines the length of the array.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a *note
     Lock: 15b5. or *note RLock: 15b6. object then that will be used to
     synchronize access to the value.  If _lock_ is `False' then access
     to the returned object will not be automatically protected by a
     lock, so it will not necessarily be "process-safe".

     Note that _lock_ is a keyword only argument.

     Note that an array of *note ctypes.c_char: 149a. has _value_ and
     _raw_ attributes which allow one to use it to store and retrieve
     strings.

* Menu:

* The multiprocessing.sharedctypes module: The multiprocessing sharedctypes module.


File: python.info,  Node: The multiprocessing sharedctypes module,  Up: Shared ctypes Objects

5.16.6.14 The `multiprocessing.sharedctypes' module
...................................................

The *note multiprocessing.sharedctypes: 11f. module provides functions
for allocating *note ctypes: 78. objects from shared memory which can
be inherited by child processes.

     Note: Although it is possible to store a pointer in shared memory
     remember that this will refer to a location in the address space
     of a specific process.  However, the pointer is quite likely to be
     invalid in the context of a second process and trying to
     dereference the pointer from the second process may cause a crash.

 -- Function: multiprocessing.sharedctypes.RawArray (typecode_or_type,
          size_or_initializer)
     Return a ctypes array allocated from shared memory.

     _typecode_or_type_ determines the type of the elements of the
     returned array: it is either a ctypes type or a one character
     typecode of the kind used by the *note array: e. module.  If
     _size_or_initializer_ is an integer then it determines the length
     of the array, and the array will be initially zeroed.  Otherwise
     _size_or_initializer_ is a sequence which is used to initialize the
     array and whose length determines the length of the array.

     Note that setting and getting an element is potentially non-atomic
     - use *note Array(): 15ef. instead to make sure that access is
     automatically synchronized using a lock.

 -- Function: multiprocessing.sharedctypes.RawValue (typecode_or_type,
          *args)
     Return a ctypes object allocated from shared memory.

     _typecode_or_type_ determines the type of the returned object: it
     is either a ctypes type or a one character typecode of the kind
     used by the *note array: e.  module.  _*args_ is passed on to the
     constructor for the type.

     Note that setting and getting the value is potentially non-atomic
     - use *note Value(): 15f1. instead to make sure that access is
     automatically synchronized using a lock.

     Note that an array of *note ctypes.c_char: 149a. has `value' and
     `raw' attributes which allow one to use it to store and retrieve
     strings - see documentation for *note ctypes: 78.

 -- Function: multiprocessing.sharedctypes.Array (typecode_or_type,
          size_or_initializer, *args[, lock])
     The same as *note RawArray(): 15ee. except that depending on the
     value of _lock_ a process-safe synchronization wrapper may be
     returned instead of a raw ctypes array.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a `Lock'
     or `RLock' object then that will be used to synchronize access to
     the value.  If _lock_ is `False' then access to the returned
     object will not be automatically protected by a lock, so it will
     not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.sharedctypes.Value (typecode_or_type,
          *args[, lock])
     The same as *note RawValue(): 15f0. except that depending on the
     value of _lock_ a process-safe synchronization wrapper may be
     returned instead of a raw ctypes object.

     If _lock_ is `True' (the default) then a new lock object is
     created to synchronize access to the value.  If _lock_ is a `Lock'
     or `RLock' object then that will be used to synchronize access to
     the value.  If _lock_ is `False' then access to the returned
     object will not be automatically protected by a lock, so it will
     not necessarily be "process-safe".

     Note that _lock_ is a keyword-only argument.

 -- Function: multiprocessing.sharedctypes.copy (obj)
     Return a ctypes object allocated from shared memory which is a
     copy of the ctypes object _obj_.

 -- Function: multiprocessing.sharedctypes.synchronized (obj[, lock])
     Return a process-safe wrapper object for a ctypes object which
     uses _lock_ to synchronize access.  If _lock_ is `None' (the
     default) then a *note multiprocessing.RLock: 15b6. object is
     created automatically.

     A synchronized wrapper will have two methods in addition to those
     of the object it wraps: `get_obj()' returns the wrapped object and
     `get_lock()' returns the lock object used for synchronization.

     Note that accessing the ctypes object through the wrapper can be a
     lot slower than accessing the raw ctypes object.

  The table below compares the syntax for creating shared ctypes
objects from shared memory with the normal ctypes syntax.  (In the
table `MyStruct' is some subclass of *note ctypes.Structure: 14bb.)

ctypes                   sharedctypes using type        sharedctypes using typecode
---------------------------------------------------------------------------------------- 
c_double(2.4)            RawValue(c_double, 2.4)        RawValue('d', 2.4)
MyStruct(4, 6)           RawValue(MyStruct, 4, 6)       
(c_short * 7)()          RawArray(c_short, 7)           RawArray('h', 7)
(c_int * 3)(9, 2, 8)     RawArray(c_int, (9, 2, 8))     RawArray('i', (9, 2, 8))

  Below is an example where a number of ctypes objects are modified by
a child process:

    from multiprocessing import Process, Lock
    from multiprocessing.sharedctypes import Value, Array
    from ctypes import Structure, c_double

    class Point(Structure):
        _fields_ = [('x', c_double), ('y', c_double)]

    def modify(n, x, s, A):
        n.value **= 2
        x.value **= 2
        s.value = s.value.upper()
        for a in A:
            a.x **= 2
            a.y **= 2

    if __name__ == '__main__':
        lock = Lock()

        n = Value('i', 7)
        x = Value(c_double, 1.0/3.0, lock=False)
        s = Array('c', 'hello world', lock=lock)
        A = Array(Point, [(1.875,-6.25), (-5.75,2.0), (2.375,9.5)], lock=lock)

        p = Process(target=modify, args=(n, x, s, A))
        p.start()
        p.join()

        print n.value
        print x.value
        print s.value
        print [(a.x, a.y) for a in A]

The results printed are

    49
    0.1111111111111111
    HELLO WORLD
    [(3.515625, 39.0625), (33.0625, 4.0), (5.640625, 90.25)]



File: python.info,  Node: Managers,  Next: Proxy Objects,  Prev: Shared ctypes Objects,  Up: Reference

5.16.6.15 Managers
..................

Managers provide a way to create data which can be shared between
different processes. A manager object controls a server process which
manages _shared objects_.  Other processes can access the shared
objects by using proxies.

 -- Function: multiprocessing.Manager ()
     Returns a started *note SyncManager: 15f6. object which can be
     used for sharing objects between processes.  The returned manager
     object corresponds to a spawned child process and has methods
     which will create shared objects and return corresponding proxies.

  Manager processes will be shutdown as soon as they are garbage
collected or their parent process exits.  The manager classes are
defined in the *note multiprocessing.managers: 11d. module:

 -- Class: multiprocessing.managers.BaseManager ([address[, authkey]])
     Create a BaseManager object.

     Once created one should call *note start(): 15f8. or
     `get_server().serve_forever()' to ensure that the manager object
     refers to a started manager process.

     _address_ is the address on which the manager process listens for
     new connections.  If _address_ is `None' then an arbitrary one is
     chosen.

     _authkey_ is the authentication key which will be used to check
     the validity of incoming connections to the server process.  If
     _authkey_ is `None' then `current_process().authkey'.  Otherwise
     _authkey_ is used and it must be a string.

      -- Method: start ([initializer[, initargs]])
          Start a subprocess to start the manager.  If _initializer_ is
          not `None' then the subprocess will call
          `initializer(*initargs)' when it starts.

      -- Method: get_server ()
          Returns a `Server' object which represents the actual server
          under the control of the Manager. The `Server' object
          supports the `serve_forever()' method:

              >>> from multiprocessing.managers import BaseManager
              >>> manager = BaseManager(address=('', 50000), authkey='abc')
              >>> server = manager.get_server()
              >>> server.serve_forever()

          `Server' additionally has an *note address: 15fa. attribute.

      -- Method: connect ()
          Connect a local manager object to a remote manager process:

              >>> from multiprocessing.managers import BaseManager
              >>> m = BaseManager(address=('127.0.0.1', 5000), authkey='abc')
              >>> m.connect()



      -- Method: shutdown ()
          Stop the process used by the manager.  This is only available
          if *note start(): 15f8. has been used to start the server
          process.

          This can be called multiple times.

      -- Method: register (typeid[, callable[, proxytype[, exposed[,
               method_to_typeid[, create_method]]]]])
          A classmethod which can be used for registering a type or
          callable with the manager class.

          _typeid_ is a "type identifier" which is used to identify a
          particular type of shared object.  This must be a string.

          _callable_ is a callable used for creating objects for this
          type identifier.  If a manager instance will be created using
          the `from_address()' classmethod or if the _create_method_
          argument is `False' then this can be left as `None'.

          _proxytype_ is a subclass of *note BaseProxy: 15fe. which is
          used to create proxies for shared objects with this _typeid_.
          If `None' then a proxy class is created automatically.

          _exposed_ is used to specify a sequence of method names which
          proxies for this typeid should be allowed to access using
          `BaseProxy._callMethod()'.  (If _exposed_ is `None' then
          `proxytype._exposed_' is used instead if it exists.)  In the
          case where no exposed list is specified, all "public methods"
          of the shared object will be accessible.  (Here a "public
          method" means any attribute which has a *note __call__():
          6d4. method and whose name does not begin with `'_''.)

          _method_to_typeid_ is a mapping used to specify the return
          type of those exposed methods which should return a proxy.
          It maps method names to typeid strings.  (If
          _method_to_typeid_ is `None' then
          `proxytype._method_to_typeid_' is used instead if it exists.)
          If a method's name is not a key of this mapping or if the
          mapping is `None' then the object returned by the method will
          be copied by value.

          _create_method_ determines whether a method should be created
          with name _typeid_ which can be used to tell the server
          process to create a new shared object and return a proxy for
          it.  By default it is `True'.

     *note BaseManager: 15f7. instances also have one read-only
     property:

      -- Attribute: address
          The address used by the manager.

 -- Class: multiprocessing.managers.SyncManager
     A subclass of *note BaseManager: 15f7. which can be used for the
     synchronization of processes.  Objects of this type are returned by
     `multiprocessing.Manager()'.

     It also supports creation of shared lists and dictionaries.

      -- Method: BoundedSemaphore ([value])
          Create a shared *note threading.BoundedSemaphore: 1564.
          object and return a proxy for it.

      -- Method: Condition ([lock])
          Create a shared *note threading.Condition: 157f. object and
          return a proxy for it.

          If _lock_ is supplied then it should be a proxy for a *note
          threading.Lock: 155f. or *note threading.RLock: 1561. object.

      -- Method: Event ()
          Create a shared *note threading.Event: 265. object and return
          a proxy for it.

      -- Method: Lock ()
          Create a shared *note threading.Lock: 155f. object and return
          a proxy for it.

      -- Method: Namespace ()
          Create a shared *note Namespace: 1603. object and return a
          proxy for it.

      -- Method: Queue ([maxsize])
          Create a shared *note Queue.Queue: 5e5. object and return a
          proxy for it.

      -- Method: RLock ()
          Create a shared *note threading.RLock: 1561. object and
          return a proxy for it.

      -- Method: Semaphore ([value])
          Create a shared *note threading.Semaphore: 1587. object and
          return a proxy for it.

      -- Method: Array (typecode, sequence)
          Create an array and return a proxy for it.

      -- Method: Value (typecode, value)
          Create an object with a writable `value' attribute and return
          a proxy for it.

      -- Method: dict ()
      -- Method: dict (mapping)
      -- Method: dict (sequence)
          Create a shared `dict' object and return a proxy for it.

      -- Method: list ()
      -- Method: list (sequence)
          Create a shared `list' object and return a proxy for it.

          Note: Modifications to mutable values or items in dict and
          list proxies will not be propagated through the manager,
          because the proxy has no way of knowing when its values or
          items are modified.  To modify such an item, you can
          re-assign the modified object to the container proxy:

              # create a list proxy and append a mutable object (a dictionary)
              lproxy = manager.list()
              lproxy.append({})
              # now mutate the dictionary
              d = lproxy[0]
              d['a'] = 1
              d['b'] = 2
              # at this point, the changes to d are not yet synced, but by
              # reassigning the dictionary, the proxy is notified of the change
              lproxy[0] = d



* Menu:

* Namespace objects::
* Customized managers::
* Using a remote manager::


File: python.info,  Node: Namespace objects,  Next: Customized managers,  Up: Managers

5.16.6.16 Namespace objects
...........................

A namespace object has no public methods, but does have writable
attributes.  Its representation shows the values of its attributes.

  However, when using a proxy for a namespace object, an attribute
beginning with `'_'' will be an attribute of the proxy and not an
attribute of the referent:

    >>> manager = multiprocessing.Manager()
    >>> Global = manager.Namespace()
    >>> Global.x = 10
    >>> Global.y = 'hello'
    >>> Global._z = 12.3    # this is an attribute of the proxy
    >>> print Global
    Namespace(x=10, y='hello')



File: python.info,  Node: Customized managers,  Next: Using a remote manager,  Prev: Namespace objects,  Up: Managers

5.16.6.17 Customized managers
.............................

To create one's own manager, one creates a subclass of *note
BaseManager: 15f7. and use the *note register(): 15fd. classmethod to
register new types or callables with the manager class.  For example:

    from multiprocessing.managers import BaseManager

    class MathsClass(object):
        def add(self, x, y):
            return x + y
        def mul(self, x, y):
            return x * y

    class MyManager(BaseManager):
        pass

    MyManager.register('Maths', MathsClass)

    if __name__ == '__main__':
        manager = MyManager()
        manager.start()
        maths = manager.Maths()
        print maths.add(4, 3)         # prints 7
        print maths.mul(7, 8)         # prints 56



File: python.info,  Node: Using a remote manager,  Prev: Customized managers,  Up: Managers

5.16.6.18 Using a remote manager
................................

It is possible to run a manager server on one machine and have clients
use it from other machines (assuming that the firewalls involved allow
it).

  Running the following commands creates a server for a single shared
queue which remote clients can access:

    >>> from multiprocessing.managers import BaseManager
    >>> import Queue
    >>> queue = Queue.Queue()
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue', callable=lambda:queue)
    >>> m = QueueManager(address=('', 50000), authkey='abracadabra')
    >>> s = m.get_server()
    >>> s.serve_forever()

One client can access the server as follows:

    >>> from multiprocessing.managers import BaseManager
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue')
    >>> m = QueueManager(address=('foo.bar.org', 50000), authkey='abracadabra')
    >>> m.connect()
    >>> queue = m.get_queue()
    >>> queue.put('hello')

Another client can also use it:

    >>> from multiprocessing.managers import BaseManager
    >>> class QueueManager(BaseManager): pass
    >>> QueueManager.register('get_queue')
    >>> m = QueueManager(address=('foo.bar.org', 50000), authkey='abracadabra')
    >>> m.connect()
    >>> queue = m.get_queue()
    >>> queue.get()
    'hello'

Local processes can also access that queue, using the code from above
on the client to access it remotely:

    >>> from multiprocessing import Process, Queue
    >>> from multiprocessing.managers import BaseManager
    >>> class Worker(Process):
    ...     def __init__(self, q):
    ...         self.q = q
    ...         super(Worker, self).__init__()
    ...     def run(self):
    ...         self.q.put('local hello')
    ...
    >>> queue = Queue()
    >>> w = Worker(queue)
    >>> w.start()
    >>> class QueueManager(BaseManager): pass
    ...
    >>> QueueManager.register('get_queue', callable=lambda: queue)
    >>> m = QueueManager(address=('', 50000), authkey='abracadabra')
    >>> s = m.get_server()
    >>> s.serve_forever()



File: python.info,  Node: Proxy Objects,  Next: Process Pools,  Prev: Managers,  Up: Reference

5.16.6.19 Proxy Objects
.......................

A proxy is an object which _refers_ to a shared object which lives
(presumably) in a different process.  The shared object is said to be
the _referent_ of the proxy.  Multiple proxy objects may have the same
referent.

  A proxy object has methods which invoke corresponding methods of its
referent (although not every method of the referent will necessarily be
available through the proxy).  A proxy can usually be used in most of
the same ways that its referent can:

    >>> from multiprocessing import Manager
    >>> manager = Manager()
    >>> l = manager.list([i*i for i in range(10)])
    >>> print l
    [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
    >>> print repr(l)
    <ListProxy object, typeid 'list' at 0x...>
    >>> l[4]
    16
    >>> l[2:5]
    [4, 9, 16]

Notice that applying *note str(): 1e7. to a proxy will return the
representation of the referent, whereas applying *note repr(): 146.
will return the representation of the proxy.

  An important feature of proxy objects is that they are picklable so
they can be passed between processes.  Note, however, that if a proxy
is sent to the corresponding manager's process then unpickling it will
produce the referent itself.  This means, for example, that one shared
object can contain a second:

    >>> a = manager.list()
    >>> b = manager.list()
    >>> a.append(b)         # referent of a now contains referent of b
    >>> print a, b
    [[]] []
    >>> b.append('hello')
    >>> print a, b
    [['hello']] ['hello']


     Note: The proxy types in *note multiprocessing: 11a. do nothing to
     support comparisons by value.  So, for instance, we have:

         >>> manager.list([1,2,3]) == [1,2,3]
         False

     One should just use a copy of the referent instead when making
     comparisons.

 -- Class: multiprocessing.managers.BaseProxy
     Proxy objects are instances of subclasses of *note BaseProxy: 15fe.

      -- Method: _callmethod (methodname[, args[, kwds]])
          Call and return the result of a method of the proxy's
          referent.

          If `proxy' is a proxy whose referent is `obj' then the
          expression

              proxy._callmethod(methodname, args, kwds)

          will evaluate the expression

              getattr(obj, methodname)(*args, **kwds)

          in the manager's process.

          The returned value will be a copy of the result of the call
          or a proxy to a new shared object - see documentation for the
          _method_to_typeid_ argument of *note BaseManager.register():
          15fd.

          If an exception is raised by the call, then then is re-raised
          by *note _callmethod(): 160f.  If some other exception is
          raised in the manager's process then this is converted into a
          `RemoteError' exception and is raised by *note _callmethod():
          160f.

          Note in particular that an exception will be raised if
          _methodname_ has not been _exposed_

          An example of the usage of *note _callmethod(): 160f.:

              >>> l = manager.list(range(10))
              >>> l._callmethod('__len__')
              10
              >>> l._callmethod('__getslice__', (2, 7))   # equiv to `l[2:7]`
              [2, 3, 4, 5, 6]
              >>> l._callmethod('__getitem__', (20,))     # equiv to `l[20]`
              Traceback (most recent call last):
              ...
              IndexError: list index out of range



      -- Method: _getvalue ()
          Return a copy of the referent.

          If the referent is unpicklable then this will raise an
          exception.

      -- Method: __repr__ ()
          Return a representation of the proxy object.

      -- Method: __str__ ()
          Return the representation of the referent.

* Menu:

* Cleanup: Cleanup<2>.


File: python.info,  Node: Cleanup<2>,  Up: Proxy Objects

5.16.6.20 Cleanup
.................

A proxy object uses a weakref callback so that when it gets garbage
collected it deregisters itself from the manager which owns its
referent.

  A shared object gets deleted from the manager process when there are
no longer any proxies referring to it.


File: python.info,  Node: Process Pools,  Next: Listeners and Clients,  Prev: Proxy Objects,  Up: Reference

5.16.6.21 Process Pools
.......................

One can create a pool of processes which will carry out tasks submitted
to it with the `Pool' class.

 -- Class: multiprocessing.Pool ([processes[, initializer[, initargs[,
          maxtasksperchild]]]])
     A process pool object which controls a pool of worker processes to
     which jobs can be submitted.  It supports asynchronous results
     with timeouts and callbacks and has a parallel map implementation.

     _processes_ is the number of worker processes to use.  If
     _processes_ is `None' then the number returned by `cpu_count()' is
     used.  If _initializer_ is not `None' then each worker process
     will call `initializer(*initargs)' when it starts.

     New in version 2.7: _maxtasksperchild_ is the number of tasks a
     worker process can complete before it will exit and be replaced
     with a fresh worker process, to enable unused resources to be
     freed. The default _maxtasksperchild_ is None, which means worker
     processes will live as long as the pool.

          Note: Worker processes within a `Pool' typically live for the
          complete duration of the Pool's work queue. A frequent
          pattern found in other systems (such as Apache, mod_wsgi,
          etc) to free resources held by workers is to allow a worker
          within a pool to complete only a set amount of work before
          being exiting, being cleaned up and a new process spawned to
          replace the old one. The _maxtasksperchild_ argument to the
          `Pool' exposes this ability to the end user.

      -- Method: apply (func[, args[, kwds]])
          Equivalent of the *note apply(): 2f4. built-in function.  It
          blocks till the result is ready.  Given this blocks, *note
          apply_async(): 1617. is better suited for performing work in
          parallel. Additionally, the passed in function is only
          executed in one of the workers of the pool.

      -- Method: apply_async (func[, args[, kwds[, callback]]])
          A variant of the *note apply(): 2f4. method which returns a
          result object.

          If _callback_ is specified then it should be a callable which
          accepts a single argument.  When the result becomes ready
          _callback_ is applied to it (unless the call failed).
          _callback_ should complete immediately since otherwise the
          thread which handles the results will get blocked.

      -- Method: map (func, iterable[, chunksize])
          A parallel equivalent of the *note map(): 2f5. built-in
          function (it supports only one _iterable_ argument though).
          It blocks till the result is ready.

          This method chops the iterable into a number of chunks which
          it submits to the process pool as separate tasks.  The
          (approximate) size of these chunks can be specified by
          setting _chunksize_ to a positive integer.

      -- Method: map_async (func, iterable[, chunksize[, callback]])
          A variant of the *note map(): 1618. method which returns a
          result object.

          If _callback_ is specified then it should be a callable which
          accepts a single argument.  When the result becomes ready
          _callback_ is applied to it (unless the call failed).
          _callback_ should complete immediately since otherwise the
          thread which handles the results will get blocked.

      -- Method: imap (func, iterable[, chunksize])
          An equivalent of *note itertools.imap(): d1f.

          The _chunksize_ argument is the same as the one used by the
          *note map(): 1618.  method.  For very long iterables using a
          large value for _chunksize_ can make make the job complete
          *much* faster than using the default value of `1'.

          Also if _chunksize_ is `1' then the `next()' method of the
          iterator returned by the *note imap(): 161a. method has an
          optional _timeout_ parameter: `next(timeout)' will raise
          `multiprocessing.TimeoutError' if the result cannot be
          returned within _timeout_ seconds.

      -- Method: imap_unordered (func, iterable[, chunksize])
          The same as *note imap(): 161a. except that the ordering of
          the results from the returned iterator should be considered
          arbitrary.  (Only when there is only one worker process is
          the order guaranteed to be "correct".)

      -- Method: close ()
          Prevents any more tasks from being submitted to the pool.
          Once all the tasks have been completed the worker processes
          will exit.

      -- Method: terminate ()
          Stops the worker processes immediately without completing
          outstanding work.  When the pool object is garbage collected
          *note terminate(): 161d. will be called immediately.

      -- Method: join ()
          Wait for the worker processes to exit.  One must call *note
          close(): 161c. or *note terminate(): 161d. before using *note
          join(): 161e.

 -- Class: multiprocessing.pool.AsyncResult
     The class of the result returned by `Pool.apply_async()' and
     `Pool.map_async()'.

      -- Method: get ([timeout])
          Return the result when it arrives.  If _timeout_ is not
          `None' and the result does not arrive within _timeout_
          seconds then `multiprocessing.TimeoutError' is raised.  If
          the remote call raised an exception then that exception will
          be reraised by *note get(): 1620.

      -- Method: wait ([timeout])
          Wait until the result is available or until _timeout_ seconds
          pass.

      -- Method: ready ()
          Return whether the call has completed.

      -- Method: successful ()
          Return whether the call completed without raising an
          exception.  Will raise *note AssertionError: 7dd. if the
          result is not ready.

  The following example demonstrates the use of a pool:

    from multiprocessing import Pool

    def f(x):
        return x*x

    if __name__ == '__main__':
        pool = Pool(processes=4)              # start 4 worker processes

        result = pool.apply_async(f, (10,))    # evaluate "f(10)" asynchronously
        print result.get(timeout=1)           # prints "100" unless your computer is *very* slow

        print pool.map(f, range(10))          # prints "[0, 1, 4,..., 81]"

        it = pool.imap(f, range(10))
        print it.next()                       # prints "0"
        print it.next()                       # prints "1"
        print it.next(timeout=1)              # prints "4" unless your computer is *very* slow

        import time
        result = pool.apply_async(time.sleep, (10,))
        print result.get(timeout=1)           # raises TimeoutError



File: python.info,  Node: Listeners and Clients,  Next: Authentication keys,  Prev: Process Pools,  Up: Reference

5.16.6.22 Listeners and Clients
...............................

Usually message passing between processes is done using queues or by
using `Connection' objects returned by `Pipe()'.

  However, the *note multiprocessing.connection: 11b. module allows
some extra flexibility.  It basically gives a high level message
oriented API for dealing with sockets or Windows named pipes, and also
has support for _digest authentication_ using the *note hmac: e9.
module.

 -- Function: multiprocessing.connection.deliver_challenge (connection,
          authkey)
     Send a randomly generated message to the other end of the
     connection and wait for a reply.

     If the reply matches the digest of the message using _authkey_ as
     the key then a welcome message is sent to the other end of the
     connection.  Otherwise *note AuthenticationError: 1626. is raised.

 -- Function: multiprocessing.connection.answerChallenge (connection,
          authkey)
     Receive a message, calculate the digest of the message using
     _authkey_ as the key, and then send the digest back.

     If a welcome message is not received, then *note
     AuthenticationError: 1626. is raised.

 -- Function: multiprocessing.connection.Client (address[, family[,
          authenticate[, authkey]]])
     Attempt to set up a connection to the listener which is using
     address _address_, returning a *note Connection: 15cf.

     The type of the connection is determined by _family_ argument, but
     this can generally be omitted since it can usually be inferred
     from the format of _address_. (See *note Address Formats: 1629.)

     If _authenticate_ is `True' or _authkey_ is a string then digest
     authentication is used.  The key used for authentication will be
     either _authkey_ or `current_process().authkey)' if _authkey_ is
     `None'.  If authentication fails then *note AuthenticationError:
     1626. is raised.  See *note Authentication keys: 15c6.

 -- Class: multiprocessing.connection.Listener ([address[, family[,
          backlog[, authenticate[, authkey]]]]])
     A wrapper for a bound socket or Windows named pipe which is
     'listening' for connections.

     _address_ is the address to be used by the bound socket or named
     pipe of the listener object.

          Note: If an address of '0.0.0.0' is used, the address will
          not be a connectable end point on Windows. If you require a
          connectable end-point, you should use '127.0.0.1'.

     _family_ is the type of socket (or named pipe) to use.  This can
     be one of the strings `'AF_INET'' (for a TCP socket), `'AF_UNIX''
     (for a Unix domain socket) or `'AF_PIPE'' (for a Windows named
     pipe).  Of these only the first is guaranteed to be available.  If
     _family_ is `None' then the family is inferred from the format of
     _address_.  If _address_ is also `None' then a default is chosen.
     This default is the family which is assumed to be the fastest
     available.  See *note Address Formats: 1629.  Note that if
     _family_ is `'AF_UNIX'' and address is `None' then the socket will
     be created in a private temporary directory created using *note
     tempfile.mkstemp(): e3d.

     If the listener object uses a socket then _backlog_ (1 by default)
     is passed to the `listen()' method of the socket once it has been
     bound.

     If _authenticate_ is `True' (`False' by default) or _authkey_ is
     not `None' then digest authentication is used.

     If _authkey_ is a string then it will be used as the
     authentication key; otherwise it must be _None_.

     If _authkey_ is `None' and _authenticate_ is `True' then
     `current_process().authkey' is used as the authentication key.  If
     _authkey_ is `None' and _authenticate_ is `False' then no
     authentication is done.  If authentication fails then *note
     AuthenticationError: 1626. is raised.  See *note Authentication
     keys: 15c6.

      -- Method: accept ()
          Accept a connection on the bound socket or named pipe of the
          listener object and return a `Connection' object.  If
          authentication is attempted and fails, then *note
          AuthenticationError: 1626. is raised.

      -- Method: close ()
          Close the bound socket or named pipe of the listener object.
          This is called automatically when the listener is garbage
          collected.  However it is advisable to call it explicitly.

     Listener objects have the following read-only properties:

      -- Attribute: address
          The address which is being used by the Listener object.

      -- Attribute: last_accepted
          The address from which the last accepted connection came.  If
          this is unavailable then it is `None'.

  The module defines two exceptions:

 -- Exception: multiprocessing.connection.AuthenticationError
     Exception raised when there is an authentication error.

  *Examples*

  The following server code creates a listener which uses `'secret
password'' as an authentication key.  It then waits for a connection
and sends some data to the client:

    from multiprocessing.connection import Listener
    from array import array

    address = ('localhost', 6000)     # family is deduced to be 'AF_INET'
    listener = Listener(address, authkey='secret password')

    conn = listener.accept()
    print 'connection accepted from', listener.last_accepted

    conn.send([2.25, None, 'junk', float])

    conn.send_bytes('hello')

    conn.send_bytes(array('i', [42, 1729]))

    conn.close()
    listener.close()

The following code connects to the server and receives some data from
the server:

    from multiprocessing.connection import Client
    from array import array

    address = ('localhost', 6000)
    conn = Client(address, authkey='secret password')

    print conn.recv()                 # => [2.25, None, 'junk', float]

    print conn.recv_bytes()            # => 'hello'

    arr = array('i', [0, 0, 0, 0, 0])
    print conn.recv_bytes_into(arr)     # => 8
    print arr                         # => array('i', [42, 1729, 0, 0, 0])

    conn.close()


* Menu:

* Address Formats::


File: python.info,  Node: Address Formats,  Up: Listeners and Clients

5.16.6.23 Address Formats
.........................

   * An `'AF_INET'' address is a tuple of the form `(hostname, port)'
     where _hostname_ is a string and _port_ is an integer.

   * An `'AF_UNIX'' address is a string representing a filename on the
     filesystem.

   *
    An `'AF_PIPE'' address is a string of the form
          `r'\\.\pipe\_PipeName_''.  To use *note Client(): 1628. to
          connect to a named pipe on a remote computer called
          _ServerName_ one should use an address of the form
          `r'\\_ServerName_\pipe\_PipeName_'' instead.

Note that any string beginning with two backslashes is assumed by
default to be an `'AF_PIPE'' address rather than an `'AF_UNIX'' address.


File: python.info,  Node: Authentication keys,  Next: Logging<2>,  Prev: Listeners and Clients,  Up: Reference

5.16.6.24 Authentication keys
.............................

When one uses `Connection.recv()', the data received is automatically
unpickled.  Unfortunately unpickling data from an untrusted source is a
security risk.  Therefore *note Listener: 162a. and *note Client():
1628. use the *note hmac: e9. module to provide digest authentication.

  An authentication key is a string which can be thought of as a
password: once a connection is established both ends will demand proof
that the other knows the authentication key.  (Demonstrating that both
ends are using the same key does *not* involve sending the key over the
connection.)

  If authentication is requested but do authentication key is specified
then the return value of `current_process().authkey' is used (see *note
Process: 15ab.).  This value will automatically inherited by any *note
Process: 15ab. object that the current process creates.  This means
that (by default) all processes of a multi-process program will share a
single authentication key which can be used when setting up connections
between themselves.

  Suitable authentication keys can also be generated by using *note
os.urandom(): d0b.


File: python.info,  Node: Logging<2>,  Next: The multiprocessing dummy module,  Prev: Authentication keys,  Up: Reference

5.16.6.25 Logging
.................

Some support for logging is available.  Note, however, that the *note
logging: 102.  package does not use process shared locks so it is
possible (depending on the handler type) for messages from different
processes to get mixed up.

 -- Function: multiprocessing.get_logger ()
     Returns the logger used by *note multiprocessing: 11a.  If
     necessary, a new one will be created.

     When first created the logger has level `logging.NOTSET' and no
     default handler. Messages sent to this logger will not by default
     propagate to the root logger.

     Note that on Windows child processes will only inherit the level
     of the parent process's logger - any other customization of the
     logger will not be inherited.

 -- Function: multiprocessing.log_to_stderr ()
     This function performs a call to *note get_logger(): 1632. but in
     addition to returning the logger created by get_logger, it adds a
     handler which sends output to *note sys.stderr: 620. using format
     `'[%(levelname)s/%(processName)s] %(message)s''.

  Below is an example session with logging turned on:

    >>> import multiprocessing, logging
    >>> logger = multiprocessing.log_to_stderr()
    >>> logger.setLevel(logging.INFO)
    >>> logger.warning('doomed')
    [WARNING/MainProcess] doomed
    >>> m = multiprocessing.Manager()
    [INFO/SyncManager-...] child process calling self.run()
    [INFO/SyncManager-...] created temp directory /.../pymp-...
    [INFO/SyncManager-...] manager serving at '/.../listener-...'
    >>> del m
    [INFO/MainProcess] sending shutdown message to manager
    [INFO/SyncManager-...] manager exiting with exitcode 0

In addition to having these two logging functions, the multiprocessing
also exposes two additional logging level attributes. These are
`SUBWARNING' and `SUBDEBUG'. The table below illustrates where theses
fit in the normal level hierarchy.

Level                Numeric value
------------------------------------------ 
`SUBWARNING'         25
`SUBDEBUG'           5

  For a full table of logging levels, see the *note logging: 102.
module.

  These additional logging levels are used primarily for certain debug
messages within the multiprocessing module. Below is the same example
as above, except with `SUBDEBUG' enabled:

    >>> import multiprocessing, logging
    >>> logger = multiprocessing.log_to_stderr()
    >>> logger.setLevel(multiprocessing.SUBDEBUG)
    >>> logger.warning('doomed')
    [WARNING/MainProcess] doomed
    >>> m = multiprocessing.Manager()
    [INFO/SyncManager-...] child process calling self.run()
    [INFO/SyncManager-...] created temp directory /.../pymp-...
    [INFO/SyncManager-...] manager serving at '/.../pymp-djGBXN/listener-...'
    >>> del m
    [SUBDEBUG/MainProcess] finalizer calling ...
    [INFO/MainProcess] sending shutdown message to manager
    [DEBUG/SyncManager-...] manager received shutdown message
    [SUBDEBUG/SyncManager-...] calling <Finalize object, callback=unlink, ...
    [SUBDEBUG/SyncManager-...] finalizer calling <built-in function unlink> ...
    [SUBDEBUG/SyncManager-...] calling <Finalize object, dead>
    [SUBDEBUG/SyncManager-...] finalizer calling <function rmtree at 0x5aa730> ...
    [INFO/SyncManager-...] manager exiting with exitcode 0



File: python.info,  Node: The multiprocessing dummy module,  Prev: Logging<2>,  Up: Reference

5.16.6.26 The `multiprocessing.dummy' module
............................................

*note multiprocessing.dummy: 11c. replicates the API of *note
multiprocessing: 11a. but is no more than a wrapper around the *note
threading: 17a. module.


File: python.info,  Node: Programming guidelines,  Next: Examples<7>,  Prev: Reference,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.27 Programming guidelines
................................

There are certain guidelines and idioms which should be adhered to when
using *note multiprocessing: 11a.

* Menu:

* All platforms::
* Windows::


File: python.info,  Node: All platforms,  Next: Windows,  Up: Programming guidelines

5.16.6.28 All platforms
.......................

Avoid shared state

     As far as possible one should try to avoid shifting large amounts
     of data between processes.

     It is probably best to stick to using queues or pipes for
     communication between processes rather than using the lower level
     synchronization primitives from the *note threading: 17a. module.

  Picklability

     Ensure that the arguments to the methods of proxies are picklable.

  Thread safety of proxies

     Do not use a proxy object from more than one thread unless you
     protect it with a lock.

     (There is never a problem with different processes using the
     _same_ proxy.)

  Joining zombie processes

     On Unix when a process finishes but has not been joined it becomes
     a zombie.  There should never be very many because each time a new
     process starts (or `active_children()' is called) all completed
     processes which have not yet been joined will be joined.  Also
     calling a finished process's `Process.is_alive()' will join the
     process.  Even so it is probably good practice to explicitly join
     all the processes that you start.

  Better to inherit than pickle/unpickle

     On Windows many types from *note multiprocessing: 11a. need to be
     picklable so that child processes can use them.  However, one
     should generally avoid sending shared objects to other processes
     using pipes or queues.  Instead you should arrange the program so
     that a process which need access to a shared resource created
     elsewhere can inherit it from an ancestor process.

  Avoid terminating processes

     Using the `Process.terminate()' method to stop a process is liable
     to cause any shared resources (such as locks, semaphores, pipes
     and queues) currently being used by the process to become broken
     or unavailable to other processes.

     Therefore it is probably best to only consider using
     `Process.terminate()' on processes which never use any shared
     resources.

  Joining processes that use queues

     Bear in mind that a process that has put items in a queue will
     wait before terminating until all the buffered items are fed by
     the "feeder" thread to the underlying pipe.  (The child process
     can call the `Queue.cancel_join_thread()' method of the queue to
     avoid this behaviour.)

     This means that whenever you use a queue you need to make sure
     that all items which have been put on the queue will eventually be
     removed before the process is joined.  Otherwise you cannot be
     sure that processes which have put items on the queue will
     terminate.  Remember also that non-daemonic processes will be
     automatically be joined.

     An example which will deadlock is the following:

         from multiprocessing import Process, Queue

         def f(q):
             q.put('X' * 1000000)

         if __name__ == '__main__':
             queue = Queue()
             p = Process(target=f, args=(queue,))
             p.start()
             p.join()                    # this deadlocks
             obj = queue.get()

     A fix here would be to swap the last two lines round (or simply
     remove the `p.join()' line).

  Explicitly pass resources to child processes

     On Unix a child process can make use of a shared resource created
     in a parent process using a global resource.  However, it is
     better to pass the object as an argument to the constructor for
     the child process.

     Apart from making the code (potentially) compatible with Windows
     this also ensures that as long as the child process is still alive
     the object will not be garbage collected in the parent process.
     This might be important if some resource is freed when the object
     is garbage collected in the parent process.

     So for instance

         from multiprocessing import Process, Lock

         def f():
             ... do something using "lock" ...

         if __name__ == '__main__':
            lock = Lock()
            for i in range(10):
                 Process(target=f).start()

     should be rewritten as

         from multiprocessing import Process, Lock

         def f(l):
             ... do something using "l" ...

         if __name__ == '__main__':
            lock = Lock()
            for i in range(10):
                 Process(target=f, args=(lock,)).start()



  Beware replacing sys.stdin with a "file like object"

     *note multiprocessing: 11a. originally unconditionally called:

         os.close(sys.stdin.fileno())

     in the `multiprocessing.Process._bootstrap()' method -- this
     resulted in issues with processes-in-processes. This has been
     changed to:

         sys.stdin.close()
         sys.stdin = open(os.devnull)

     Which solves the fundamental issue of processes colliding with
     each other resulting in a bad file descriptor error, but
     introduces a potential danger to applications which replace *note
     sys.stdin(): 611. with a "file-like object" with output buffering.
     This danger is that if multiple processes call `close()' on this
     file-like object, it could result in the same data being flushed
     to the object multiple times, resulting in corruption.

     If you write a file-like object and implement your own caching,
     you can make it fork-safe by storing the pid whenever you append
     to the cache, and discarding the cache when the pid changes. For
     example:

         @property
         def cache(self):
             pid = os.getpid()
             if pid != self._pid:
                 self._pid = pid
                 self._cache = []
             return self._cache

     For more information, see issue 5155(1), issue 5313(2) and issue
     5331(3)

  ---------- Footnotes ----------

  (1) http://bugs.python.org/issue5155

  (2) http://bugs.python.org/issue5313

  (3) http://bugs.python.org/issue5331


File: python.info,  Node: Windows,  Prev: All platforms,  Up: Programming guidelines

5.16.6.29 Windows
.................

Since Windows lacks *note os.fork(): 241. it has a few extra
restrictions:

  More picklability

     Ensure that all arguments to `Process.__init__()' are picklable.
     This means, in particular, that bound or unbound methods cannot be
     used directly as the `target' argument on Windows -- just define a
     function and use that instead.

     Also, if you subclass `Process' then make sure that instances will
     be picklable when the `Process.start()' method is called.

  Global variables

     Bear in mind that if code run in a child process tries to access a
     global variable, then the value it sees (if any) may not be the
     same as the value in the parent process at the time that
     `Process.start()' was called.

     However, global variables which are just module level constants
     cause no problems.

  Safe importing of main module

     Make sure that the main module can be safely imported by a new
     Python interpreter without causing unintended side effects (such a
     starting a new process).

     For example, under Windows running the following module would fail
     with a *note RuntimeError: 38a.:

         from multiprocessing import Process

         def foo():
             print 'hello'

         p = Process(target=foo)
         p.start()

     Instead one should protect the "entry point" of the program by
     using `if __name__ == '__main__':' as follows:

         from multiprocessing import Process, freeze_support

         def foo():
             print 'hello'

         if __name__ == '__main__':
             freeze_support()
             p = Process(target=foo)
             p.start()

     (The `freeze_support()' line can be omitted if the program will be
     run normally instead of frozen.)

     This allows the newly spawned Python interpreter to safely import
     the module and then run the module's `foo()' function.

     Similar restrictions apply if a pool or manager is created in the
     main module.


File: python.info,  Node: Examples<7>,  Prev: Programming guidelines,  Up: multiprocessing --- Process-based "threading" interface

5.16.6.30 Examples
..................

Demonstration of how to create and use customized managers and proxies:

    #
    # This module shows how to use arbitrary callables with a subclass of
    # `BaseManager`.
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    from multiprocessing import freeze_support
    from multiprocessing.managers import BaseManager, BaseProxy
    import operator

    ##

    class Foo(object):
        def f(self):
            print 'you called Foo.f()'
        def g(self):
            print 'you called Foo.g()'
        def _h(self):
            print 'you called Foo._h()'

    # A simple generator function
    def baz():
        for i in xrange(10):
            yield i*i

    # Proxy type for generator objects
    class GeneratorProxy(BaseProxy):
        _exposed_ = ('next', '__next__')
        def __iter__(self):
            return self
        def next(self):
            return self._callmethod('next')
        def __next__(self):
            return self._callmethod('__next__')

    # Function to return the operator module
    def get_operator_module():
        return operator

    ##

    class MyManager(BaseManager):
        pass

    # register the Foo class; make `f()` and `g()` accessible via proxy
    MyManager.register('Foo1', Foo)

    # register the Foo class; make `g()` and `_h()` accessible via proxy
    MyManager.register('Foo2', Foo, exposed=('g', '_h'))

    # register the generator function baz; use `GeneratorProxy` to make proxies
    MyManager.register('baz', baz, proxytype=GeneratorProxy)

    # register get_operator_module(); make public functions accessible via proxy
    MyManager.register('operator', get_operator_module)

    ##

    def test():
        manager = MyManager()
        manager.start()

        print '-' * 20

        f1 = manager.Foo1()
        f1.f()
        f1.g()
        assert not hasattr(f1, '_h')
        assert sorted(f1._exposed_) == sorted(['f', 'g'])

        print '-' * 20

        f2 = manager.Foo2()
        f2.g()
        f2._h()
        assert not hasattr(f2, 'f')
        assert sorted(f2._exposed_) == sorted(['g', '_h'])

        print '-' * 20

        it = manager.baz()
        for i in it:
            print '<%d>' % i,
        print

        print '-' * 20

        op = manager.operator()
        print 'op.add(23, 45) =', op.add(23, 45)
        print 'op.pow(2, 94) =', op.pow(2, 94)
        print 'op.getslice(range(10), 2, 6) =', op.getslice(range(10), 2, 6)
        print 'op.repeat(range(5), 3) =', op.repeat(range(5), 3)
        print 'op._exposed_ =', op._exposed_

    ##

    if __name__ == '__main__':
        freeze_support()
        test()

Using `Pool':

    #
    # A test of `multiprocessing.Pool` class
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import multiprocessing
    import time
    import random
    import sys

    #
    # Functions used by test code
    #

    def calculate(func, args):
        result = func(*args)
        return '%s says that %s%s = %s' % (
            multiprocessing.current_process().name,
            func.__name__, args, result
            )

    def calculatestar(args):
        return calculate(*args)

    def mul(a, b):
        time.sleep(0.5*random.random())
        return a * b

    def plus(a, b):
        time.sleep(0.5*random.random())
        return a + b

    def f(x):
        return 1.0 / (x-5.0)

    def pow3(x):
        return x**3

    def noop(x):
        pass

    #
    # Test code
    #

    def test():
        print 'cpu_count() = %d\n' % multiprocessing.cpu_count()

        #
        # Create pool
        #

        PROCESSES = 4
        print 'Creating pool with %d processes\n' % PROCESSES
        pool = multiprocessing.Pool(PROCESSES)
        print 'pool = %s' % pool
        print

        #
        # Tests
        #

        TASKS = [(mul, (i, 7)) for i in range(10)] + \
                [(plus, (i, 8)) for i in range(10)]

        results = [pool.apply_async(calculate, t) for t in TASKS]
        imap_it = pool.imap(calculatestar, TASKS)
        imap_unordered_it = pool.imap_unordered(calculatestar, TASKS)

        print 'Ordered results using pool.apply_async():'
        for r in results:
            print '\t', r.get()
        print

        print 'Ordered results using pool.imap():'
        for x in imap_it:
            print '\t', x
        print

        print 'Unordered results using pool.imap_unordered():'
        for x in imap_unordered_it:
            print '\t', x
        print

        print 'Ordered results using pool.map() --- will block till complete:'
        for x in pool.map(calculatestar, TASKS):
            print '\t', x
        print

        #
        # Simple benchmarks
        #

        N = 100000
        print 'def pow3(x): return x**3'

        t = time.time()
        A = map(pow3, xrange(N))
        print '\tmap(pow3, xrange(%d)):\n\t\t%s seconds' % \
              (N, time.time() - t)

        t = time.time()
        B = pool.map(pow3, xrange(N))
        print '\tpool.map(pow3, xrange(%d)):\n\t\t%s seconds' % \
              (N, time.time() - t)

        t = time.time()
        C = list(pool.imap(pow3, xrange(N), chunksize=N//8))
        print '\tlist(pool.imap(pow3, xrange(%d), chunksize=%d)):\n\t\t%s' \
              ' seconds' % (N, N//8, time.time() - t)

        assert A == B == C, (len(A), len(B), len(C))
        print

        L = [None] * 1000000
        print 'def noop(x): pass'
        print 'L = [None] * 1000000'

        t = time.time()
        A = map(noop, L)
        print '\tmap(noop, L):\n\t\t%s seconds' % \
              (time.time() - t)

        t = time.time()
        B = pool.map(noop, L)
        print '\tpool.map(noop, L):\n\t\t%s seconds' % \
              (time.time() - t)

        t = time.time()
        C = list(pool.imap(noop, L, chunksize=len(L)//8))
        print '\tlist(pool.imap(noop, L, chunksize=%d)):\n\t\t%s seconds' % \
              (len(L)//8, time.time() - t)

        assert A == B == C, (len(A), len(B), len(C))
        print

        del A, B, C, L

        #
        # Test error handling
        #

        print 'Testing error handling:'

        try:
            print pool.apply(f, (5,))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from pool.apply()'
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print pool.map(f, range(10))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from pool.map()'
        else:
            raise AssertionError('expected ZeroDivisionError')

        try:
            print list(pool.imap(f, range(10)))
        except ZeroDivisionError:
            print '\tGot ZeroDivisionError as expected from list(pool.imap())'
        else:
            raise AssertionError('expected ZeroDivisionError')

        it = pool.imap(f, range(10))
        for i in range(10):
            try:
                x = it.next()
            except ZeroDivisionError:
                if i == 5:
                    pass
            except StopIteration:
                break
            else:
                if i == 5:
                    raise AssertionError('expected ZeroDivisionError')

        assert i == 9
        print '\tGot ZeroDivisionError as expected from IMapIterator.next()'
        print

        #
        # Testing timeouts
        #

        print 'Testing ApplyResult.get() with timeout:',
        res = pool.apply_async(calculate, TASKS[0])
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % res.get(0.02))
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print
        print

        print 'Testing IMapIterator.next() with timeout:',
        it = pool.imap(calculatestar, TASKS)
        while 1:
            sys.stdout.flush()
            try:
                sys.stdout.write('\n\t%s' % it.next(0.02))
            except StopIteration:
                break
            except multiprocessing.TimeoutError:
                sys.stdout.write('.')
        print
        print

        #
        # Testing callback
        #

        print 'Testing callback:'

        A = []
        B = [56, 0, 1, 8, 27, 64, 125, 216, 343, 512, 729]

        r = pool.apply_async(mul, (7, 8), callback=A.append)
        r.wait()

        r = pool.map_async(pow3, range(10), callback=A.extend)
        r.wait()

        if A == B:
            print '\tcallbacks succeeded\n'
        else:
            print '\t*** callbacks failed\n\t\t%s != %s\n' % (A, B)

        #
        # Check there are no outstanding tasks
        #

        assert not pool._cache, 'cache = %r' % pool._cache

        #
        # Check close() methods
        #

        print 'Testing close():'

        for worker in pool._pool:
            assert worker.is_alive()

        result = pool.apply_async(time.sleep, [0.5])
        pool.close()
        pool.join()

        assert result.get() is None

        for worker in pool._pool:
            assert not worker.is_alive()

        print '\tclose() succeeded\n'

        #
        # Check terminate() method
        #

        print 'Testing terminate():'

        pool = multiprocessing.Pool(2)
        DELTA = 0.1
        ignore = pool.apply(pow3, [2])
        results = [pool.apply_async(time.sleep, [DELTA]) for i in range(100)]
        pool.terminate()
        pool.join()

        for worker in pool._pool:
            assert not worker.is_alive()

        print '\tterminate() succeeded\n'

        #
        # Check garbage collection
        #

        print 'Testing garbage collection:'

        pool = multiprocessing.Pool(2)
        DELTA = 0.1
        processes = pool._pool
        ignore = pool.apply(pow3, [2])
        results = [pool.apply_async(time.sleep, [DELTA]) for i in range(100)]

        results = pool = None

        time.sleep(DELTA * 2)

        for worker in processes:
            assert not worker.is_alive()

        print '\tgarbage collection succeeded\n'


    if __name__ == '__main__':
        multiprocessing.freeze_support()

        assert len(sys.argv) in (1, 2)

        if len(sys.argv) == 1 or sys.argv[1] == 'processes':
            print ' Using processes '.center(79, '-')
        elif sys.argv[1] == 'threads':
            print ' Using threads '.center(79, '-')
            import multiprocessing.dummy as multiprocessing
        else:
            print 'Usage:\n\t%s [processes | threads]' % sys.argv[0]
            raise SystemExit(2)

        test()

Synchronization types like locks, conditions and queues:

    #
    # A test file for the `multiprocessing` package
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time, sys, random
    from Queue import Empty

    import multiprocessing               # may get overwritten


    #### TEST_VALUE

    def value_func(running, mutex):
        random.seed()
        time.sleep(random.random()*4)

        mutex.acquire()
        print '\n\t\t\t' + str(multiprocessing.current_process()) + ' has finished'
        running.value -= 1
        mutex.release()

    def test_value():
        TASKS = 10
        running = multiprocessing.Value('i', TASKS)
        mutex = multiprocessing.Lock()

        for i in range(TASKS):
            p = multiprocessing.Process(target=value_func, args=(running, mutex))
            p.start()

        while running.value > 0:
            time.sleep(0.08)
            mutex.acquire()
            print running.value,
            sys.stdout.flush()
            mutex.release()

        print
        print 'No more running processes'


    #### TEST_QUEUE

    def queue_func(queue):
        for i in range(30):
            time.sleep(0.5 * random.random())
            queue.put(i*i)
        queue.put('STOP')

    def test_queue():
        q = multiprocessing.Queue()

        p = multiprocessing.Process(target=queue_func, args=(q,))
        p.start()

        o = None
        while o != 'STOP':
            try:
                o = q.get(timeout=0.3)
                print o,
                sys.stdout.flush()
            except Empty:
                print 'TIMEOUT'

        print


    #### TEST_CONDITION

    def condition_func(cond):
        cond.acquire()
        print '\t' + str(cond)
        time.sleep(2)
        print '\tchild is notifying'
        print '\t' + str(cond)
        cond.notify()
        cond.release()

    def test_condition():
        cond = multiprocessing.Condition()

        p = multiprocessing.Process(target=condition_func, args=(cond,))
        print cond

        cond.acquire()
        print cond
        cond.acquire()
        print cond

        p.start()

        print 'main is waiting'
        cond.wait()
        print 'main has woken up'

        print cond
        cond.release()
        print cond
        cond.release()

        p.join()
        print cond


    #### TEST_SEMAPHORE

    def semaphore_func(sema, mutex, running):
        sema.acquire()

        mutex.acquire()
        running.value += 1
        print running.value, 'tasks are running'
        mutex.release()

        random.seed()
        time.sleep(random.random()*2)

        mutex.acquire()
        running.value -= 1
        print '%s has finished' % multiprocessing.current_process()
        mutex.release()

        sema.release()

    def test_semaphore():
        sema = multiprocessing.Semaphore(3)
        mutex = multiprocessing.RLock()
        running = multiprocessing.Value('i', 0)

        processes = [
            multiprocessing.Process(target=semaphore_func,
                                    args=(sema, mutex, running))
            for i in range(10)
            ]

        for p in processes:
            p.start()

        for p in processes:
            p.join()


    #### TEST_JOIN_TIMEOUT

    def join_timeout_func():
        print '\tchild sleeping'
        time.sleep(5.5)
        print '\n\tchild terminating'

    def test_join_timeout():
        p = multiprocessing.Process(target=join_timeout_func)
        p.start()

        print 'waiting for process to finish'

        while 1:
            p.join(timeout=1)
            if not p.is_alive():
                break
            print '.',
            sys.stdout.flush()


    #### TEST_EVENT

    def event_func(event):
        print '\t%r is waiting' % multiprocessing.current_process()
        event.wait()
        print '\t%r has woken up' % multiprocessing.current_process()

    def test_event():
        event = multiprocessing.Event()

        processes = [multiprocessing.Process(target=event_func, args=(event,))
                     for i in range(5)]

        for p in processes:
            p.start()

        print 'main is sleeping'
        time.sleep(2)

        print 'main is setting event'
        event.set()

        for p in processes:
            p.join()


    #### TEST_SHAREDVALUES

    def sharedvalues_func(values, arrays, shared_values, shared_arrays):
        for i in range(len(values)):
            v = values[i][1]
            sv = shared_values[i].value
            assert v == sv

        for i in range(len(values)):
            a = arrays[i][1]
            sa = list(shared_arrays[i][:])
            assert a == sa

        print 'Tests passed'

    def test_sharedvalues():
        values = [
            ('i', 10),
            ('h', -2),
            ('d', 1.25)
            ]
        arrays = [
            ('i', range(100)),
            ('d', [0.25 * i for i in range(100)]),
            ('H', range(1000))
            ]

        shared_values = [multiprocessing.Value(id, v) for id, v in values]
        shared_arrays = [multiprocessing.Array(id, a) for id, a in arrays]

        p = multiprocessing.Process(
            target=sharedvalues_func,
            args=(values, arrays, shared_values, shared_arrays)
            )
        p.start()
        p.join()

        assert p.exitcode == 0


    ####

    def test(namespace=multiprocessing):
        global multiprocessing

        multiprocessing = namespace

        for func in [ test_value, test_queue, test_condition,
                      test_semaphore, test_join_timeout, test_event,
                      test_sharedvalues ]:

            print '\n\t######## %s\n' % func.__name__
            func()

        ignore = multiprocessing.active_children()      # cleanup any old processes
        if hasattr(multiprocessing, '_debug_info'):
            info = multiprocessing._debug_info()
            if info:
                print info
                raise ValueError('there should be no positive refcounts left')


    if __name__ == '__main__':
        multiprocessing.freeze_support()

        assert len(sys.argv) in (1, 2)

        if len(sys.argv) == 1 or sys.argv[1] == 'processes':
            print ' Using processes '.center(79, '-')
            namespace = multiprocessing
        elif sys.argv[1] == 'manager':
            print ' Using processes and a manager '.center(79, '-')
            namespace = multiprocessing.Manager()
            namespace.Process = multiprocessing.Process
            namespace.current_process = multiprocessing.current_process
            namespace.active_children = multiprocessing.active_children
        elif sys.argv[1] == 'threads':
            print ' Using threads '.center(79, '-')
            import multiprocessing.dummy as namespace
        else:
            print 'Usage:\n\t%s [processes | manager | threads]' % sys.argv[0]
            raise SystemExit(2)

        test(namespace)

An example showing how to use queues to feed tasks to a collection of
worker process and collect the results:

    #
    # Simple example which uses a pool of workers to carry out some tasks.
    #
    # Notice that the results will probably not come out of the output
    # queue in the same in the same order as the corresponding tasks were
    # put on the input queue.  If it is important to get the results back
    # in the original order then consider using `Pool.map()` or
    # `Pool.imap()` (which will save on the amount of code needed anyway).
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time
    import random

    from multiprocessing import Process, Queue, current_process, freeze_support

    #
    # Function run by worker processes
    #

    def worker(input, output):
        for func, args in iter(input.get, 'STOP'):
            result = calculate(func, args)
            output.put(result)

    #
    # Function used to calculate result
    #

    def calculate(func, args):
        result = func(*args)
        return '%s says that %s%s = %s' % \
            (current_process().name, func.__name__, args, result)

    #
    # Functions referenced by tasks
    #

    def mul(a, b):
        time.sleep(0.5*random.random())
        return a * b

    def plus(a, b):
        time.sleep(0.5*random.random())
        return a + b

    #
    #
    #

    def test():
        NUMBER_OF_PROCESSES = 4
        TASKS1 = [(mul, (i, 7)) for i in range(20)]
        TASKS2 = [(plus, (i, 8)) for i in range(10)]

        # Create queues
        task_queue = Queue()
        done_queue = Queue()

        # Submit tasks
        for task in TASKS1:
            task_queue.put(task)

        # Start worker processes
        for i in range(NUMBER_OF_PROCESSES):
            Process(target=worker, args=(task_queue, done_queue)).start()

        # Get and print results
        print 'Unordered results:'
        for i in range(len(TASKS1)):
            print '\t', done_queue.get()

        # Add more tasks using `put()`
        for task in TASKS2:
            task_queue.put(task)

        # Get and print some more results
        for i in range(len(TASKS2)):
            print '\t', done_queue.get()

        # Tell child processes to stop
        for i in range(NUMBER_OF_PROCESSES):
            task_queue.put('STOP')


    if __name__ == '__main__':
        freeze_support()
        test()

An example of how a pool of worker processes can each run a
`SimpleHTTPServer.HttpServer' instance while sharing a single listening
socket.

    #
    # Example where a pool of http servers share a single listening socket
    #
    # On Windows this module depends on the ability to pickle a socket
    # object so that the worker processes can inherit a copy of the server
    # object.  (We import `multiprocessing.reduction` to enable this pickling.)
    #
    # Not sure if we should synchronize access to `socket.accept()` method by
    # using a process-shared lock -- does not seem to be necessary.
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import os
    import sys

    from multiprocessing import Process, current_process, freeze_support
    from BaseHTTPServer import HTTPServer
    from SimpleHTTPServer import SimpleHTTPRequestHandler

    if sys.platform == 'win32':
        import multiprocessing.reduction    # make sockets pickable/inheritable


    def note(format, *args):
        sys.stderr.write('[%s]\t%s\n' % (current_process().name, format%args))


    class RequestHandler(SimpleHTTPRequestHandler):
        # we override log_message() to show which process is handling the request
        def log_message(self, format, *args):
            note(format, *args)

    def serve_forever(server):
        note('starting server')
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass


    def runpool(address, number_of_processes):
        # create a single server object -- children will each inherit a copy
        server = HTTPServer(address, RequestHandler)

        # create child processes to act as workers
        for i in range(number_of_processes-1):
            Process(target=serve_forever, args=(server,)).start()

        # main process also acts as a worker
        serve_forever(server)


    def test():
        DIR = os.path.join(os.path.dirname(__file__), '..')
        ADDRESS = ('localhost', 8000)
        NUMBER_OF_PROCESSES = 4

        print 'Serving at http://%s:%d using %d worker processes' % \
              (ADDRESS[0], ADDRESS[1], NUMBER_OF_PROCESSES)
        print 'To exit press Ctrl-' + ['C', 'Break'][sys.platform=='win32']

        os.chdir(DIR)
        runpool(ADDRESS, NUMBER_OF_PROCESSES)


    if __name__ == '__main__':
        freeze_support()
        test()

Some simple benchmarks comparing *note multiprocessing: 11a. with *note
threading: 17a.:

    #
    # Simple benchmarks for the multiprocessing package
    #
    # Copyright (c) 2006-2008, R Oudkerk
    # All rights reserved.
    #

    import time, sys, multiprocessing, threading, Queue, gc

    if sys.platform == 'win32':
        _timer = time.clock
    else:
        _timer = time.time

    delta = 1


    #### TEST_QUEUESPEED

    def queuespeed_func(q, c, iterations):
        a = '0' * 256
        c.acquire()
        c.notify()
        c.release()

        for i in xrange(iterations):
            q.put(a)

        q.put('STOP')

    def test_queuespeed(Process, q, c):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            p = Process(target=queuespeed_func, args=(q, c, iterations))
            c.acquire()
            p.start()
            c.wait()
            c.release()

            result = None
            t = _timer()

            while result != 'STOP':
                result = q.get()

            elapsed = _timer() - t

            p.join()

        print iterations, 'objects passed through the queue in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_PIPESPEED

    def pipe_func(c, cond, iterations):
        a = '0' * 256
        cond.acquire()
        cond.notify()
        cond.release()

        for i in xrange(iterations):
            c.send(a)

        c.send('STOP')

    def test_pipespeed():
        c, d = multiprocessing.Pipe()
        cond = multiprocessing.Condition()
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            p = multiprocessing.Process(target=pipe_func,
                                        args=(d, cond, iterations))
            cond.acquire()
            p.start()
            cond.wait()
            cond.release()

            result = None
            t = _timer()

            while result != 'STOP':
                result = c.recv()

            elapsed = _timer() - t
            p.join()

        print iterations, 'objects passed through connection in',elapsed,'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_SEQSPEED

    def test_seqspeed(seq):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            t = _timer()

            for i in xrange(iterations):
                a = seq[5]

            elapsed = _timer()-t

        print iterations, 'iterations in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_LOCK

    def test_lockspeed(l):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            t = _timer()

            for i in xrange(iterations):
                l.acquire()
                l.release()

            elapsed = _timer()-t

        print iterations, 'iterations in', elapsed, 'seconds'
        print 'average number/sec:', iterations/elapsed


    #### TEST_CONDITION

    def conditionspeed_func(c, N):
        c.acquire()
        c.notify()

        for i in xrange(N):
            c.wait()
            c.notify()

        c.release()

    def test_conditionspeed(Process, c):
        elapsed = 0
        iterations = 1

        while elapsed < delta:
            iterations *= 2

            c.acquire()
            p = Process(target=conditionspeed_func, args=(c, iterations))
            p.start()

            c.wait()

            t = _timer()

            for i in xrange(iterations):
                c.notify()
                c.wait()

            elapsed = _timer()-t

            c.release()
            p.join()

        print iterations * 2, 'waits in', elapsed, 'seconds'
        print 'average number/sec:', iterations * 2 / elapsed

    ####

    def test():
        manager = multiprocessing.Manager()

        gc.disable()

        print '\n\t######## testing Queue.Queue\n'
        test_queuespeed(threading.Thread, Queue.Queue(),
                        threading.Condition())
        print '\n\t######## testing multiprocessing.Queue\n'
        test_queuespeed(multiprocessing.Process, multiprocessing.Queue(),
                        multiprocessing.Condition())
        print '\n\t######## testing Queue managed by server process\n'
        test_queuespeed(multiprocessing.Process, manager.Queue(),
                        manager.Condition())
        print '\n\t######## testing multiprocessing.Pipe\n'
        test_pipespeed()

        print

        print '\n\t######## testing list\n'
        test_seqspeed(range(10))
        print '\n\t######## testing list managed by server process\n'
        test_seqspeed(manager.list(range(10)))
        print '\n\t######## testing Array("i", ..., lock=False)\n'
        test_seqspeed(multiprocessing.Array('i', range(10), lock=False))
        print '\n\t######## testing Array("i", ..., lock=True)\n'
        test_seqspeed(multiprocessing.Array('i', range(10), lock=True))

        print

        print '\n\t######## testing threading.Lock\n'
        test_lockspeed(threading.Lock())
        print '\n\t######## testing threading.RLock\n'
        test_lockspeed(threading.RLock())
        print '\n\t######## testing multiprocessing.Lock\n'
        test_lockspeed(multiprocessing.Lock())
        print '\n\t######## testing multiprocessing.RLock\n'
        test_lockspeed(multiprocessing.RLock())
        print '\n\t######## testing lock managed by server process\n'
        test_lockspeed(manager.Lock())
        print '\n\t######## testing rlock managed by server process\n'
        test_lockspeed(manager.RLock())

        print

        print '\n\t######## testing threading.Condition\n'
        test_conditionspeed(threading.Thread, threading.Condition())
        print '\n\t######## testing multiprocessing.Condition\n'
        test_conditionspeed(multiprocessing.Process, multiprocessing.Condition())
        print '\n\t######## testing condition managed by a server process\n'
        test_conditionspeed(multiprocessing.Process, manager.Condition())

        gc.enable()

    if __name__ == '__main__':
        multiprocessing.freeze_support()
        test()



File: python.info,  Node: mmap --- Memory-mapped file support,  Next: readline --- GNU readline interface,  Prev: multiprocessing --- Process-based "threading" interface,  Up: Optional Operating System Services

5.16.7 `mmap' -- Memory-mapped file support
-------------------------------------------

Memory-mapped file objects behave like both strings and like file
objects.  Unlike normal string objects, however, these are mutable.
You can use mmap objects in most places where strings are expected; for
example, you can use the *note re: 144. module to search through a
memory-mapped file.  Since they're mutable, you can change a single
character by doing `obj[index] = 'a'', or change a substring by
assigning to a slice: `obj[i1:i2] = '...''.  You can also read and
write data starting at the current file position, and *note seek():
163b. through the file to different positions.

  A memory-mapped file is created by the *note mmap: 115. constructor,
which is different on Unix and on Windows.  In either case you must
provide a file descriptor for a file opened for update. If you wish to
map an existing Python file object, use its `fileno()' method to obtain
the correct value for the _fileno_ parameter.  Otherwise, you can open
the file using the *note os.open(): 5c3. function, which returns a file
descriptor directly (the file still needs to be closed when done).

  For both the Unix and Windows versions of the constructor, _access_
may be specified as an optional keyword parameter. _access_ accepts one
of three values: `ACCESS_READ', `ACCESS_WRITE', or `ACCESS_COPY' to
specify read-only, write-through or copy-on-write memory respectively.
_access_ can be used on both Unix and Windows.  If _access_ is not
specified, Windows mmap returns a write-through mapping.  The initial
memory values for all three access types are taken from the specified
file.  Assignment to an `ACCESS_READ' memory map raises a *note
TypeError: 215. exception.  Assignment to an `ACCESS_WRITE' memory map
affects both memory and the underlying file.  Assignment to an
`ACCESS_COPY' memory map affects memory but does not update the
underlying file.

  Changed in version 2.5: To map anonymous memory, -1 should be passed
as the fileno along with the length.

  Changed in version 2.6: mmap.mmap has formerly been a factory
function creating mmap objects. Now mmap.mmap is the class itself.

 -- Class: mmap.mmap (fileno, length[, tagname[, access[, offset]]])
     *(Windows version)* Maps _length_ bytes from the file specified by
     the file handle _fileno_, and creates a mmap object.  If _length_
     is larger than the current size of the file, the file is extended
     to contain _length_ bytes.  If _length_ is `0', the maximum length
     of the map is the current size of the file, except that if the
     file is empty Windows raises an exception (you cannot create an
     empty mapping on Windows).

     _tagname_, if specified and not `None', is a string giving a tag
     name for the mapping.  Windows allows you to have many different
     mappings against the same file.  If you specify the name of an
     existing tag, that tag is opened, otherwise a new tag of this name
     is created.  If this parameter is omitted or `None', the mapping
     is created without a name.  Avoiding the use of the tag parameter
     will assist in keeping your code portable between Unix and Windows.

     _offset_ may be specified as a non-negative integer offset. mmap
     references will be relative to the offset from the beginning of
     the file. _offset_ defaults to 0.  _offset_ must be a multiple of
     the ALLOCATIONGRANULARITY.

 -- Class: mmap.mmap (fileno, length[, flags[, prot[, access[,
          offset]]]])
     *(Unix version)* Maps _length_ bytes from the file specified by
     the file descriptor _fileno_, and returns a mmap object.  If
     _length_ is `0', the maximum length of the map will be the current
     size of the file when *note mmap: 115. is called.

     _flags_ specifies the nature of the mapping. `MAP_PRIVATE' creates
     a private copy-on-write mapping, so changes to the contents of the
     mmap object will be private to this process, and `MAP_SHARED'
     creates a mapping that's shared with all other processes mapping
     the same areas of the file.  The default value is `MAP_SHARED'.

     _prot_, if specified, gives the desired memory protection; the two
     most useful values are `PROT_READ' and `PROT_WRITE', to specify
     that the pages may be read or written.  _prot_ defaults to
     `PROT_READ | PROT_WRITE'.

     _access_ may be specified in lieu of _flags_ and _prot_ as an
     optional keyword parameter.  It is an error to specify both
     _flags_, _prot_ and _access_.  See the description of _access_
     above for information on how to use this parameter.

     _offset_ may be specified as a non-negative integer offset. mmap
     references will be relative to the offset from the beginning of
     the file. _offset_ defaults to 0.  _offset_ must be a multiple of
     the PAGESIZE or ALLOCATIONGRANULARITY.

     To ensure validity of the created memory mapping the file specified
     by the descriptor _fileno_ is internally automatically synchronized
     with physical backing store on Mac OS X and OpenVMS.

     This example shows a simple way of using *note mmap: 115.:

         import mmap

         # write a simple example file
         with open("hello.txt", "wb") as f:
             f.write("Hello Python!\n")

         with open("hello.txt", "r+b") as f:
             # memory-map the file, size 0 means whole file
             map = mmap.mmap(f.fileno(), 0)
             # read content via standard file methods
             print map.readline()  # prints "Hello Python!"
             # read content via slice notation
             print map[:5]  # prints "Hello"
             # update content using slice notation;
             # note that new content must have same size
             map[6:] = " world!\n"
             # ... and read again using standard file methods
             map.seek(0)
             print map.readline()  # prints "Hello  world!"
             # close the map
             map.close()

     The next example demonstrates how to create an anonymous map and
     exchange data between the parent and child processes:

         import mmap
         import os

         map = mmap.mmap(-1, 13)
         map.write("Hello world!")

         pid = os.fork()

         if pid == 0: # In a child process
             map.seek(0)
             print map.readline()

             map.close()

     Memory-mapped file objects support the following methods:

      -- Method: mmap.close ()
          Close the file.  Subsequent calls to other methods of the
          object will result in an exception being raised.

      -- Method: mmap.find (string[, start[, end]])
          Returns the lowest index in the object where the substring
          _string_ is found, such that _string_ is contained in the
          range [_start_, _end_].  Optional arguments _start_ and _end_
          are interpreted as in slice notation.  Returns `-1' on
          failure.

      -- Method: mmap.flush ([offset, size])
          Flushes changes made to the in-memory copy of a file back to
          disk. Without use of this call there is no guarantee that
          changes are written back before the object is destroyed.  If
          _offset_ and _size_ are specified, only changes to the given
          range of bytes will be flushed to disk; otherwise, the whole
          extent of the mapping is flushed.

          *(Windows version)* A nonzero value returned indicates
          success; zero indicates failure.

          *(Unix version)* A zero value is returned to indicate
          success. An exception is raised when the call failed.

      -- Method: mmap.move (dest, src, count)
          Copy the _count_ bytes starting at offset _src_ to the
          destination index _dest_.  If the mmap was created with
          `ACCESS_READ', then calls to move will raise a *note
          TypeError: 215. exception.

      -- Method: mmap.read (num)
          Return a string containing up to _num_ bytes starting from
          the current file position; the file position is updated to
          point after the bytes that were returned.

      -- Method: mmap.read_byte ()
          Returns a string of length 1 containing the character at the
          current file position, and advances the file position by 1.

      -- Method: mmap.readline ()
          Returns a single line, starting at the current file position
          and up to the next newline.

      -- Method: mmap.resize (newsize)
          Resizes the map and the underlying file, if any. If the mmap
          was created with `ACCESS_READ' or `ACCESS_COPY', resizing the
          map will raise a *note TypeError: 215. exception.

      -- Method: mmap.rfind (string[, start[, end]])
          Returns the highest index in the object where the substring
          _string_ is found, such that _string_ is contained in the
          range [_start_, _end_].  Optional arguments _start_ and _end_
          are interpreted as in slice notation.  Returns `-1' on
          failure.

      -- Method: mmap.seek (pos[, whence])
          Set the file's current position.  _whence_ argument is
          optional and defaults to `os.SEEK_SET' or `0' (absolute file
          positioning); other values are `os.SEEK_CUR' or `1' (seek
          relative to the current position) and `os.SEEK_END' or `2'
          (seek relative to the file's end).

      -- Method: mmap.size ()
          Return the length of the file, which can be larger than the
          size of the memory-mapped area.

      -- Method: mmap.tell ()
          Returns the current position of the file pointer.

      -- Method: mmap.write (string)
          Write the bytes in _string_ into memory at the current
          position of the file pointer; the file position is updated to
          point after the bytes that were written. If the mmap was
          created with `ACCESS_READ', then writing to it will raise a
          *note TypeError: 215. exception.

      -- Method: mmap.write_byte (byte)
          Write the single-character string _byte_ into memory at the
          current position of the file pointer; the file position is
          advanced by `1'. If the mmap was created with `ACCESS_READ',
          then writing to it will raise a *note TypeError: 215.
          exception.


File: python.info,  Node: readline --- GNU readline interface,  Next: rlcompleter --- Completion function for GNU readline,  Prev: mmap --- Memory-mapped file support,  Up: Optional Operating System Services

5.16.8 `readline' -- GNU readline interface
-------------------------------------------

The *note readline: 145. module defines a number of functions to
facilitate completion and reading/writing of history files from the
Python interpreter.  This module can be used directly or via the *note
rlcompleter: 14a. module.  Settings made using  this module affect the
behaviour of both the interpreter's interactive prompt  and the prompts
offered by the *note raw_input(): 83a. and *note input(): 3ae. built-in
functions.

     Note: On MacOS X the *note readline: 145. module can be
     implemented using the `libedit' library instead of GNU readline.

     The configuration file for `libedit' is different from that of GNU
     readline. If you programmatically load configuration strings you
     can check for the text "libedit" in `readline.__doc__' to
     differentiate between GNU readline and libedit.

  The *note readline: 145. module defines the following functions:

 -- Function: readline.parse_and_bind (string)
     Parse and execute single line of a readline init file.

 -- Function: readline.get_line_buffer ()
     Return the current contents of the line buffer.

 -- Function: readline.insert_text (string)
     Insert text into the command line.

 -- Function: readline.read_init_file ([filename])
     Parse a readline initialization file. The default filename is the
     last filename used.

 -- Function: readline.read_history_file ([filename])
     Load a readline history file. The default filename is `~/.history'.

 -- Function: readline.write_history_file ([filename])
     Save a readline history file. The default filename is `~/.history'.

 -- Function: readline.clear_history ()
     Clear the current history.  (Note: this function is not available
     if the installed version of GNU readline doesn't support it.)

     New in version 2.4.

 -- Function: readline.get_history_length ()
     Return the desired length of the history file.  Negative values
     imply unlimited history file size.

 -- Function: readline.set_history_length (length)
     Set the number of lines to save in the history file. *note
     write_history_file(): 1651.  uses this value to truncate the
     history file when saving.  Negative values imply unlimited history
     file size.

 -- Function: readline.get_current_history_length ()
     Return the number of lines currently in the history.  (This is
     different from *note get_history_length(): 1653, which returns the
     maximum number of lines that will be written to a history file.)

     New in version 2.3.

 -- Function: readline.get_history_item (index)
     Return the current contents of history item at _index_.

     New in version 2.3.

 -- Function: readline.remove_history_item (pos)
     Remove history item specified by its position from the history.

     New in version 2.4.

 -- Function: readline.replace_history_item (pos, line)
     Replace history item specified by its position with the given line.

     New in version 2.4.

 -- Function: readline.redisplay ()
     Change what's displayed on the screen to reflect the current
     contents of the line buffer.

     New in version 2.3.

 -- Function: readline.set_startup_hook ([function])
     Set or remove the startup_hook function.  If _function_ is
     specified, it will be used as the new startup_hook function; if
     omitted or `None', any hook function already installed is removed.
     The startup_hook function is called with no arguments just before
     readline prints the first prompt.

 -- Function: readline.set_pre_input_hook ([function])
     Set or remove the pre_input_hook function.  If _function_ is
     specified, it will be used as the new pre_input_hook function; if
     omitted or `None', any hook function already installed is removed.
     The pre_input_hook function is called with no arguments after the
     first prompt has been printed and just before readline starts
     reading input characters.

 -- Function: readline.set_completer ([function])
     Set or remove the completer function.  If _function_ is specified,
     it will be used as the new completer function; if omitted or
     `None', any completer function already installed is removed.  The
     completer function is called as `function(text, state)', for
     _state_ in `0', `1', `2', ..., until it returns a non-string
     value.  It should return the next possible completion starting
     with _text_.

 -- Function: readline.get_completer ()
     Get the completer function, or `None' if no completer function has
     been set.

     New in version 2.3.

 -- Function: readline.get_completion_type ()
     Get the type of completion being attempted.

     New in version 2.6.

 -- Function: readline.get_begidx ()
     Get the beginning index of the readline tab-completion scope.

 -- Function: readline.get_endidx ()
     Get the ending index of the readline tab-completion scope.

 -- Function: readline.set_completer_delims (string)
     Set the readline word delimiters for tab-completion.

 -- Function: readline.get_completer_delims ()
     Get the readline word delimiters for tab-completion.

 -- Function: readline.set_completion_display_matches_hook ([function])
     Set or remove the completion display function.  If _function_ is
     specified, it will be used as the new completion display function;
     if omitted or `None', any completion display function already
     installed is removed.  The completion display function is called as
     `function(substitution, [matches], longest_match_length)' once
     each time matches need to be displayed.

     New in version 2.6.

 -- Function: readline.add_history (line)
     Append a line to the history buffer, as if it was the last line
     typed.

See also
........

Module *note rlcompleter: 14a.
     Completion of Python identifiers at the interactive prompt.

* Menu:

* Example: Example<6>.


File: python.info,  Node: Example<6>,  Up: readline --- GNU readline interface

5.16.8.1 Example
................

The following example demonstrates how to use the *note readline: 145.
module's history reading and writing functions to automatically load
and save a history file named `.pyhist' from the user's home directory.
The code below would normally be executed automatically during
interactive sessions from the user's *note PYTHONSTARTUP: 500. file.

    import os
    import readline
    histfile = os.path.join(os.path.expanduser("~"), ".pyhist")
    try:
        readline.read_history_file(histfile)
    except IOError:
        pass
    import atexit
    atexit.register(readline.write_history_file, histfile)
    del os, histfile

The following example extends the *note code.InteractiveConsole: 1667.
class to support history save/restore.

    import code
    import readline
    import atexit
    import os

    class HistoryConsole(code.InteractiveConsole):
        def __init__(self, locals=None, filename="<console>",
                     histfile=os.path.expanduser("~/.console-history")):
            code.InteractiveConsole.__init__(self, locals, filename)
            self.init_history(histfile)

        def init_history(self, histfile):
            readline.parse_and_bind("tab: complete")
            if hasattr(readline, "read_history_file"):
                try:
                    readline.read_history_file(histfile)
                except IOError:
                    pass
                atexit.register(self.save_history, histfile)

        def save_history(self, histfile):
            readline.write_history_file(histfile)



File: python.info,  Node: rlcompleter --- Completion function for GNU readline,  Prev: readline --- GNU readline interface,  Up: Optional Operating System Services

5.16.9 `rlcompleter' -- Completion function for GNU readline
------------------------------------------------------------

The *note rlcompleter: 14a. module defines a completion function
suitable for the *note readline: 145. module by completing valid Python
identifiers and keywords.

  When this module is imported on a Unix platform with the *note
readline: 145. module available, an instance of the `Completer' class
is automatically created and its `complete()' method is set as the
*note readline: 145. completer.

  Example:

    >>> import rlcompleter
    >>> import readline
    >>> readline.parse_and_bind("tab: complete")
    >>> readline. <TAB PRESSED>
    readline.__doc__          readline.get_line_buffer(  readline.read_init_file(
    readline.__file__         readline.insert_text(      readline.set_completer(
    readline.__name__         readline.parse_and_bind(
    >>> readline.

The *note rlcompleter: 14a. module is designed for use with Python's
interactive mode.  A user can add the following lines to his or her
initialization file (identified by the *note PYTHONSTARTUP: 500.
environment variable) to get automatic `Tab' completion:

    try:
        import readline
    except ImportError:
        print "Module readline not available."
    else:
        import rlcompleter
        readline.parse_and_bind("tab: complete")

On platforms without *note readline: 145, the `Completer' class defined
by this module can still be used for custom purposes.

* Menu:

* Completer Objects::


File: python.info,  Node: Completer Objects,  Up: rlcompleter --- Completion function for GNU readline

5.16.9.1 Completer Objects
..........................

Completer objects have the following method:

 -- Method: Completer.complete (text, state)
     Return the _state_th completion for _text_.

     If called for _text_ that doesn't include a period character
     (`'.''), it will complete from names currently defined in *note
     __main__: 2, *note __builtin__: 0. and keywords (as defined by the
     *note keyword: fe. module).

     If called for a dotted name, it will try to evaluate anything
     without obvious side-effects (functions will not be evaluated, but
     it can generate calls to *note __getattr__(): 320.) up to the last
     part, and find matches for the rest via the *note dir(): 329.
     function.  Any exception raised during the evaluation of the
     expression is caught, silenced and *note None: 389. is returned.


File: python.info,  Node: Interprocess Communication and Networking,  Next: Internet Data Handling,  Prev: Optional Operating System Services,  Up: The Python Standard Library

5.17 Interprocess Communication and Networking
==============================================

The modules described in this chapter provide mechanisms for different
processes to communicate.

  Some modules only work for two processes that are on the same
machine, e.g.  *note signal: 156. and *note subprocess: 168.  Other
modules support networking protocols that two or more processes can
used to communicate across machines.

  The list of modules described in this chapter is:

* Menu:

* subprocess: subprocess --- Subprocess management. Subprocess management
* socket: socket --- Low-level networking interface. Low-level networking interface
* ssl: ssl --- TLS/SSL wrapper for socket objects. TLS/SSL wrapper for socket objects
* signal: signal --- Set handlers for asynchronous events. Set handlers for asynchronous events
* popen2: popen2 --- Subprocesses with accessible I/O streams. Subprocesses with accessible I/O streams
* asyncore: asyncore --- Asynchronous socket handler. Asynchronous socket handler
* asynchat: asynchat --- Asynchronous socket command/response handler. Asynchronous socket command/response handler

subprocess --- Subprocess management

* Using the subprocess Module::
* Popen Objects::
* Windows Popen Helpers::
* Replacing Older Functions with the subprocess Module::
* Notes::

Using the subprocess Module

* Convenience Functions::
* Exceptions: Exceptions<4>.
* Security::

Windows Popen Helpers

* Constants: Constants<4>.

Replacing Older Functions with the subprocess Module

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::

Notes

* Converting an argument sequence to a string on Windows::

socket --- Low-level networking interface

* Socket Objects::
* Example: Example<7>.

ssl --- TLS/SSL wrapper for socket objects

* Functions, Constants, and Exceptions: Functions Constants and Exceptions.
* SSLSocket Objects::
* Certificates::
* Examples: Examples<8>.

Examples

* Testing for SSL support::
* Client-side operation::
* Server-side operation::

signal --- Set handlers for asynchronous events

* Example: Example<8>.

popen2 --- Subprocesses with accessible I/O streams

* Popen3 and Popen4 Objects::
* Flow Control Issues::

asyncore --- Asynchronous socket handler

* asyncore Example basic HTTP client::
* asyncore Example basic echo server::

asynchat --- Asynchronous socket command/response handler

* asynchat - Auxiliary Classes::
* asynchat Example::


File: python.info,  Node: subprocess --- Subprocess management,  Next: socket --- Low-level networking interface,  Up: Interprocess Communication and Networking

5.17.1 `subprocess' -- Subprocess management
--------------------------------------------

New in version 2.4.

  The *note subprocess: 168. module allows you to spawn new processes,
connect to their input/output/error pipes, and obtain their return
codes.  This module intends to replace several other, older modules and
functions, such as:

    os.system
    os.spawn*
    os.popen*
    popen2.*
    commands.*

Information about how the *note subprocess: 168. module can be used to
replace these modules and functions can be found in the following
sections.

See also
........

PEP 324(1) - PEP proposing the subprocess module

* Menu:

* Using the subprocess Module::
* Popen Objects::
* Windows Popen Helpers::
* Replacing Older Functions with the subprocess Module::
* Notes::

Using the subprocess Module

* Convenience Functions::
* Exceptions: Exceptions<4>.
* Security::

Windows Popen Helpers

* Constants: Constants<4>.

Replacing Older Functions with the subprocess Module

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::

Notes

* Converting an argument sequence to a string on Windows::

  ---------- Footnotes ----------

  (1) http://www.python.org/dev/peps/pep-0324


File: python.info,  Node: Using the subprocess Module,  Next: Popen Objects,  Up: subprocess --- Subprocess management

5.17.1.1 Using the subprocess Module
....................................

This module defines one class called *note Popen: 1673.:

 -- Class: subprocess.Popen (args, bufsize=0, executable=None,
          stdin=None, stdout=None, stderr=None, preexec_fn=None,
          close_fds=False, shell=False, cwd=None, env=None,
          universal_newlines=False, startupinfo=None, creationflags=0)
     Arguments are:

     _args_ should be a string, or a sequence of program arguments.
     The program to execute is normally the first item in the args
     sequence or the string if a string is given, but can be explicitly
     set by using the _executable_ argument.  When _executable_ is
     given, the first item in the args sequence is still treated by
     most programs as the command name, which can then be different
     from the actual executable name.  On Unix, it becomes the display
     name for the executing program in utilities such as *ps*.

     On Unix, with _shell=False_ (default): In this case, the Popen
     class uses *note os.execvp(): 110c. to execute the child program.
     _args_ should normally be a sequence.  If a string is specified
     for _args_, it will be used as the name or path of the program to
     execute; this will only work if the program is being given no
     arguments.

          Note: *note shlex.split(): 1674. can be useful when
          determining the correct tokenization for _args_, especially
          in complex cases:

              >>> import shlex, subprocess
              >>> command_line = raw_input()
              /bin/vikings -input eggs.txt -output "spam spam.txt" -cmd "echo '$MONEY'"
              >>> args = shlex.split(command_line)
              >>> print args
              ['/bin/vikings', '-input', 'eggs.txt', '-output', 'spam spam.txt', '-cmd', "echo '$MONEY'"]
              >>> p = subprocess.Popen(args) # Success!

          Note in particular that options (such as _-input_) and
          arguments (such as _eggs.txt_) that are separated by
          whitespace in the shell go in separate list elements, while
          arguments that need quoting or backslash escaping when used
          in the shell (such as filenames containing spaces or the
          _echo_ command shown above) are single list elements.

     On Unix, with _shell=True_: If args is a string, it specifies the
     command string to execute through the shell.  This means that the
     string must be formatted exactly as it would be when typed at the
     shell prompt.  This includes, for example, quoting or backslash
     escaping filenames with spaces in them.  If _args_ is a sequence,
     the first item specifies the command string, and any additional
     items will be treated as additional arguments to the shell itself.
     That is to say, _Popen_ does the equivalent of:

         Popen(['/bin/sh', '-c', args[0], args[1], ...])


          Warning: Executing shell commands that incorporate
          unsanitized input from an untrusted source makes a program
          vulnerable to shell injection(1), a serious security flaw
          which can result in arbitrary command execution.  For this
          reason, the use of _shell=True_ is *strongly discouraged* in
          cases where the command string is constructed from external
          input:

              >>> from subprocess import call
              >>> filename = input("What file would you like to display?\n")
              What file would you like to display?
              non_existent; rm -rf / #
              >>> call("cat " + filename, shell=True) # Uh-oh. This will end badly...

          _shell=False_ does not suffer from this vulnerability; the
          above Note may be helpful in getting code using _shell=False_
          to work.

     On Windows: the *note Popen: 1673. class uses CreateProcess() to
     execute the child child program, which operates on strings.  If
     _args_ is a sequence, it will be converted to a string in a manner
     described in *note Converting an argument sequence to a string on
     Windows: 1675.

     _bufsize_, if given, has the same meaning as the corresponding
     argument to the built-in open() function: `0' means unbuffered,
     `1' means line buffered, any other positive value means use a
     buffer of (approximately) that size.  A negative _bufsize_ means
     to use the system default, which usually means fully buffered.
     The default value for _bufsize_ is `0' (unbuffered).

          Note: If you experience performance issues, it is recommended
          that you try to enable buffering by setting _bufsize_ to
          either -1 or a large enough positive value (such as 4096).

     The _executable_ argument specifies the program to execute. It is
     very seldom needed: Usually, the program to execute is defined by
     the _args_ argument. If `shell=True', the _executable_ argument
     specifies which shell to use. On Unix, the default shell is
     `/bin/sh'.  On Windows, the default shell is specified by the `COMSPEC'
     environment variable. The only reason you would need to specify
     `shell=True' on Windows is where the command you wish to execute
     is actually built in to the shell, eg `dir', `copy'.  You don't
     need `shell=True' to run a batch file, nor to run a console-based
     executable.

     _stdin_, _stdout_ and _stderr_ specify the executed programs'
     standard input, standard output and standard error file handles,
     respectively.  Valid values are *note PIPE: 1676, an existing file
     descriptor (a positive integer), an existing file object, and
     `None'.  *note PIPE: 1676. indicates that a new pipe to the child
     should be created.  With `None', no redirection will occur; the
     child's file handles will be inherited from the parent.
     Additionally, _stderr_ can be *note STDOUT: 1677, which indicates
     that the stderr data from the applications should be captured into
     the same file handle as for stdout.

     If _preexec_fn_ is set to a callable object, this object will be
     called in the child process just before the child is executed.
     (Unix only)

     If _close_fds_ is true, all file descriptors except `0', `1' and
     `2' will be closed before the child process is executed. (Unix
     only).  Or, on Windows, if _close_fds_ is true then no handles
     will be inherited by the child process.  Note that on Windows, you
     cannot set _close_fds_ to true and also redirect the standard
     handles by setting _stdin_, _stdout_ or _stderr_.

     If _shell_ is *note True: 39f, the specified command will be
     executed through the shell.

     If _cwd_ is not `None', the child's current directory will be
     changed to _cwd_ before it is executed.  Note that this directory
     is not considered when searching the executable, so you can't
     specify the program's path relative to _cwd_.

     If _env_ is not `None', it must be a mapping that defines the
     environment variables for the new process; these are used instead
     of inheriting the current process' environment, which is the
     default behavior.

          Note: If specified, _env_ must provide any variables required
          for the program to execute.  On Windows, in order to run a
          side-by-side assembly(2) the specified _env_ *must* include a
          valid `SystemRoot'.

     If _universal_newlines_ is *note True: 39f, the file objects
     stdout and stderr are opened as text files, but lines may be
     terminated by any of `'\n'', the Unix end-of-line convention,
     `'\r'', the old Macintosh convention or `'\r\n'', the Windows
     convention. All of these external representations are seen as
     `'\n'' by the Python program.

          Note: This feature is only available if Python is built with
          universal newline support (the default).  Also, the newlines
          attribute of the file objects *note stdout: 1678, *note
          stdin: 1679. and *note stderr: 167a. are not updated by the
          communicate() method.

     If given, _startupinfo_ will be a *note STARTUPINFO: 167b. object,
     which is passed to the underlying `CreateProcess' function.
     _creationflags_, if given, can be *note CREATE_NEW_CONSOLE: 167c.
     or *note CREATE_NEW_PROCESS_GROUP: 167d. (Windows only)

 -- Data: subprocess.PIPE
     Special value that can be used as the _stdin_, _stdout_ or
     _stderr_ argument to *note Popen: 1673. and indicates that a pipe
     to the standard stream should be opened.

 -- Data: subprocess.STDOUT
     Special value that can be used as the _stderr_ argument to *note
     Popen: 1673. and indicates that standard error should go into the
     same handle as standard output.

* Menu:

* Convenience Functions::
* Exceptions: Exceptions<4>.
* Security::

  ---------- Footnotes ----------

  (1) http://en.wikipedia.org/wiki/Shell_injection#Shell_injection

  (2) http://en.wikipedia.org/wiki/Side-by-Side_Assembly


File: python.info,  Node: Convenience Functions,  Next: Exceptions<4>,  Up: Using the subprocess Module

5.17.1.2 Convenience Functions
..............................

This module also defines the following shortcut functions:

 -- Function: subprocess.call (*popenargs, **kwargs)
     Run command with arguments.  Wait for command to complete, then
     return the `returncode' attribute.

     The arguments are the same as for the *note Popen: 1673.
     constructor.  Example:

         >>> retcode = subprocess.call(["ls", "-l"])


          Warning: Like *note Popen.wait(): 1680, this will deadlock
          when using `stdout=PIPE' and/or `stderr=PIPE' and the child
          process generates enough output to a pipe such that it blocks
          waiting for the OS pipe buffer to accept more data.

 -- Function: subprocess.check_call (*popenargs, **kwargs)
     Run command with arguments.  Wait for command to complete. If the
     exit code was zero then return, otherwise raise
     `CalledProcessError'. The `CalledProcessError' object will have
     the return code in the `returncode' attribute.

     The arguments are the same as for the *note Popen: 1673.
     constructor.  Example:

         >>> subprocess.check_call(["ls", "-l"])
         0

     New in version 2.5.

          Warning: See the warning for *note call(): 167f.

 -- Function: subprocess.check_output (*popenargs, **kwargs)
     Run command with arguments and return its output as a byte string.

     If the exit code was non-zero it raises a `CalledProcessError'.
     The `CalledProcessError' object will have the return code in the
     `returncode' attribute and output in the `output' attribute.

     The arguments are the same as for the *note Popen: 1673.
     constructor.  Example:

         >>> subprocess.check_output(["ls", "-l", "/dev/null"])
         'crw-rw-rw- 1 root root 1, 3 Oct 18  2007 /dev/null\n'

     The stdout argument is not allowed as it is used internally.  To
     capture standard error in the result, use
     `stderr=subprocess.STDOUT':

         >>> subprocess.check_output(
         ...     ["/bin/sh", "-c", "ls non_existent_file; exit 0"],
         ...     stderr=subprocess.STDOUT)
         'ls: non_existent_file: No such file or directory\n'

     New in version 2.7.


File: python.info,  Node: Exceptions<4>,  Next: Security,  Prev: Convenience Functions,  Up: Using the subprocess Module

5.17.1.3 Exceptions
...................

Exceptions raised in the child process, before the new program has
started to execute, will be re-raised in the parent.  Additionally, the
exception object will have one extra attribute called
`child_traceback', which is a string containing traceback information
from the child's point of view.

  The most common exception raised is *note OSError: 22e.  This occurs,
for example, when trying to execute a non-existent file.  Applications
should prepare for *note OSError: 22e. exceptions.

  A *note ValueError: 233. will be raised if *note Popen: 1673. is
called with invalid arguments.

  check_call() will raise `CalledProcessError', if the called process
returns a non-zero return code.


File: python.info,  Node: Security,  Prev: Exceptions<4>,  Up: Using the subprocess Module

5.17.1.4 Security
.................

Unlike some other popen functions, this implementation will never call
/bin/sh implicitly.  This means that all characters, including shell
metacharacters, can safely be passed to child processes.


File: python.info,  Node: Popen Objects,  Next: Windows Popen Helpers,  Prev: Using the subprocess Module,  Up: subprocess --- Subprocess management

5.17.1.5 Popen Objects
......................

Instances of the *note Popen: 1673. class have the following methods:

 -- Method: Popen.poll ()
     Check if child process has terminated.  Set and return *note
     returncode: 1686.  attribute.

 -- Method: Popen.wait ()
     Wait for child process to terminate.  Set and return *note
     returncode: 1686.  attribute.

          Warning: This will deadlock when using `stdout=PIPE' and/or
          `stderr=PIPE' and the child process generates enough output to
          a pipe such that it blocks waiting for the OS pipe buffer to
          accept more data.  Use *note communicate(): 1687. to avoid
          that.

 -- Method: Popen.communicate (input=None)
     Interact with process: Send data to stdin.  Read data from stdout
     and stderr, until end-of-file is reached.  Wait for process to
     terminate. The optional _input_ argument should be a string to be
     sent to the child process, or `None', if no data should be sent to
     the child.

     *note communicate(): 1687. returns a tuple `(stdoutdata,
     stderrdata)'.

     Note that if you want to send data to the process's stdin, you
     need to create the Popen object with `stdin=PIPE'.  Similarly, to
     get anything other than `None' in the result tuple, you need to
     give `stdout=PIPE' and/or `stderr=PIPE' too.

          Note: The data read is buffered in memory, so do not use this
          method if the data size is large or unlimited.

 -- Method: Popen.send_signal (signal)
     Sends the signal _signal_ to the child.

          Note: On Windows, SIGTERM is an alias for *note terminate():
          1689. CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to
          processes started with a _creationflags_ parameter which
          includes `CREATE_NEW_PROCESS_GROUP'.

     New in version 2.6.

 -- Method: Popen.terminate ()
     Stop the child. On Posix OSs the method sends SIGTERM to the
     child. On Windows the Win32 API function `TerminateProcess()' is
     called to stop the child.

     New in version 2.6.

 -- Method: Popen.kill ()
     Kills the child. On Posix OSs the function sends SIGKILL to the
     child.  On Windows *note kill(): 168a. is an alias for *note
     terminate(): 1689.

     New in version 2.6.

  The following attributes are also available:

     Warning: Use `communicate()' rather than `.stdin.write',
     `.stdout.read' or `.stderr.read' to avoid deadlocks due to any of
     the other OS pipe buffers filling up and blocking the child
     process.

 -- Attribute: Popen.stdin
     If the _stdin_ argument was *note PIPE: 1676, this attribute is a
     file object that provides input to the child process.  Otherwise,
     it is `None'.

 -- Attribute: Popen.stdout
     If the _stdout_ argument was *note PIPE: 1676, this attribute is a
     file object that provides output from the child process.
     Otherwise, it is `None'.

 -- Attribute: Popen.stderr
     If the _stderr_ argument was *note PIPE: 1676, this attribute is a
     file object that provides error output from the child process.
     Otherwise, it is `None'.

 -- Attribute: Popen.pid
     The process ID of the child process.

     Note that if you set the _shell_ argument to `True', this is the
     process ID of the spawned shell.

 -- Attribute: Popen.returncode
     The child return code, set by *note poll(): 1685. and *note
     wait(): 1680. (and indirectly by *note communicate(): 1687.).  A
     `None' value indicates that the process hasn't terminated yet.

     A negative value `-N' indicates that the child was terminated by
     signal `N' (Unix only).


File: python.info,  Node: Windows Popen Helpers,  Next: Replacing Older Functions with the subprocess Module,  Prev: Popen Objects,  Up: subprocess --- Subprocess management

5.17.1.6 Windows Popen Helpers
..............................

The *note STARTUPINFO: 167b. class and following constants are only
available on Windows.

 -- Class: subprocess.STARTUPINFO
     Partial support of the Windows STARTUPINFO(1) structure is used
     for *note Popen: 1673. creation.

      -- Attribute: dwFlags
          A bit field that determines whether certain *note
          STARTUPINFO: 167b. members are used when the process creates
          a window.

              si = subprocess.STARTUPINFO()
              si.dwFlags = subprocess.STARTF_USESTDHANDLES | subprocess.STARTF_USESHOWWINDOW



      -- Attribute: hStdInput
          If *note dwFlags: 168d. specifies *note STARTF_USESTDHANDLES:
          168f, this member is the standard input handle for the
          process. If *note STARTF_USESTDHANDLES: 168f.  is not
          specified, the default for standard input is the keyboard
          buffer.

      -- Attribute: hStdOutput
          If *note dwFlags: 168d. specifies *note STARTF_USESTDHANDLES:
          168f, this member is the standard output handle for the
          process. Otherwise, this member is ignored and the default
          for standard output is the console window's buffer.

      -- Attribute: hStdError
          If *note dwFlags: 168d. specifies *note STARTF_USESTDHANDLES:
          168f, this member is the standard error handle for the
          process. Otherwise, this member is ignored and the default
          for standard error is the console window's buffer.

      -- Attribute: wShowWindow
          If *note dwFlags: 168d. specifies *note STARTF_USESHOWWINDOW:
          1693, this member can be any of the values that can be
          specified in the `nCmdShow' parameter for the ShowWindow(2)
          function, except for `SW_SHOWDEFAULT'. Otherwise, this member
          is ignored.

          *note SW_HIDE: 1694. is provided for this attribute. It is
          used when *note Popen: 1673. is called with `shell=True'.

* Menu:

* Constants: Constants<4>.

  ---------- Footnotes ----------

  (1) http://msdn.microsoft.com/en-us/library/ms686331(v=vs.85).aspx

  (2) http://msdn.microsoft.com/en-us/library/ms633548(v=vs.85).aspx


File: python.info,  Node: Constants<4>,  Up: Windows Popen Helpers

5.17.1.7 Constants
..................

The *note subprocess: 168. module exposes the following constants.

 -- Data: subprocess.STD_INPUT_HANDLE
     The standard input device. Initially, this is the console input
     buffer, `CONIN$'.

 -- Data: subprocess.STD_OUTPUT_HANDLE
     The standard output device. Initially, this is the active console
     screen buffer, `CONOUT$'.

 -- Data: subprocess.STD_ERROR_HANDLE
     The standard error device. Initially, this is the active console
     screen buffer, `CONOUT$'.

 -- Data: subprocess.SW_HIDE
     Hides the window. Another window will be activated.

 -- Data: subprocess.STARTF_USESTDHANDLES
     Specifies that the *note STARTUPINFO.hStdInput: 168e, *note
     STARTUPINFO.hStdOutput: 1690, and *note STARTUPINFO.hStdError:
     1691. members contain additional information.

 -- Data: subprocess.STARTF_USESHOWWINDOW
     Specifies that the *note STARTUPINFO.wShowWindow: 1692. member
     contains additional information.

 -- Data: subprocess.CREATE_NEW_CONSOLE
     The new process has a new console, instead of inheriting its
     parent's console (the default).

     This flag is always set when *note Popen: 1673. is created with
     `shell=True'.

 -- Data: subprocess.CREATE_NEW_PROCESS_GROUP
     A *note Popen: 1673. `creationflags' parameter to specify that a
     new process group will be created. This flag is necessary for
     using *note os.kill(): 2c6.  on the subprocess.

     This flag is ignored if *note CREATE_NEW_CONSOLE: 167c. is
     specified.


File: python.info,  Node: Replacing Older Functions with the subprocess Module,  Next: Notes,  Prev: Windows Popen Helpers,  Up: subprocess --- Subprocess management

5.17.1.8 Replacing Older Functions with the subprocess Module
.............................................................

In this section, "a ==> b" means that b can be used as a replacement
for a.

     Note: All functions in this section fail (more or less) silently
     if the executed program cannot be found; this module raises an
     *note OSError: 22e. exception.

  In the following examples, we assume that the subprocess module is
imported with "from subprocess import *".

* Menu:

* Replacing /bin/sh shell backquote::
* Replacing shell pipeline::
* Replacing os.system(): Replacing os system.
* Replacing the os.spawn family: Replacing the os spawn family.
* Replacing os.popen(), os.popen2(), os.popen3(): Replacing os popen os popen2 os popen3.
* Replacing functions from the popen2 module::


File: python.info,  Node: Replacing /bin/sh shell backquote,  Next: Replacing shell pipeline,  Up: Replacing Older Functions with the subprocess Module

5.17.1.9 Replacing /bin/sh shell backquote
..........................................

    output=`mycmd myarg`
    ==>
    output = Popen(["mycmd", "myarg"], stdout=PIPE).communicate()[0]



File: python.info,  Node: Replacing shell pipeline,  Next: Replacing os system,  Prev: Replacing /bin/sh shell backquote,  Up: Replacing Older Functions with the subprocess Module

5.17.1.10 Replacing shell pipeline
..................................

    output=`dmesg | grep hda`
    ==>
    p1 = Popen(["dmesg"], stdout=PIPE)
    p2 = Popen(["grep", "hda"], stdin=p1.stdout, stdout=PIPE)
    p1.stdout.close()  # Allow p1 to receive a SIGPIPE if p2 exits.
    output = p2.communicate()[0]

The p1.stdout.close() call after starting the p2 is important in order
for p1 to receive a SIGPIPE if p2 exits before p1.


File: python.info,  Node: Replacing os system,  Next: Replacing the os spawn family,  Prev: Replacing shell pipeline,  Up: Replacing Older Functions with the subprocess Module

5.17.1.11 Replacing `os.system()'
.................................

    sts = os.system("mycmd" + " myarg")
    ==>
    p = Popen("mycmd" + " myarg", shell=True)
    sts = os.waitpid(p.pid, 0)[1]

Notes:

   * Calling the program through the shell is usually not required.

   * It's easier to look at the `returncode' attribute than the exit
     status.

  A more realistic example would look like this:

    try:
        retcode = call("mycmd" + " myarg", shell=True)
        if retcode < 0:
            print >>sys.stderr, "Child was terminated by signal", -retcode
        else:
            print >>sys.stderr, "Child returned", retcode
    except OSError, e:
        print >>sys.stderr, "Execution failed:", e



File: python.info,  Node: Replacing the os spawn family,  Next: Replacing os popen os popen2 os popen3,  Prev: Replacing os system,  Up: Replacing Older Functions with the subprocess Module

5.17.1.12 Replacing the `os.spawn' family
.........................................

P_NOWAIT example:

    pid = os.spawnlp(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg")
    ==>
    pid = Popen(["/bin/mycmd", "myarg"]).pid

P_WAIT example:

    retcode = os.spawnlp(os.P_WAIT, "/bin/mycmd", "mycmd", "myarg")
    ==>
    retcode = call(["/bin/mycmd", "myarg"])

Vector example:

    os.spawnvp(os.P_NOWAIT, path, args)
    ==>
    Popen([path] + args[1:])

Environment example:

    os.spawnlpe(os.P_NOWAIT, "/bin/mycmd", "mycmd", "myarg", env)
    ==>
    Popen(["/bin/mycmd", "myarg"], env={"PATH": "/usr/bin"})



File: python.info,  Node: Replacing os popen os popen2 os popen3,  Next: Replacing functions from the popen2 module,  Prev: Replacing the os spawn family,  Up: Replacing Older Functions with the subprocess Module

5.17.1.13 Replacing `os.popen()', `os.popen2()', `os.popen3()'
..............................................................

    pipe = os.popen("cmd", 'r', bufsize)
    ==>
    pipe = Popen("cmd", shell=True, bufsize=bufsize, stdout=PIPE).stdout


    pipe = os.popen("cmd", 'w', bufsize)
    ==>
    pipe = Popen("cmd", shell=True, bufsize=bufsize, stdin=PIPE).stdin


    (child_stdin, child_stdout) = os.popen2("cmd", mode, bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdin, child_stdout) = (p.stdin, p.stdout)


    (child_stdin,
     child_stdout,
     child_stderr) = os.popen3("cmd", mode, bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
    (child_stdin,
     child_stdout,
     child_stderr) = (p.stdin, p.stdout, p.stderr)


    (child_stdin, child_stdout_and_stderr) = os.popen4("cmd", mode,
                                                       bufsize)
    ==>
    p = Popen("cmd", shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, stderr=STDOUT, close_fds=True)
    (child_stdin, child_stdout_and_stderr) = (p.stdin, p.stdout)

On Unix, os.popen2, os.popen3 and os.popen4 also accept a sequence as
the command to execute, in which case arguments will be passed directly
to the program without shell intervention.  This usage can be replaced
as follows:

    (child_stdin, child_stdout) = os.popen2(["/bin/ls", "-l"], mode,
                                            bufsize)
    ==>
    p = Popen(["/bin/ls", "-l"], bufsize=bufsize, stdin=PIPE, stdout=PIPE)
    (child_stdin, child_stdout) = (p.stdin, p.stdout)

Return code handling translates as follows:

    pipe = os.popen("cmd", 'w')
    ...
    rc = pipe.close()
    if rc is not None and rc >> 8:
        print "There were some errors"
    ==>
    process = Popen("cmd", 'w', shell=True, stdin=PIPE)
    ...
    process.stdin.close()
    if process.wait() != 0:
        print "There were some errors"



File: python.info,  Node: Replacing functions from the popen2 module,  Prev: Replacing os popen os popen2 os popen3,  Up: Replacing Older Functions with the subprocess Module

5.17.1.14 Replacing functions from the `popen2' module
......................................................

    (child_stdout, child_stdin) = popen2.popen2("somestring", bufsize, mode)
    ==>
    p = Popen(["somestring"], shell=True, bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdout, child_stdin) = (p.stdout, p.stdin)

On Unix, popen2 also accepts a sequence as the command to execute, in
which case arguments will be passed directly to the program without
shell intervention.  This usage can be replaced as follows:

    (child_stdout, child_stdin) = popen2.popen2(["mycmd", "myarg"], bufsize,
                                                mode)
    ==>
    p = Popen(["mycmd", "myarg"], bufsize=bufsize,
              stdin=PIPE, stdout=PIPE, close_fds=True)
    (child_stdout, child_stdin) = (p.stdout, p.stdin)

*note popen2.Popen3: 16a0. and *note popen2.Popen4: 16a1. basically
work as *note subprocess.Popen: 1673, except that:

   * *note Popen: 1673. raises an exception if the execution fails.

   * the _capturestderr_ argument is replaced with the _stderr_
     argument.

   * `stdin=PIPE' and `stdout=PIPE' must be specified.

   * popen2 closes all file descriptors by default, but you have to
     specify `close_fds=True' with *note Popen: 1673.


File: python.info,  Node: Notes,  Prev: Replacing Older Functions with the subprocess Module,  Up: subprocess --- Subprocess management

5.17.1.15 Notes
...............

* Menu:

* Converting an argument sequence to a string on Windows::


File: python.info,  Node: Converting an argument sequence to a string on Windows,  Up: Notes

5.17.1.16 Converting an argument sequence to a string on Windows
................................................................

On Windows, an _args_ sequence is converted to a string that can be
parsed using the following rules (which correspond to the rules used by
the MS C runtime):

  1. Arguments are delimited by white space, which is either a space or
     a tab.

  2. A string surrounded by double quotation marks is interpreted as a
     single argument, regardless of white space contained within.  A
     quoted string can be embedded in an argument.

  3. A double quotation mark preceded by a backslash is interpreted as
     a literal double quotation mark.

  4. Backslashes are interpreted literally, unless they immediately
     precede a double quotation mark.

  5. If backslashes immediately precede a double quotation mark, every
     pair of backslashes is interpreted as a literal backslash.  If the
     number of backslashes is odd, the last backslash escapes the next
     double quotation mark as described in rule 3.


File: python.info,  Node: socket --- Low-level networking interface,  Next: ssl --- TLS/SSL wrapper for socket objects,  Prev: subprocess --- Subprocess management,  Up: Interprocess Communication and Networking

5.17.2 `socket' -- Low-level networking interface
-------------------------------------------------

This module provides access to the BSD _socket_ interface. It is
available on all modern Unix systems, Windows, Mac OS X, BeOS, OS/2,
and probably additional platforms.

     Note: Some behavior may be platform dependent, since calls are
     made to the operating system socket APIs.

  For an introduction to socket programming (in C), see the following
papers: An Introductory 4.3BSD Interprocess Communication Tutorial, by
Stuart Sechrest and An Advanced 4.3BSD Interprocess Communication
Tutorial, by Samuel J.  Leffler et al, both in the UNIX Programmer's
Manual, Supplementary Documents 1 (sections PS1:7 and PS1:8).  The
platform-specific reference material for the various socket-related
system calls are also a valuable source of information on the details
of socket semantics.  For Unix, refer to the manual pages; for Windows,
see the WinSock (or Winsock 2) specification. For IPv6-ready APIs,
readers may want to refer to RFC 3493(1) titled Basic Socket Interface
Extensions for IPv6.

  The Python interface is a straightforward transliteration of the Unix
system call and library interface for sockets to Python's
object-oriented style: the *note socket(): 15d. function returns a
_socket object_ whose methods implement the various socket system
calls.  Parameter types are somewhat higher-level than in the C
interface: as with `read()' and `write()' operations on Python files,
buffer allocation on receive operations is automatic, and buffer length
is implicit on send operations.

  Socket addresses are represented as follows: A single string is used
for the *note AF_UNIX: 16a6. address family. A pair `(host, port)' is
used for the *note AF_INET: 16a7. address family, where _host_ is a
string representing either a hostname in Internet domain notation like
`'daring.cwi.nl'' or an IPv4 address like `'100.50.200.5'', and _port_
is an integral port number. For *note AF_INET6: 16a8. address family, a
four-tuple `(host, port, flowinfo, scopeid)' is used, where _flowinfo_
and _scopeid_ represents `sin6_flowinfo' and `sin6_scope_id' member in
`struct sockaddr_in6' in C. For *note socket: 15d. module methods,
_flowinfo_ and _scopeid_ can be omitted just for backward
compatibility. Note, however, omission of _scopeid_ can cause problems
in manipulating scoped IPv6 addresses. Other address families are
currently not supported. The address format required by a particular
socket object is automatically selected based on the address family
specified when the socket object was created.

  For IPv4 addresses, two special forms are accepted instead of a host
address: the empty string represents `INADDR_ANY', and the string
`'<broadcast>'' represents `INADDR_BROADCAST'. The behavior is not
available for IPv6 for backward compatibility, therefore, you may want
to avoid these if you intend to support IPv6 with your Python programs.

  If you use a hostname in the _host_ portion of IPv4/v6 socket
address, the program may show a nondeterministic behavior, as Python
uses the first address returned from the DNS resolution.  The socket
address will be resolved differently into an actual IPv4/v6 address,
depending on the results from DNS resolution and/or the host
configuration.  For deterministic behavior use a numeric address in
_host_ portion.

  New in version 2.5: AF_NETLINK sockets are represented as  pairs
`pid, groups'.

  New in version 2.6: Linux-only support for TIPC is also available
using the `AF_TIPC' address family. TIPC is an open, non-IP based
networked protocol designed for use in clustered computer environments.
Addresses are represented by a tuple, and the fields depend on the
address type. The general tuple form is `(addr_type, v1, v2, v3 [,
scope])', where:

   - _addr_type_ is one of TIPC_ADDR_NAMESEQ, TIPC_ADDR_NAME, or
     TIPC_ADDR_ID.

   - _scope_ is one of TIPC_ZONE_SCOPE, TIPC_CLUSTER_SCOPE, and
     TIPC_NODE_SCOPE.

   - If _addr_type_ is TIPC_ADDR_NAME, then _v1_ is the server type,
     _v2_ is the port identifier, and _v3_ should be 0.

     If _addr_type_ is TIPC_ADDR_NAMESEQ, then _v1_ is the server type,
     _v2_ is the lower port number, and _v3_ is the upper port number.

     If _addr_type_ is TIPC_ADDR_ID, then _v1_ is the node, _v2_ is the
     reference, and _v3_ should be set to 0.

  All errors raise exceptions.  The normal exceptions for invalid
argument types and out-of-memory conditions can be raised; errors
related to socket or address semantics raise the error *note
socket.error: 370.

  Non-blocking mode is supported through *note setblocking(): 16a9.  A
generalization of this based on timeouts is supported through *note
settimeout(): 16aa.

  The module *note socket: 15d. exports the following constants and
functions:

 -- Exception: socket.error
     This exception is raised for socket-related errors. The
     accompanying value is either a string telling what went wrong or a
     pair `(errno, string)' representing an error returned by a system
     call, similar to the value accompanying *note os.error: dba. See
     the module *note errno: c9, which contains names for the error
     codes defined by the underlying operating system.

     Changed in version 2.6: *note socket.error: 370. is now a child
     class of *note IOError: 1f7.

 -- Exception: socket.herror
     This exception is raised for address-related errors, i.e. for
     functions that use _h_errno_ in the C API, including *note
     gethostbyname_ex(): 16ac. and *note gethostbyaddr(): 16ad.

     The accompanying value is a pair `(h_errno, string)' representing
     an error returned by a library call. _string_ represents the
     description of _h_errno_, as returned by the `hstrerror()' C
     function.

 -- Exception: socket.gaierror
     This exception is raised for address-related errors, for *note
     getaddrinfo(): 16af. and *note getnameinfo(): 16b0. The
     accompanying value is a pair `(error, string)' representing an
     error returned by a library call. _string_ represents the
     description of _error_, as returned by the `gai_strerror()' C
     function. The _error_ value will match one of the `EAI_*'
     constants defined in this module.

 -- Exception: socket.timeout
     This exception is raised when a timeout occurs on a socket which
     has had timeouts enabled via a prior call to `settimeout()'.  The
     accompanying value is a string whose value is currently always
     "timed out".

     New in version 2.3.

 -- Data: socket.AF_UNIX
 -- Data: socket.AF_INET
 -- Data: socket.AF_INET6
     These constants represent the address (and protocol) families,
     used for the first argument to *note socket(): 15d.  If the *note
     AF_UNIX: 16a6. constant is not defined then this protocol is
     unsupported.

 -- Data: socket.SOCK_STREAM
 -- Data: socket.SOCK_DGRAM
 -- Data: socket.SOCK_RAW
 -- Data: socket.SOCK_RDM
 -- Data: socket.SOCK_SEQPACKET
     These constants represent the socket types, used for the second
     argument to *note socket(): 15d. (Only *note SOCK_STREAM: 1d9. and
     *note SOCK_DGRAM: 1d8. appear to be generally useful.)

 -- Data: SO_*
 -- Data: socket.SOMAXCONN
 -- Data: MSG_*
 -- Data: SOL_*
 -- Data: IPPROTO_*
 -- Data: IPPORT_*
 -- Data: INADDR_*
 -- Data: IP_*
 -- Data: IPV6_*
 -- Data: EAI_*
 -- Data: AI_*
 -- Data: NI_*
 -- Data: TCP_*
     Many constants of these forms, documented in the Unix
     documentation on sockets and/or the IP protocol, are also defined
     in the socket module. They are generally used in arguments to the
     `setsockopt()' and `getsockopt()' methods of socket objects.  In
     most cases, only those symbols that are defined in the Unix header
     files are defined; for a few symbols, default values are provided.

 -- Data: SIO_*
 -- Data: RCVALL_*
     Constants for Windows' WSAIoctl(). The constants are used as
     arguments to the `ioctl()' method of socket objects.

     New in version 2.6.

 -- Data: TIPC_*
     TIPC related constants, matching the ones exported by the C socket
     API. See the TIPC documentation for more information.

     New in version 2.6.

 -- Data: socket.has_ipv6
     This constant contains a boolean value which indicates if IPv6 is
     supported on this platform.

     New in version 2.3.

 -- Function: socket.create_connection (address[, timeout[,
          source_address]])
     Convenience function.  Connect to _address_ (a 2-tuple `(host,
     port)'), and return the socket object.  Passing the optional
     _timeout_ parameter will set the timeout on the socket instance
     before attempting to connect.  If no _timeout_ is supplied, the
     global default timeout setting returned by *note
     getdefaulttimeout(): 16b6. is used.

     If supplied, _source_address_ must be a 2-tuple `(host, port)' for
     the socket to bind to as its source address before connecting.  If
     host or port are '' or 0 respectively the OS default behavior will
     be used.

     New in version 2.6.

     Changed in version 2.7: _source_address_ was added.

 -- Function: socket.getaddrinfo (host, port, family=0, socktype=0,
          proto=0, flags=0)
     Translate the _host_/_port_ argument into a sequence of 5-tuples
     that contain all the necessary arguments for creating a socket
     connected to that service.  _host_ is a domain name, a string
     representation of an IPv4/v6 address or `None'. _port_ is a string
     service name such as `'http'', a numeric port number or `None'.
     By passing `None' as the value of _host_ and _port_, you can pass
     `NULL' to the underlying C API.

     The _family_, _socktype_ and _proto_ arguments can be optionally
     specified in order to narrow the list of addresses returned.
     Passing zero as a value for each of these arguments selects the
     full range of results.  The _flags_ argument can be one or several
     of the `AI_*' constants, and will influence how results are
     computed and returned.  For example, `AI_NUMERICHOST' will disable
     domain name resolution and will raise an error if _host_ is a
     domain name.

     The function returns a list of 5-tuples with the following
     structure:

     `(family, socktype, proto, canonname, sockaddr)'

     In these tuples, _family_, _socktype_, _proto_ are all integers
     and are meant to be passed to the *note socket(): 15d. function.
     _canonname_ will be a string representing the canonical name of
     the _host_ if `AI_CANONNAME' is part of the _flags_ argument; else
     _canonname_ will be empty.  _sockaddr_ is a tuple describing a
     socket address, whose format depends on the returned _family_ (a
     `(address, port)' 2-tuple for *note AF_INET: 16a7, a `(address,
     port, flow info, scope id)' 4-tuple for *note AF_INET6: 16a8.),
     and is meant to be passed to the *note socket.connect(): 16b7.
     method.

     The following example fetches address information for a
     hypothetical TCP connection to `www.python.org' on port 80
     (results may differ on your system if IPv6 isn't enabled):

         >>> socket.getaddrinfo("www.python.org", 80, 0, 0, socket.SOL_TCP)
         [(2, 1, 6, '', ('82.94.164.162', 80)),
          (10, 1, 6, '', ('2001:888:2000:d::a2', 80, 0, 0))]

     New in version 2.2.

 -- Function: socket.getfqdn ([name])
     Return a fully qualified domain name for _name_. If _name_ is
     omitted or empty, it is interpreted as the local host.  To find
     the fully qualified name, the hostname returned by *note
     gethostbyaddr(): 16ad. is checked, followed by aliases for the
     host, if available.  The first name which includes a period is
     selected.  In case no fully qualified domain name is available,
     the hostname as returned by *note gethostname(): 10a7. is returned.

     New in version 2.0.

 -- Function: socket.gethostbyname (hostname)
     Translate a host name to IPv4 address format.  The IPv4 address is
     returned as a string, such as  `'100.50.200.5''.  If the host name
     is an IPv4 address itself it is returned unchanged.  See *note
     gethostbyname_ex(): 16ac. for a more complete interface. *note
     gethostbyname(): 16b9. does not support IPv6 name resolution, and
     *note getaddrinfo(): 16af. should be used instead for IPv4/v6 dual
     stack support.

 -- Function: socket.gethostbyname_ex (hostname)
     Translate a host name to IPv4 address format, extended interface.
     Return a triple `(hostname, aliaslist, ipaddrlist)' where
     _hostname_ is the primary host name responding to the given
     _ip_address_, _aliaslist_ is a (possibly empty) list of
     alternative host names for the same address, and _ipaddrlist_ is a
     list of IPv4 addresses for the same interface on the same host
     (often but not always a single address). *note gethostbyname_ex():
     16ac. does not support IPv6 name resolution, and *note
     getaddrinfo(): 16af. should be used instead for IPv4/v6 dual stack
     support.

 -- Function: socket.gethostname ()
     Return a string containing the hostname of the machine where  the
     Python interpreter is currently executing.

     If you want to know the current machine's IP address, you may want
     to use `gethostbyname(gethostname())'. This operation assumes that
     there is a valid address-to-host mapping for the host, and the
     assumption does not always hold.

     Note: *note gethostname(): 10a7. doesn't always return the fully
     qualified domain name; use `getfqdn()' (see above).

 -- Function: socket.gethostbyaddr (ip_address)
     Return a triple `(hostname, aliaslist, ipaddrlist)' where
     _hostname_ is the primary host name responding to the given
     _ip_address_, _aliaslist_ is a (possibly empty) list of
     alternative host names for the same address, and _ipaddrlist_ is a
     list of IPv4/v6 addresses for the same interface on the same host
     (most likely containing only a single address). To find the fully
     qualified domain name, use the function *note getfqdn(): 16b8.
     *note gethostbyaddr(): 16ad. supports both IPv4 and IPv6.

 -- Function: socket.getnameinfo (sockaddr, flags)
     Translate a socket address _sockaddr_ into a 2-tuple `(host,
     port)'. Depending on the settings of _flags_, the result can
     contain a fully-qualified domain name or numeric address
     representation in _host_.  Similarly, _port_ can contain a string
     port name or a numeric port number.

     New in version 2.2.

 -- Function: socket.getprotobyname (protocolname)
     Translate an Internet protocol name (for example, `'icmp'') to a
     constant suitable for passing as the (optional) third argument to
     the *note socket(): 15d.  function.  This is usually only needed
     for sockets opened in "raw" mode (*note SOCK_RAW: 16b1.); for the
     normal socket modes, the correct protocol is chosen automatically
     if the protocol is omitted or zero.

 -- Function: socket.getservbyname (servicename[, protocolname])
     Translate an Internet service name and protocol name to a port
     number for that service.  The optional protocol name, if given,
     should be `'tcp'' or `'udp'', otherwise any protocol will match.

 -- Function: socket.getservbyport (port[, protocolname])
     Translate an Internet port number and protocol name to a service
     name for that service.  The optional protocol name, if given,
     should be `'tcp'' or `'udp'', otherwise any protocol will match.

 -- Function: socket.socket ([family[, type[, proto]]])
     Create a new socket using the given address family, socket type
     and protocol number.  The address family should be *note AF_INET:
     16a7. (the default), *note AF_INET6: 16a8. or *note AF_UNIX: 16a6.
     The socket type should be *note SOCK_STREAM: 1d9. (the default),
     *note SOCK_DGRAM: 1d8. or perhaps one of the other `SOCK_'
     constants.  The protocol number is usually zero and may be omitted
     in that case.

 -- Function: socket.socketpair ([family[, type[, proto]]])
     Build a pair of connected socket objects using the given address
     family, socket type, and protocol number.  Address family, socket
     type, and protocol number are as for the *note socket(): 15d.
     function above. The default family is *note AF_UNIX: 16a6.  if
     defined on the platform; otherwise, the default is *note AF_INET:
     16a7.  Availability: Unix.

     New in version 2.4.

 -- Function: socket.fromfd (fd, family, type[, proto])
     Duplicate the file descriptor _fd_ (an integer as returned by a
     file object's `fileno()' method) and build a socket object from
     the result.  Address family, socket type and protocol number are
     as for the *note socket(): 15d. function above. The file
     descriptor should refer to a socket, but this is not checked --
     subsequent operations on the object may fail if the file
     descriptor is invalid.  This function is rarely needed, but can be
     used to get or set socket options on a socket passed to a program
     as standard input or output (such as a server started by the Unix
     inet daemon).  The socket is assumed to be in blocking mode.
     Availability: Unix.

 -- Function: socket.ntohl (x)
     Convert 32-bit positive integers from network to host byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 4-byte swap
     operation.

 -- Function: socket.ntohs (x)
     Convert 16-bit positive integers from network to host byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 2-byte swap
     operation.

 -- Function: socket.htonl (x)
     Convert 32-bit positive integers from host to network byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 4-byte swap
     operation.

 -- Function: socket.htons (x)
     Convert 16-bit positive integers from host to network byte order.
     On machines where the host byte order is the same as network byte
     order, this is a no-op; otherwise, it performs a 2-byte swap
     operation.

 -- Function: socket.inet_aton (ip_string)
     Convert an IPv4 address from dotted-quad string format (for
     example, '123.45.67.89') to 32-bit packed binary format, as a
     string four characters in length.  This is useful when conversing
     with a program that uses the standard C library and needs objects
     of type `struct in_addr', which is the C type for the 32-bit
     packed binary this function returns.

     *note inet_aton(): 16c3. also accepts strings with less than three
     dots; see the Unix manual page `inet(3)' for details.

     If the IPv4 address string passed to this function is invalid,
     *note socket.error: 370. will be raised. Note that exactly what is
     valid depends on the underlying C implementation of `inet_aton()'.

     *note inet_aton(): 16c3. does not support IPv6, and *note
     inet_pton(): 16c4. should be used instead for IPv4/v6 dual stack
     support.

 -- Function: socket.inet_ntoa (packed_ip)
     Convert a 32-bit packed IPv4 address (a string four characters in
     length) to its standard dotted-quad string representation (for
     example, '123.45.67.89').  This is useful when conversing with a
     program that uses the standard C library and needs objects of type
     `struct in_addr', which is the C type for the 32-bit packed binary
     data this function takes as an argument.

     If the string passed to this function is not exactly 4 bytes in
     length, *note socket.error: 370. will be raised. *note
     inet_ntoa(): 16c5. does not support IPv6, and *note inet_ntop():
     16c6. should be used instead for IPv4/v6 dual stack support.

 -- Function: socket.inet_pton (address_family, ip_string)
     Convert an IP address from its family-specific string format to a
     packed, binary format. *note inet_pton(): 16c4. is useful when a
     library or network protocol calls for an object of type `struct
     in_addr' (similar to *note inet_aton(): 16c3.) or `struct
     in6_addr'.

     Supported values for _address_family_ are currently *note AF_INET:
     16a7. and *note AF_INET6: 16a8. If the IP address string
     _ip_string_ is invalid, *note socket.error: 370. will be raised.
     Note that exactly what is valid depends on both the value of
     _address_family_ and the underlying implementation of
     `inet_pton()'.

     Availability: Unix (maybe not all platforms).

     New in version 2.3.

 -- Function: socket.inet_ntop (address_family, packed_ip)
     Convert a packed IP address (a string of some number of
     characters) to its standard, family-specific string representation
     (for example, `'7.10.0.5'' or `'5aef:2b::8'') *note inet_ntop():
     16c6. is useful when a library or network protocol returns an
     object of type `struct in_addr' (similar to *note inet_ntoa():
     16c5.)  or `struct in6_addr'.

     Supported values for _address_family_ are currently *note AF_INET:
     16a7. and *note AF_INET6: 16a8. If the string _packed_ip_ is not
     the correct length for the specified address family, *note
     ValueError: 233. will be raised.  A *note socket.error: 370. is
     raised for errors from the call to *note inet_ntop(): 16c6.

     Availability: Unix (maybe not all platforms).

     New in version 2.3.

 -- Function: socket.getdefaulttimeout ()
     Return the default timeout in floating seconds for new socket
     objects. A value of `None' indicates that new socket objects have
     no timeout. When the socket module is first imported, the default
     is `None'.

     New in version 2.3.

 -- Function: socket.setdefaulttimeout (timeout)
     Set the default timeout in floating seconds for new socket
     objects. A value of `None' indicates that new socket objects have
     no timeout. When the socket module is first imported, the default
     is `None'.

     New in version 2.3.

 -- Data: socket.SocketType
     This is a Python type object that represents the socket object
     type. It is the same as `type(socket(...))'.

See also
........

Module *note SocketServer: 15e.
     Classes that simplify writing network servers.

Module *note ssl: 161.
     A TLS/SSL wrapper for socket objects.

* Menu:

* Socket Objects::
* Example: Example<7>.

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc3493.html


File: python.info,  Node: Socket Objects,  Next: Example<7>,  Up: socket --- Low-level networking interface

5.17.2.1 Socket Objects
.......................

Socket objects have the following methods.  Except for `makefile()'
these correspond to Unix system calls applicable to sockets.

 -- Method: socket.accept ()
     Accept a connection. The socket must be bound to an address and
     listening for connections. The return value is a pair `(conn,
     address)' where _conn_ is a _new_ socket object usable to send and
     receive data on the connection, and _address_ is the address bound
     to the socket on the other end of the connection.

 -- Method: socket.bind (address)
     Bind the socket to _address_.  The socket must not already be
     bound. (The format of _address_ depends on the address family --
     see above.)

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16a7.  addresses instead of
          only a tuple.  This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.close ()
     Close the socket.  All future operations on the socket object will
     fail. The remote end will receive no more data (after queued data
     is flushed). Sockets are automatically closed when they are
     garbage-collected.

 -- Method: socket.connect (address)
     Connect to a remote socket at _address_. (The format of _address_
     depends on the address family -- see above.)

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16a7.  addresses instead of
          only a tuple.  This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.connect_ex (address)
     Like `connect(address)', but return an error indicator instead of
     raising an exception for errors returned by the C-level
     `connect()' call (other problems, such as "host not found," can
     still raise exceptions).  The error indicator is `0' if the
     operation succeeded, otherwise the value of the `errno' variable.
     This is useful to support, for example, asynchronous connects.

          Note: This method has historically accepted a pair of
          parameters for *note AF_INET: 16a7.  addresses instead of
          only a tuple. This was never intentional and is no longer
          available in Python 2.0 and later.

 -- Method: socket.fileno ()
     Return the socket's file descriptor (a small integer).  This is
     useful with *note select.select(): 1537.

     Under Windows the small integer returned by this method cannot be
     used where a file descriptor can be used (such as *note
     os.fdopen(): 6d8.).  Unix does not have this limitation.

 -- Method: socket.getpeername ()
     Return the remote address to which the socket is connected.  This
     is useful to find out the port number of a remote IPv4/v6 socket,
     for instance. (The format of the address returned depends on the
     address family -- see above.)  On some systems this function is
     not supported.

 -- Method: socket.getsockname ()
     Return the socket's own address.  This is useful to find out the
     port number of an IPv4/v6 socket, for instance. (The format of the
     address returned depends on the address family -- see above.)

 -- Method: socket.getsockopt (level, optname[, buflen])
     Return the value of the given socket option (see the Unix man page
     `getsockopt(2)').  The needed symbolic constants (`SO_*' etc.)
     are defined in this module.  If _buflen_ is absent, an integer
     option is assumed and its integer value is returned by the
     function.  If _buflen_ is present, it specifies the maximum length
     of the buffer used to receive the option in, and this buffer is
     returned as a string.  It is up to the caller to decode the
     contents of the buffer (see the optional built-in module *note
     struct: 167. for a way to decode C structures encoded as strings).

 -- Method: socket.ioctl (control, option)
          Platform : Windows

     The *note ioctl(): 16d3. method is a limited interface to the
     WSAIoctl system interface.  Please refer to the Win32
     documentation(1) for more information.

     On other platforms, the generic *note fcntl.fcntl(): 16d4. and
     *note fcntl.ioctl(): 410.  functions may be used; they accept a
     socket object as their first argument.

     New in version 2.6.

 -- Method: socket.listen (backlog)
     Listen for connections made to the socket.  The _backlog_ argument
     specifies the maximum number of queued connections and should be
     at least 0; the maximum value is system-dependent (usually 5), the
     minimum value is forced to 0.

 -- Method: socket.makefile ([mode[, bufsize]])
     Return a _file object_ associated with the socket.  (File objects
     are described in *note File Objects: 61c.) The file object
     references a `dup()'ped version of the socket file descriptor, so
     the file object and socket object may be closed or
     garbage-collected independently.  The socket must be in blocking
     mode (it can not have a timeout). The optional _mode_ and
     _bufsize_ arguments are interpreted the same way as by the built-in
     *note file(): 1f6. function.

          Note: On Windows, the file-like object created by *note
          makefile(): 16d6. cannot be used where a file object with a
          file descriptor is expected, such as the stream arguments of
          *note subprocess.Popen(): 1673.

 -- Method: socket.recv (bufsize[, flags])
     Receive data from the socket.  The return value is a string
     representing the data received.  The maximum amount of data to be
     received at once is specified by _bufsize_.  See the Unix manual
     page `recv(2)' for the meaning of the optional argument _flags_;
     it defaults to zero.

          Note: For best match with hardware and network realities, the
          value of  _bufsize_ should be a relatively small power of 2,
          for example, 4096.

 -- Method: socket.recvfrom (bufsize[, flags])
     Receive data from the socket.  The return value is a pair
     `(string, address)' where _string_ is a string representing the
     data received and _address_ is the address of the socket sending
     the data.  See the Unix manual page `recv(2)' for the meaning of
     the optional argument _flags_; it defaults to zero. (The format of
     _address_ depends on the address family -- see above.)

 -- Method: socket.recvfrom_into (buffer[, nbytes[, flags]])
     Receive data from the socket, writing it into _buffer_ instead of
     creating a new string.  The return value is a pair `(nbytes,
     address)' where _nbytes_ is the number of bytes received and
     _address_ is the address of the socket sending the data.  See the
     Unix manual page `recv(2)' for the meaning of the optional
     argument _flags_; it defaults to zero.  (The format of _address_
     depends on the address family -- see above.)

     New in version 2.5.

 -- Method: socket.recv_into (buffer[, nbytes[, flags]])
     Receive up to _nbytes_ bytes from the socket, storing the data
     into a buffer rather than creating a new string.  If _nbytes_ is
     not specified (or 0), receive up to the size available in the
     given buffer.  Returns the number of bytes received.  See the Unix
     manual page `recv(2)' for the meaning of the optional argument
     _flags_; it defaults to zero.

     New in version 2.5.

 -- Method: socket.send (string[, flags])
     Send data to the socket.  The socket must be connected to a remote
     socket.  The optional _flags_ argument has the same meaning as for
     *note recv(): 16d7. above.  Returns the number of bytes sent.
     Applications are responsible for checking that all data has been
     sent; if only some of the data was transmitted, the application
     needs to attempt delivery of the remaining data.

 -- Method: socket.sendall (string[, flags])
     Send data to the socket.  The socket must be connected to a remote
     socket.  The optional _flags_ argument has the same meaning as for
     *note recv(): 16d7. above.  Unlike *note send(): 16d9, this method
     continues to send data from _string_ until either all data has
     been sent or an error occurs.  `None' is returned on success.  On
     error, an exception is raised, and there is no way to determine how
     much data, if any, was successfully sent.

 -- Method: socket.sendto (string[, flags], address)
     Send data to the socket.  The socket should not be connected to a
     remote socket, since the destination socket is specified by
     _address_.  The optional _flags_ argument has the same meaning as
     for *note recv(): 16d7. above.  Return the number of bytes sent.
     (The format of _address_ depends on the address family -- see
     above.)

 -- Method: socket.setblocking (flag)
     Set blocking or non-blocking mode of the socket: if _flag_ is 0,
     the socket is set to non-blocking, else to blocking mode.
     Initially all sockets are in blocking mode.  In non-blocking mode,
     if a *note recv(): 16d7. call doesn't find any data, or if a *note
     send(): 16d9. call can't immediately dispose of the data, a *note
     error: 370. exception is raised; in blocking mode, the calls block
     until they can proceed. `s.setblocking(0)' is equivalent to
     `s.settimeout(0.0)'; `s.setblocking(1)' is equivalent to
     `s.settimeout(None)'.

 -- Method: socket.settimeout (value)
     Set a timeout on blocking socket operations.  The _value_ argument
     can be a nonnegative float expressing seconds, or `None'. If a
     float is given, subsequent socket operations will raise a *note
     timeout: 452. exception if the timeout period _value_ has elapsed
     before the operation has completed.  Setting a timeout of `None'
     disables timeouts on socket operations.  `s.settimeout(0.0)' is
     equivalent to `s.setblocking(0)'; `s.settimeout(None)' is
     equivalent to `s.setblocking(1)'.

     New in version 2.3.

 -- Method: socket.gettimeout ()
     Return the timeout in floating seconds associated with socket
     operations, or `None' if no timeout is set.  This reflects the
     last call to *note setblocking(): 16a9. or *note settimeout():
     16aa.

     New in version 2.3.

  Some notes on socket blocking and timeouts: A socket object can be in
one of three modes: blocking, non-blocking, or timeout.  Sockets are
always created in blocking mode.  In blocking mode, operations block
until complete or the system returns an error (such as connection timed
out).  In non-blocking mode, operations fail (with an error that is
unfortunately system-dependent) if they cannot be completed
immediately.  In timeout mode, operations fail if they cannot be
completed within the timeout specified for the socket or if the system
returns an error.  The *note setblocking(): 16a9.  method is simply a
shorthand for certain *note settimeout(): 16aa. calls.

  Timeout mode internally sets the socket in non-blocking mode.  The
blocking and timeout modes are shared between file descriptors and
socket objects that refer to the same network endpoint.  A consequence
of this is that file objects returned by the *note makefile(): 16d6.
method must only be used when the socket is in blocking mode; in
timeout or non-blocking mode file operations that cannot be completed
immediately will fail.

  Note that the *note connect(): 16b7. operation is subject to the
timeout setting, and in general it is recommended to call *note
settimeout(): 16aa.  before calling *note connect(): 16b7. or pass a
timeout parameter to *note create_connection(): 24f.  The system
network stack may return a connection timeout error of its own
regardless of any Python socket timeout setting.

 -- Method: socket.setsockopt (level, optname, value)
     Set the value of the given socket option (see the Unix manual page
     `setsockopt(2)').  The needed symbolic constants are defined in the
     *note socket: 15d. module (`SO_*' etc.).  The value can be an
     integer or a string representing a buffer.  In the latter case it
     is up to the caller to ensure that the string contains the proper
     bits (see the optional built-in module *note struct: 167. for a
     way to encode C structures as strings).

 -- Method: socket.shutdown (how)
     Shut down one or both halves of the connection.  If _how_ is
     `SHUT_RD', further receives are disallowed.  If _how_ is
     `SHUT_WR', further sends are disallowed.  If _how_ is `SHUT_RDWR',
     further sends and receives are disallowed.  Depending on the
     platform, shutting down one half of the connection can also close
     the opposite half (e.g. on Mac OS X, `shutdown(SHUT_WR)' does not
     allow further reads on the other end of the connection).

  Note that there are no methods `read()' or `write()'; use *note
recv(): 16d7. and *note send(): 16d9. without _flags_ argument instead.

  Socket objects also have these (read-only) attributes that correspond
to the values given to the *note socket: 15d. constructor.

 -- Attribute: socket.family
     The socket family.

     New in version 2.5.

 -- Attribute: socket.type
     The socket type.

     New in version 2.5.

 -- Attribute: socket.proto
     The socket protocol.

     New in version 2.5.

  ---------- Footnotes ----------

  (1) http://msdn.microsoft.com/en-us/library/ms741621%28VS.85%29.aspx


File: python.info,  Node: Example<7>,  Prev: Socket Objects,  Up: socket --- Low-level networking interface

5.17.2.2 Example
................

Here are four minimal example programs using the TCP/IP protocol: a
server that echoes all data that it receives back (servicing only one
client), and a client using it.  Note that a server must perform the
sequence *note socket(): 15d, *note bind(): 16cc, *note listen(): 16d5,
*note accept(): 16cb. (possibly repeating the *note accept(): 16cb. to
service more than one client), while a client only needs the sequence
*note socket(): 15d, *note connect(): 16b7.  Also note that the server
does not *note send(): 16d9./*note recv(): 16d7. on the socket it is
listening on but on the new socket returned by *note accept(): 16cb.

  The first two examples support IPv4 only.

    # Echo server program
    import socket

    HOST = ''                 # Symbolic name meaning all available interfaces
    PORT = 50007              # Arbitrary non-privileged port
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind((HOST, PORT))
    s.listen(1)
    conn, addr = s.accept()
    print 'Connected by', addr
    while 1:
        data = conn.recv(1024)
        if not data: break
        conn.send(data)
    conn.close()


    # Echo client program
    import socket

    HOST = 'daring.cwi.nl'    # The remote host
    PORT = 50007              # The same port as used by the server
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.connect((HOST, PORT))
    s.send('Hello, world')
    data = s.recv(1024)
    s.close()
    print 'Received', repr(data)

The next two examples are identical to the above two, but support both
IPv4 and IPv6. The server side will listen to the first address family
available (it should listen to both instead). On most of IPv6-ready
systems, IPv6 will take precedence and the server may not accept IPv4
traffic. The client side will try to connect to the all addresses
returned as a result of the name resolution, and sends traffic to the
first one connected successfully.

    # Echo server program
    import socket
    import sys

    HOST = None               # Symbolic name meaning all available interfaces
    PORT = 50007              # Arbitrary non-privileged port
    s = None
    for res in socket.getaddrinfo(HOST, PORT, socket.AF_UNSPEC,
                                  socket.SOCK_STREAM, 0, socket.AI_PASSIVE):
        af, socktype, proto, canonname, sa = res
        try:
            s = socket.socket(af, socktype, proto)
        except socket.error, msg:
            s = None
            continue
        try:
            s.bind(sa)
            s.listen(1)
        except socket.error, msg:
            s.close()
            s = None
            continue
        break
    if s is None:
        print 'could not open socket'
        sys.exit(1)
    conn, addr = s.accept()
    print 'Connected by', addr
    while 1:
        data = conn.recv(1024)
        if not data: break
        conn.send(data)
    conn.close()


    # Echo client program
    import socket
    import sys

    HOST = 'daring.cwi.nl'    # The remote host
    PORT = 50007              # The same port as used by the server
    s = None
    for res in socket.getaddrinfo(HOST, PORT, socket.AF_UNSPEC, socket.SOCK_STREAM):
        af, socktype, proto, canonname, sa = res
        try:
            s = socket.socket(af, socktype, proto)
        except socket.error, msg:
            s = None
            continue
        try:
            s.connect(sa)
        except socket.error, msg:
            s.close()
            s = None
            continue
        break
    if s is None:
        print 'could not open socket'
        sys.exit(1)
    s.send('Hello, world')
    data = s.recv(1024)
    s.close()
    print 'Received', repr(data)

The last example shows how to write a very simple network sniffer with
raw sockets on Windows. The example requires administrator privileges
to modify the interface:

    import socket

    # the public network interface
    HOST = socket.gethostbyname(socket.gethostname())

    # create a raw socket and bind it to the public interface
    s = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_IP)
    s.bind((HOST, 0))

    # Include IP headers
    s.setsockopt(socket.IPPROTO_IP, socket.IP_HDRINCL, 1)

    # receive all packages
    s.ioctl(socket.SIO_RCVALL, socket.RCVALL_ON)

    # receive a package
    print s.recvfrom(65565)

    # disabled promiscuous mode
    s.ioctl(socket.SIO_RCVALL, socket.RCVALL_OFF)



File: python.info,  Node: ssl --- TLS/SSL wrapper for socket objects,  Next: signal --- Set handlers for asynchronous events,  Prev: socket --- Low-level networking interface,  Up: Interprocess Communication and Networking

5.17.3 `ssl' -- TLS/SSL wrapper for socket objects
--------------------------------------------------

New in version 2.6.

  This module provides access to Transport Layer Security (often known
as "Secure Sockets Layer") encryption and peer authentication
facilities for network sockets, both client-side and server-side.  This
module uses the OpenSSL library. It is available on all modern Unix
systems, Windows, Mac OS X, and probably additional platforms, as long
as OpenSSL is installed on that platform.

     Note: Some behavior may be platform dependent, since calls are
     made to the operating system socket APIs.  The installed version
     of OpenSSL may also cause variations in behavior.

  This section documents the objects and functions in the `ssl' module;
for more general information about TLS, SSL, and certificates, the
reader is referred to the documents in the "See Also" section at the
bottom.

  This module provides a class, `ssl.SSLSocket', which is derived from
the *note socket.socket: 1538. type, and provides a socket-like wrapper
that also encrypts and decrypts the data going over the socket with
SSL.  It supports additional `read()' and `write()' methods, along with
a method, `getpeercert()', to retrieve the certificate of the other
side of the connection, and a method, `cipher()', to retrieve the
cipher being used for the secure connection.

* Menu:

* Functions, Constants, and Exceptions: Functions Constants and Exceptions.
* SSLSocket Objects::
* Certificates::
* Examples: Examples<8>.


File: python.info,  Node: Functions Constants and Exceptions,  Next: SSLSocket Objects,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.1 Functions, Constants, and Exceptions
.............................................

 -- Exception: ssl.SSLError
     Raised to signal an error from the underlying SSL implementation.
     This signifies some problem in the higher-level encryption and
     authentication layer that's superimposed on the underlying network
     connection.  This error is a subtype of *note socket.error: 370,
     which in turn is a subtype of *note IOError: 1f7.

 -- Function: ssl.wrap_socket (sock, keyfile=None, certfile=None,
          server_side=False, cert_reqs=CERT_NONE, ssl_version={see
          docs}, ca_certs=None, do_handshake_on_connect=True,
          suppress_ragged_eofs=True, ciphers=None)
     Takes an instance `sock' of *note socket.socket: 1538, and returns
     an instance of `ssl.SSLSocket', a subtype of *note socket.socket:
     1538, which wraps the underlying socket in an SSL context.  For
     client-side sockets, the context construction is lazy; if the
     underlying socket isn't connected yet, the context construction
     will be performed after `connect()' is called on the socket.  For
     server-side sockets, if the socket has no remote peer, it is
     assumed to be a listening socket, and the server-side SSL wrapping
     is automatically performed on client connections accepted via the
     `accept()' method.  *note wrap_socket(): 256. may raise *note
     SSLError: 16e7.

     The `keyfile' and `certfile' parameters specify optional files
     which contain a certificate to be used to identify the local side
     of the connection.  See the discussion of *note Certificates:
     16e8. for more information on how the certificate is stored in the
     `certfile'.

     Often the private key is stored in the same file as the
     certificate; in this case, only the `certfile' parameter need be
     passed.  If the private key is stored in a separate file, both
     parameters must be used.  If the private key is stored in the
     `certfile', it should come before the first certificate in the
     certificate chain:

         -----BEGIN RSA PRIVATE KEY-----
         ... (private key in base64 encoding) ...
         -----END RSA PRIVATE KEY-----
         -----BEGIN CERTIFICATE-----
         ... (certificate in base64 PEM encoding) ...
         -----END CERTIFICATE-----

     The parameter `server_side' is a boolean which identifies whether
     server-side or client-side behavior is desired from this socket.

     The parameter `cert_reqs' specifies whether a certificate is
     required from the other side of the connection, and whether it
     will be validated if provided.  It must be one of the three values
     *note CERT_NONE: 16e9.  (certificates ignored), *note
     CERT_OPTIONAL: 16ea. (not required, but validated if provided), or
     *note CERT_REQUIRED: 16eb. (required and validated).  If the value
     of this parameter is not *note CERT_NONE: 16e9, then the `ca_certs'
     parameter must point to a file of CA certificates.

     The `ca_certs' file contains a set of concatenated "certification
     authority" certificates, which are used to validate certificates
     passed from the other end of the connection.  See the discussion of
     *note Certificates: 16e8. for more information about how to
     arrange the certificates in this file.

     The parameter `ssl_version' specifies which version of the SSL
     protocol to use.  Typically, the server chooses a particular
     protocol version, and the client must adapt to the server's
     choice.  Most of the versions are not interoperable with the other
     versions.  If not specified, for client-side operation, the
     default SSL version is SSLv3; for server-side operation, SSLv23.
     These version selections provide the most compatibility with other
     versions.

     Here's a table showing which versions in a client (down the side)
     can connect to which versions in a server (along the top):

           _client_ / *server*          *SSLv2*       *SSLv3*       *SSLv23*       *TLSv1*
          _SSLv2_                      yes           no            yes            no
          _SSLv3_                      yes           yes           yes            no
          _SSLv23_                     yes           no            yes            no
          _TLSv1_                      no            no            yes            yes


          Note: Which connections succeed will vary depending on the
          version of OpenSSL.  For instance, in some older versions of
          OpenSSL (such as 0.9.7l on OS X 10.4), an SSLv2 client could
          not connect to an SSLv23 server.  Another example: beginning
          with OpenSSL 1.0.0, an SSLv23 client will not actually
          attempt SSLv2 connections unless you explicitly enable SSLv2
          ciphers; for example, you might specify `"ALL"' or `"SSLv2"'
          as the _ciphers_ parameter to enable them.

     The _ciphers_ parameter sets the available ciphers for this SSL
     object.  It should be a string in the OpenSSL cipher list
     format(1).

     The parameter `do_handshake_on_connect' specifies whether to do
     the SSL handshake automatically after doing a `socket.connect()',
     or whether the application program will call it explicitly, by
     invoking the *note SSLSocket.do_handshake(): 16ec. method.  Calling
     *note SSLSocket.do_handshake(): 16ec. explicitly gives the program
     control over the blocking behavior of the socket I/O involved in
     the handshake.

     The parameter `suppress_ragged_eofs' specifies how the *note
     SSLSocket.read(): 16ed. method should signal unexpected EOF from
     the other end of the connection.  If specified as *note True: 39f.
     (the default), it returns a normal EOF in response to unexpected
     EOF errors raised from the underlying socket; if *note False: 3a0,
     it will raise the exceptions back to the caller.

     Changed in version 2.7: New optional argument _ciphers_.

 -- Function: ssl.RAND_status ()
     Returns True if the SSL pseudo-random number generator has been
     seeded with 'enough' randomness, and False otherwise.  You can use
     *note ssl.RAND_egd(): 16ef.  and *note ssl.RAND_add(): 16f0. to
     increase the randomness of the pseudo-random number generator.

 -- Function: ssl.RAND_egd (path)
     If you are running an entropy-gathering daemon (EGD) somewhere,
     and `path' is the pathname of a socket connection open to it, this
     will read 256 bytes of randomness from the socket, and add it to
     the SSL pseudo-random number generator to increase the security of
     generated secret keys.  This is typically only necessary on
     systems without better sources of randomness.

     See <http://egd.sourceforge.net/> or
     <http://prngd.sourceforge.net/> for sources of entropy-gathering
     daemons.

 -- Function: ssl.RAND_add (bytes, entropy)
     Mixes the given `bytes' into the SSL pseudo-random number
     generator.  The parameter `entropy' (a float) is a lower bound on
     the entropy contained in string (so you can always use `0.0').  See RFC
     1750(2) for more information on sources of entropy.

 -- Function: ssl.cert_time_to_seconds (timestring)
     Returns a floating-point value containing a normal
     seconds-after-the-epoch time value, given the time-string
     representing the "notBefore" or "notAfter" date from a certificate.

     Here's an example:

         >>> import ssl
         >>> ssl.cert_time_to_seconds("May  9 00:00:00 2007 GMT")
         1178694000.0
         >>> import time
         >>> time.ctime(ssl.cert_time_to_seconds("May  9 00:00:00 2007 GMT"))
         'Wed May  9 00:00:00 2007'
         >>>



 -- Function: ssl.get_server_certificate (addr,
          ssl_version=PROTOCOL_SSLv3, ca_certs=None)
     Given the address `addr' of an SSL-protected server, as a
     (_hostname_, _port-number_) pair, fetches the server's
     certificate, and returns it as a PEM-encoded string.  If
     `ssl_version' is specified, uses that version of the SSL protocol
     to attempt to connect to the server.  If `ca_certs' is specified,
     it should be a file containing a list of root certificates, the
     same format as used for the same parameter in *note wrap_socket():
     256.  The call will attempt to validate the server certificate
     against that set of root certificates, and will fail if the
     validation attempt fails.

 -- Function: ssl.DER_cert_to_PEM_cert (DER_cert_bytes)
     Given a certificate as a DER-encoded blob of bytes, returns a
     PEM-encoded string version of the same certificate.

 -- Function: ssl.PEM_cert_to_DER_cert (PEM_cert_string)
     Given a certificate as an ASCII PEM string, returns a DER-encoded
     sequence of bytes for that same certificate.

 -- Data: ssl.CERT_NONE
     Value to pass to the `cert_reqs' parameter to `sslobject()' when no
     certificates will be required or validated from the other side of
     the socket connection.

 -- Data: ssl.CERT_OPTIONAL
     Value to pass to the `cert_reqs' parameter to `sslobject()' when no
     certificates will be required from the other side of the socket
     connection, but if they are provided, will be validated.  Note
     that use of this setting requires a valid certificate validation
     file also be passed as a value of the `ca_certs' parameter.

 -- Data: ssl.CERT_REQUIRED
     Value to pass to the `cert_reqs' parameter to `sslobject()' when
     certificates will be required from the other side of the socket
     connection.  Note that use of this setting requires a valid
     certificate validation file also be passed as a value of the
     `ca_certs' parameter.

 -- Data: ssl.PROTOCOL_SSLv2
     Selects SSL version 2 as the channel encryption protocol.

     This protocol is not available if OpenSSL is compiled with
     OPENSSL_NO_SSL2 flag.

          Warning: SSL version 2 is insecure.  Its use is highly
          discouraged.

 -- Data: ssl.PROTOCOL_SSLv23
     Selects SSL version 2 or 3 as the channel encryption protocol.
     This is a setting to use with servers for maximum compatibility
     with the other end of an SSL connection, but it may cause the
     specific ciphers chosen for the encryption to be of fairly low
     quality.

 -- Data: ssl.PROTOCOL_SSLv3
     Selects SSL version 3 as the channel encryption protocol.  For
     clients, this is the maximally compatible SSL variant.

 -- Data: ssl.PROTOCOL_TLSv1
     Selects TLS version 1 as the channel encryption protocol.  This is
     the most modern version, and probably the best choice for maximum
     protection, if both sides can speak it.

 -- Data: ssl.OPENSSL_VERSION
     The version string of the OpenSSL library loaded by the
     interpreter:

         >>> ssl.OPENSSL_VERSION
         'OpenSSL 0.9.8k 25 Mar 2009'

     New in version 2.7.

 -- Data: ssl.OPENSSL_VERSION_INFO
     A tuple of five integers representing version information about the
     OpenSSL library:

         >>> ssl.OPENSSL_VERSION_INFO
         (0, 9, 8, 11, 15)

     New in version 2.7.

 -- Data: ssl.OPENSSL_VERSION_NUMBER
     The raw version number of the OpenSSL library, as a single integer:

         >>> ssl.OPENSSL_VERSION_NUMBER
         9470143L
         >>> hex(ssl.OPENSSL_VERSION_NUMBER)
         '0x9080bfL'

     New in version 2.7.

  ---------- Footnotes ----------

  (1) http://www.openssl.org/docs/apps/ciphers.html#CIPHER_LIST_FORMAT

  (2) http://tools.ietf.org/html/rfc1750.html


File: python.info,  Node: SSLSocket Objects,  Next: Certificates,  Prev: Functions Constants and Exceptions,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.2 SSLSocket Objects
..........................

 -- Method: SSLSocket.read ([nbytes=1024])
     Reads up to `nbytes' bytes from the SSL-encrypted channel and
     returns them.

 -- Method: SSLSocket.write (data)
     Writes the `data' to the other side of the connection, using the
     SSL channel to encrypt.  Returns the number of bytes written.

 -- Method: SSLSocket.getpeercert (binary_form=False)
     If there is no certificate for the peer on the other end of the
     connection, returns `None'.

     If the parameter `binary_form' is *note False: 3a0, and a
     certificate was received from the peer, this method returns a
     *note dict: 2f6. instance.  If the certificate was not validated,
     the dict is empty.  If the certificate was validated, it returns a
     dict with the keys `subject' (the principal for which the
     certificate was issued), and `notAfter' (the time after which the
     certificate should not be trusted).  The certificate was already
     validated, so the `notBefore' and `issuer' fields are not
     returned.  If a certificate contains an instance of the _Subject
     Alternative Name_ extension (see RFC 3280(1)), there will also be
     a `subjectAltName' key in the dictionary.

     The "subject" field is a tuple containing the sequence of relative
     distinguished names (RDNs) given in the certificate's data
     structure for the principal, and each RDN is a sequence of
     name-value pairs:

         {'notAfter': 'Feb 16 16:54:50 2013 GMT',
          'subject': ((('countryName', u'US'),),
                      (('stateOrProvinceName', u'Delaware'),),
                      (('localityName', u'Wilmington'),),
                      (('organizationName', u'Python Software Foundation'),),
                      (('organizationalUnitName', u'SSL'),),
                      (('commonName', u'somemachine.python.org'),))}

     If the `binary_form' parameter is *note True: 39f, and a
     certificate was provided, this method returns the DER-encoded form
     of the entire certificate as a sequence of bytes, or *note None:
     389. if the peer did not provide a certificate.  This return value
     is independent of validation; if validation was required (*note
     CERT_OPTIONAL: 16ea. or *note CERT_REQUIRED: 16eb.), it will have
     been validated, but if *note CERT_NONE: 16e9. was used to
     establish the connection, the certificate, if present, will not
     have been validated.

 -- Method: SSLSocket.cipher ()
     Returns a three-value tuple containing the name of the cipher
     being used, the version of the SSL protocol that defines its use,
     and the number of secret bits being used.  If no connection has
     been established, returns `None'.

 -- Method: SSLSocket.do_handshake ()
     Perform a TLS/SSL handshake.  If this is used with a non-blocking
     socket, it may raise *note SSLError: 16e7. with an `arg[0]' of
     `SSL_ERROR_WANT_READ' or `SSL_ERROR_WANT_WRITE', in which case it
     must be called again until it completes successfully.  For
     example, to simulate the behavior of a blocking socket, one might
     write:

         while True:
             try:
                 s.do_handshake()
                 break
             except ssl.SSLError, err:
                 if err.args[0] == ssl.SSL_ERROR_WANT_READ:
                     select.select([s], [], [])
                 elif err.args[0] == ssl.SSL_ERROR_WANT_WRITE:
                     select.select([], [s], [])
                 else:
                     raise



 -- Method: SSLSocket.unwrap ()
     Performs the SSL shutdown handshake, which removes the TLS layer
     from the underlying socket, and returns the underlying socket
     object.  This can be used to go from encrypted operation over a
     connection to unencrypted.  The socket instance returned should
     always be used for further communication with the other side of
     the connection, rather than the original socket instance (which
     may not function properly after the unwrap).

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc3280.html


File: python.info,  Node: Certificates,  Next: Examples<8>,  Prev: SSLSocket Objects,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.3 Certificates
.....................

Certificates in general are part of a public-key / private-key system.
In this system, each _principal_, (which may be a machine, or a person,
or an organization) is assigned a unique two-part encryption key.  One
part of the key is public, and is called the _public key_; the other
part is kept secret, and is called the _private key_.  The two parts
are related, in that if you encrypt a message with one of the parts,
you can decrypt it with the other part, and *only* with the other part.

  A certificate contains information about two principals.  It contains
the name of a _subject_, and the subject's public key.  It also
contains a statement by a second principal, the _issuer_, that the
subject is who he claims to be, and that this is indeed the subject's
public key.  The issuer's statement is signed with the issuer's private
key, which only the issuer knows.  However, anyone can verify the
issuer's statement by finding the issuer's public key, decrypting the
statement with it, and comparing it to the other information in the
certificate.  The certificate also contains information about the time
period over which it is valid.  This is expressed as two fields, called
"notBefore" and "notAfter".

  In the Python use of certificates, a client or server can use a
certificate to prove who they are.  The other side of a network
connection can also be required to produce a certificate, and that
certificate can be validated to the satisfaction of the client or
server that requires such validation.  The connection attempt can be
set to raise an exception if the validation fails.  Validation is done
automatically, by the underlying OpenSSL framework; the application
need not concern itself with its mechanics.  But the application does
usually need to provide sets of certificates to allow this process to
take place.

  Python uses files to contain certificates.  They should be formatted
as "PEM" (see RFC 1422(1)), which is a base-64 encoded form wrapped
with a header line and a footer line:

    -----BEGIN CERTIFICATE-----
    ... (certificate in base64 PEM encoding) ...
    -----END CERTIFICATE-----

The Python files which contain certificates can contain a sequence of
certificates, sometimes called a _certificate chain_.  This chain
should start with the specific certificate for the principal who "is"
the client or server, and then the certificate for the issuer of that
certificate, and then the certificate for the issuer of _that_
certificate, and so on up the chain till you get to a certificate which
is _self-signed_, that is, a certificate which has the same subject and
issuer, sometimes called a _root certificate_.  The certificates should
just be concatenated together in the certificate file.  For example,
suppose we had a three certificate chain, from our server certificate
to the certificate of the certification authority that signed our server
certificate, to the root certificate of the agency which issued the
certification authority's certificate:

    -----BEGIN CERTIFICATE-----
    ... (certificate for your server)...
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    ... (the certificate for the CA)...
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    ... (the root certificate for the CA's issuer)...
    -----END CERTIFICATE-----

If you are going to require validation of the other side of the
connection's certificate, you need to provide a "CA certs" file, filled
with the certificate chains for each issuer you are willing to trust.
Again, this file just contains these chains concatenated together.  For
validation, Python will use the first chain it finds in the file which
matches.

  Some "standard" root certificates are available from various
certification authorities: CACert.org(2), Thawte(3), Verisign(4),
Positive SSL(5) (used by python.org), Equifax and GeoTrust(6).

  In general, if you are using SSL3 or TLS1, you don't need to put the
full chain in your "CA certs" file; you only need the root
certificates, and the remote peer is supposed to furnish the other
certificates necessary to chain from its certificate to a root
certificate.  See RFC 4158(7) for more discussion of the way in which
certification chains can be built.

  If you are going to create a server that provides SSL-encrypted
connection services, you will need to acquire a certificate for that
service.  There are many ways of acquiring appropriate certificates,
such as buying one from a certification authority.  Another common
practice is to generate a self-signed certificate.  The simplest way to
do this is with the OpenSSL package, using something like the following:

    % openssl req -new -x509 -days 365 -nodes -out cert.pem -keyout cert.pem
    Generating a 1024 bit RSA private key
    .......++++++
    .............................++++++
    writing new private key to 'cert.pem'
    -----
    You are about to be asked to enter information that will be incorporated
    into your certificate request.
    What you are about to enter is what is called a Distinguished Name or a DN.
    There are quite a few fields but you can leave some blank
    For some fields there will be a default value,
    If you enter '.', the field will be left blank.
    -----
    Country Name (2 letter code) [AU]:US
    State or Province Name (full name) [Some-State]:MyState
    Locality Name (eg, city) []:Some City
    Organization Name (eg, company) [Internet Widgits Pty Ltd]:My Organization, Inc.
    Organizational Unit Name (eg, section) []:My Group
    Common Name (eg, YOUR name) []:myserver.mygroup.myorganization.com
    Email Address []:ops@myserver.mygroup.myorganization.com
    %

The disadvantage of a self-signed certificate is that it is its own root
certificate, and no one else will have it in their cache of known (and
trusted) root certificates.

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc1422.html

  (2) http://www.cacert.org/index.php?id=3

  (3) http://www.thawte.com/roots/

  (4) http://www.verisign.com/support/roots.html

  (5)
http://www.PositiveSSL.com/ssl-certificate-support/cert_installation/UTN-USERFirst-Hardware.crt

  (6) http://www.geotrust.com/resources/root_certificates/index.asp

  (7) http://tools.ietf.org/html/rfc4158.html


File: python.info,  Node: Examples<8>,  Prev: Certificates,  Up: ssl --- TLS/SSL wrapper for socket objects

5.17.3.4 Examples
.................

* Menu:

* Testing for SSL support::
* Client-side operation::
* Server-side operation::


File: python.info,  Node: Testing for SSL support,  Next: Client-side operation,  Up: Examples<8>

5.17.3.5 Testing for SSL support
................................

To test for the presence of SSL support in a Python installation, user
code should use the following idiom:

    try:
        import ssl
    except ImportError:
        pass
    else:
        ... # do something that requires SSL support



File: python.info,  Node: Client-side operation,  Next: Server-side operation,  Prev: Testing for SSL support,  Up: Examples<8>

5.17.3.6 Client-side operation
..............................

This example connects to an SSL server, prints the server's address and
certificate, sends some bytes, and reads part of the response:

    import socket, ssl, pprint

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

    # require a certificate from the server
    ssl_sock = ssl.wrap_socket(s,
                               ca_certs="/etc/ca_certs_file",
                               cert_reqs=ssl.CERT_REQUIRED)

    ssl_sock.connect(('www.verisign.com', 443))

    print repr(ssl_sock.getpeername())
    print ssl_sock.cipher()
    print pprint.pformat(ssl_sock.getpeercert())

    # Set a simple HTTP request -- use httplib in actual code.
    ssl_sock.write("""GET / HTTP/1.0\r
    Host: www.verisign.com\r\n\r\n""")

    # Read a chunk of data.  Will not necessarily
    # read all the data returned by the server.
    data = ssl_sock.read()

    # note that closing the SSLSocket will also close the underlying socket
    ssl_sock.close()

As of September 6, 2007, the certificate printed by this program looked
like this:

    {'notAfter': 'May  8 23:59:59 2009 GMT',
     'subject': ((('serialNumber', u'2497886'),),
                 (('1.3.6.1.4.1.311.60.2.1.3', u'US'),),
                 (('1.3.6.1.4.1.311.60.2.1.2', u'Delaware'),),
                 (('countryName', u'US'),),
                 (('postalCode', u'94043'),),
                 (('stateOrProvinceName', u'California'),),
                 (('localityName', u'Mountain View'),),
                 (('streetAddress', u'487 East Middlefield Road'),),
                 (('organizationName', u'VeriSign, Inc.'),),
                 (('organizationalUnitName',
                   u'Production Security Services'),),
                 (('organizationalUnitName',
                   u'Terms of use at www.verisign.com/rpa (c)06'),),
                 (('commonName', u'www.verisign.com'),))}

which is a fairly poorly-formed `subject' field.


File: python.info,  Node: Server-side operation,  Prev: Client-side operation,  Up: Examples<8>

5.17.3.7 Server-side operation
..............................

For server operation, typically you'd need to have a server
certificate, and private key, each in a file.  You'd open a socket,
bind it to a port, call `listen()' on it, then start waiting for
clients to connect:

    import socket, ssl

    bindsocket = socket.socket()
    bindsocket.bind(('myaddr.mydomain.com', 10023))
    bindsocket.listen(5)

When one did, you'd call `accept()' on the socket to get the new socket
from the other end, and use *note wrap_socket(): 256. to create a
server-side SSL context for it:

    while True:
        newsocket, fromaddr = bindsocket.accept()
        connstream = ssl.wrap_socket(newsocket,
                                     server_side=True,
                                     certfile="mycertfile",
                                     keyfile="mykeyfile",
                                     ssl_version=ssl.PROTOCOL_TLSv1)
        try:
            deal_with_client(connstream)
        finally:
            connstream.shutdown(socket.SHUT_RDWR)
            connstream.close()

Then you'd read data from the `connstream' and do something with it
till you are finished with the client (or the client is finished with
you):

    def deal_with_client(connstream):
        data = connstream.read()
        # null data means the client is finished with us
        while data:
            if not do_something(connstream, data):
                # we'll assume do_something returns False
                # when we're finished with client
                break
            data = connstream.read()
        # finished with client

And go back to listening for new client connections.

See also
........

Class *note socket.socket: 1538.
     Documentation of underlying *note socket: 15d. class

Introducing SSL and Certificates using OpenSSL(1)
     Frederick J. Hirsch

RFC 1422: Privacy Enhancement for Internet Electronic Mail: Part II: Certificate-Based Key Management(2)
     Steve Kent

RFC 1750: Randomness Recommendations for Security(3)
     D. Eastlake et. al.

RFC 3280: Internet X.509 Public Key Infrastructure Certificate and CRL Profile(4)
     Housley et. al.

  ---------- Footnotes ----------

  (1) http://old.pseudonym.org/ssl/wwwj-index.html

  (2) http://www.ietf.org/rfc/rfc1422

  (3) http://www.ietf.org/rfc/rfc1750

  (4) http://www.ietf.org/rfc/rfc3280


File: python.info,  Node: signal --- Set handlers for asynchronous events,  Next: popen2 --- Subprocesses with accessible I/O streams,  Prev: ssl --- TLS/SSL wrapper for socket objects,  Up: Interprocess Communication and Networking

5.17.4 `signal' -- Set handlers for asynchronous events
-------------------------------------------------------

This module provides mechanisms to use signal handlers in Python. Some
general rules for working with signals and their handlers:

   * A handler for a particular signal, once set, remains installed
     until it is explicitly reset (Python emulates the BSD style
     interface regardless of the underlying implementation), with the
     exception of the handler for `SIGCHLD', which follows the
     underlying implementation.

   * There is no way to "block" signals temporarily from critical
     sections (since this is not supported by all Unix flavors).

   * Although Python signal handlers are called asynchronously as far
     as the Python user is concerned, they can only occur between the
     "atomic" instructions of the Python interpreter.  This means that
     signals arriving during long calculations implemented purely in C
     (such as regular expression matches on large bodies of text) may
     be delayed for an arbitrary amount of time.

   * When a signal arrives during an I/O operation, it is possible that
     the I/O operation raises an exception after the signal handler
     returns. This is dependent on the underlying Unix system's
     semantics regarding interrupted system calls.

   * Because the C signal handler always returns, it makes little sense
     to catch synchronous errors like `SIGFPE' or `SIGSEGV'.

   * Python installs a small number of signal handlers by default:
     `SIGPIPE' is ignored (so write errors on pipes and sockets can be
     reported as ordinary Python exceptions) and `SIGINT' is translated
     into a *note KeyboardInterrupt: 24e. exception.  All of these can
     be overridden.

   * Some care must be taken if both signals and threads are used in
     the same program.  The fundamental thing to remember in using
     signals and threads simultaneously is: always perform *note
     signal(): 156. operations in the main thread of execution.  Any
     thread can perform an *note alarm(): 1705, *note getsignal(): 1706,
     *note pause(): 1707, *note setitimer(): 1708. or *note
     getitimer(): 1709.; only the main thread can set a new signal
     handler, and the main thread will be the only one to receive
     signals (this is enforced by the Python *note signal: 156. module,
     even if the underlying thread implementation supports sending
     signals to individual threads).  This means that signals can't be
     used as a means of inter-thread communication.  Use locks instead.

  The variables defined in the *note signal: 156. module are:

 -- Data: signal.SIG_DFL
     This is one of two standard signal handling options; it will
     simply perform the default function for the signal.  For example,
     on most systems the default action for `SIGQUIT' is to dump core
     and exit, while the default action for `SIGCHLD' is to simply
     ignore it.

 -- Data: signal.SIG_IGN
     This is another standard signal handler, which will simply ignore
     the given signal.

 -- Data: SIG*
     All the signal numbers are defined symbolically.  For example, the
     hangup signal is defined as `signal.SIGHUP'; the variable names
     are identical to the names used in C programs, as found in
     `<signal.h>'. The Unix man page for '`signal()'' lists the
     existing signals (on some systems this is `signal(2)', on others
     the list is in `signal(7)'). Note that not all systems define the
     same set of signal names; only those names defined by the system
     are defined by this module.

 -- Data: signal.CTRL_C_EVENT
     The signal corresponding to the CTRL+C keystroke event. This
     signal can only be used with *note os.kill(): 2c6.

     Availability: Windows.

     New in version 2.7.

 -- Data: signal.CTRL_BREAK_EVENT
     The signal corresponding to the CTRL+BREAK keystroke event. This
     signal can only be used with *note os.kill(): 2c6.

     Availability: Windows.

     New in version 2.7.

 -- Data: signal.NSIG
     One more than the number of the highest signal number.

 -- Data: signal.ITIMER_REAL
     Decrements interval timer in real time, and delivers `SIGALRM'
     upon expiration.

 -- Data: signal.ITIMER_VIRTUAL
     Decrements interval timer only when the process is executing, and
     delivers SIGVTALRM upon expiration.

 -- Data: signal.ITIMER_PROF
     Decrements interval timer both when the process executes and when
     the system is executing on behalf of the process. Coupled with
     ITIMER_VIRTUAL, this timer is usually used to profile the time
     spent by the application in user and kernel space. SIGPROF is
     delivered upon expiration.

  The *note signal: 156. module defines one exception:

 -- Exception: signal.ItimerError
     Raised to signal an error from the underlying *note setitimer():
     1708. or *note getitimer(): 1709. implementation. Expect this
     error if an invalid interval timer or a negative time is passed to
     *note setitimer(): 1708.  This error is a subtype of *note
     IOError: 1f7.

  The *note signal: 156. module defines the following functions:

 -- Function: signal.alarm (time)
     If _time_ is non-zero, this function requests that a `SIGALRM'
     signal be sent to the process in _time_ seconds. Any previously
     scheduled alarm is canceled (only one alarm can be scheduled at
     any time).  The returned value is then the number of seconds
     before any previously set alarm was to have been delivered. If
     _time_ is zero, no alarm is scheduled, and any scheduled alarm is
     canceled.  If the return value is zero, no alarm is currently
     scheduled.  (See the Unix man page `alarm(2)'.) Availability: Unix.

 -- Function: signal.getsignal (signalnum)
     Return the current signal handler for the signal _signalnum_. The
     returned value may be a callable Python object, or one of the
     special values *note signal.SIG_IGN: 170b, *note signal.SIG_DFL:
     170a. or *note None: 389.  Here, *note signal.SIG_IGN: 170b. means
     that the signal was previously ignored, *note signal.SIG_DFL:
     170a. means that the default way of handling the signal was
     previously in use, and `None' means that the previous signal
     handler was not installed from Python.

 -- Function: signal.pause ()
     Cause the process to sleep until a signal is received; the
     appropriate handler will then be called.  Returns nothing.  Not on
     Windows. (See the Unix man page `signal(2)'.)

 -- Function: signal.setitimer (which, seconds[, interval])
     Sets given interval timer (one of *note signal.ITIMER_REAL: 170d,
     *note signal.ITIMER_VIRTUAL: 170e. or *note signal.ITIMER_PROF:
     170f.) specified by _which_ to fire after _seconds_ (float is
     accepted, different from *note alarm(): 1705.) and after that
     every _interval_ seconds. The interval timer specified by _which_
     can be cleared by setting seconds to zero.

     When an interval timer fires, a signal is sent to the process.
     The signal sent is dependent on the timer being used; *note
     signal.ITIMER_REAL: 170d. will deliver `SIGALRM', *note
     signal.ITIMER_VIRTUAL: 170e. sends `SIGVTALRM', and *note
     signal.ITIMER_PROF: 170f. will deliver `SIGPROF'.

     The old values are returned as a tuple: (delay, interval).

     Attempting to pass an invalid interval timer will cause an *note
     ItimerError: 1710.  Availability: Unix.

     New in version 2.6.

 -- Function: signal.getitimer (which)
     Returns current value of a given interval timer specified by
     _which_.  Availability: Unix.

     New in version 2.6.

 -- Function: signal.set_wakeup_fd (fd)
     Set the wakeup fd to _fd_.  When a signal is received, a `'\0''
     byte is written to the fd.  This can be used by a library to
     wakeup a poll or select call, allowing the signal to be fully
     processed.

     The old wakeup fd is returned.  _fd_ must be non-blocking.  It is
     up to the library to remove any bytes before calling poll or
     select again.

     When threads are enabled, this function can only be called from
     the main thread; attempting to call it from other threads will
     cause a *note ValueError: 233.  exception to be raised.

     New in version 2.6.

 -- Function: signal.siginterrupt (signalnum, flag)
     Change system call restart behaviour: if _flag_ is *note False:
     3a0, system calls will be restarted when interrupted by signal
     _signalnum_, otherwise system calls will be interrupted.  Returns
     nothing.  Availability: Unix (see the man page `siginterrupt(3)'
     for further information).

     Note that installing a signal handler with *note signal(): 156.
     will reset the restart behaviour to interruptible by implicitly
     calling `siginterrupt()' with a true _flag_ value for the given
     signal.

     New in version 2.6.

 -- Function: signal.signal (signalnum, handler)
     Set the handler for signal _signalnum_ to the function _handler_.
     _handler_ can be a callable Python object taking two arguments
     (see below), or one of the special values *note signal.SIG_IGN:
     170b. or *note signal.SIG_DFL: 170a.  The previous signal handler
     will be returned (see the description of *note getsignal(): 1706.
     above).  (See the Unix man page `signal(2)'.)

     When threads are enabled, this function can only be called from
     the main thread; attempting to call it from other threads will
     cause a *note ValueError: 233.  exception to be raised.

     The _handler_ is called with two arguments: the signal number and
     the current stack frame (`None' or a frame object; for a
     description of frame objects, see the *note description in the
     type hierarchy: 6d9. or see the attribute descriptions in the
     *note inspect: f9. module).

     On Windows, *note signal(): 156. can only be called with `SIGABRT',
     `SIGFPE', `SIGILL', `SIGINT', `SIGSEGV', or `SIGTERM'. A *note
     ValueError: 233. will be raised in any other case.

* Menu:

* Example: Example<8>.


File: python.info,  Node: Example<8>,  Up: signal --- Set handlers for asynchronous events

5.17.4.1 Example
................

Here is a minimal example program. It uses the *note alarm(): 1705.
function to limit the time spent waiting to open a file; this is useful
if the file is for a serial device that may not be turned on, which
would normally cause the *note os.open(): 5c3. to hang indefinitely.
The solution is to set a 5-second alarm before opening the file; if the
operation takes too long, the alarm signal will be sent, and the
handler raises an exception.

    import signal, os

    def handler(signum, frame):
        print 'Signal handler called with signal', signum
        raise IOError("Couldn't open device!")

    # Set the signal handler and a 5-second alarm
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(5)

    # This open() may hang indefinitely
    fd = os.open('/dev/ttyS0', os.O_RDWR)

    signal.alarm(0)          # Disable the alarm



File: python.info,  Node: popen2 --- Subprocesses with accessible I/O streams,  Next: asyncore --- Asynchronous socket handler,  Prev: signal --- Set handlers for asynchronous events,  Up: Interprocess Communication and Networking

5.17.5 `popen2' -- Subprocesses with accessible I/O streams
-----------------------------------------------------------

Deprecated since version 2.6: This module is obsolete.  Use the *note
subprocess: 168. module.  Check especially the *note Replacing Older
Functions with the subprocess Module: 10ab. section.

  This module allows you to spawn processes and connect to their
input/output/error pipes and obtain their return codes under Unix and
Windows.

  The *note subprocess: 168. module provides more powerful facilities
for spawning new processes and retrieving their results.  Using the
*note subprocess: 168. module is preferable to using the *note popen2:
135. module.

  The primary interface offered by this module is a trio of factory
functions.  For each of these, if _bufsize_ is specified,  it specifies
the buffer size for the I/O pipes.  _mode_, if provided, should be the
string `'b'' or `'t''; on Windows this is needed to determine whether
the file objects should be opened in binary or text mode.  The default
value for _mode_ is `'t''.

  On Unix, _cmd_ may be a sequence, in which case arguments will be
passed directly to the program without shell intervention (as with
*note os.spawnv(): 10ad.).  If _cmd_ is a string it will be passed to
the shell (as with *note os.system(): 3e9.).

  The only way to retrieve the return codes for the child processes is
by using the `poll()' or `wait()' methods on the *note Popen3: 16a0. and
*note Popen4: 16a1. classes; these are only available on Unix.  This
information is not available when using the *note popen2(): 135, *note
popen3(): 1717, and *note popen4(): 1718.  functions, or the equivalent
functions in the *note os: 129. module. (Note that the tuples returned
by the *note os: 129. module's functions are in a different order from
the ones returned by the *note popen2: 135. module.)

 -- Function: popen2.popen2 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout, child_stdin)'.

 -- Function: popen2.popen3 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout, child_stdin, child_stderr)'.

 -- Function: popen2.popen4 (cmd[, bufsize[, mode]])
     Executes _cmd_ as a sub-process.  Returns the file objects
     `(child_stdout_and_stderr, child_stdin)'.

     New in version 2.0.

  On Unix, a class defining the objects returned by the factory
functions is also available.  These are not used for the Windows
implementation, and are not available on that platform.

 -- Class: popen2.Popen3 (cmd[, capturestderr[, bufsize]])
     This class represents a child process.  Normally, *note Popen3:
     16a0. instances are created using the *note popen2(): 135. and
     *note popen3(): 1717. factory functions described above.

     If not using one of the helper functions to create *note Popen3:
     16a0. objects, the parameter _cmd_ is the shell command to execute
     in a sub-process.  The _capturestderr_ flag, if true, specifies
     that the object should capture standard error output of the child
     process. The default is false.  If the _bufsize_ parameter is
     specified, it specifies the size of the I/O buffers to/from the
     child process.

 -- Class: popen2.Popen4 (cmd[, bufsize])
     Similar to *note Popen3: 16a0, but always captures standard error
     into the same file object as standard output.  These are typically
     created using *note popen4(): 1718.

     New in version 2.0.

* Menu:

* Popen3 and Popen4 Objects::
* Flow Control Issues::


File: python.info,  Node: Popen3 and Popen4 Objects,  Next: Flow Control Issues,  Up: popen2 --- Subprocesses with accessible I/O streams

5.17.5.1 Popen3 and Popen4 Objects
..................................

Instances of the *note Popen3: 16a0. and *note Popen4: 16a1. classes
have the following methods:

 -- Method: Popen3.poll ()
     Returns `-1' if child process hasn't completed yet, or its status
     code (see *note wait(): 171d.) otherwise.

 -- Method: Popen3.wait ()
     Waits for and returns the status code of the child process.  The
     status code encodes both the return code of the process and
     information about whether it exited using the `exit()' system call
     or died due to a signal.  Functions to help interpret the status
     code are defined in the *note os: 129. module; see section *note
     Process Management: 1105. for the `W*()' family of functions.

  The following attributes are also available:

 -- Attribute: Popen3.fromchild
     A file object that provides output from the child process.  For
     *note Popen4: 16a1.  instances, this will provide both the
     standard output and standard error streams.

 -- Attribute: Popen3.tochild
     A file object that provides input to the child process.

 -- Attribute: Popen3.childerr
     A file object that provides error output from the child process, if
     _capturestderr_ was true for the constructor, otherwise `None'.
     This will always be `None' for *note Popen4: 16a1. instances.

 -- Attribute: Popen3.pid
     The process ID of the child process.


File: python.info,  Node: Flow Control Issues,  Prev: Popen3 and Popen4 Objects,  Up: popen2 --- Subprocesses with accessible I/O streams

5.17.5.2 Flow Control Issues
............................

Any time you are working with any form of inter-process communication,
control flow needs to be carefully thought out.  This remains the case
with the file objects provided by this module (or the *note os: 129.
module equivalents).

  When reading output from a child process that writes a lot of data to
standard error while the parent is reading from the child's standard
output, a deadlock can occur.  A similar situation can occur with other
combinations of reads and writes.  The essential factors are that more
than `_PC_PIPE_BUF' bytes are being written by one process in a
blocking fashion, while the other process is reading from the first
process, also in a blocking fashion.

  There are several ways to deal with this situation.

  The simplest application change, in many cases, will be to follow
this model in the parent process:

    import popen2

    r, w, e = popen2.popen3('python slave.py')
    e.readlines()
    r.readlines()
    r.close()
    e.close()
    w.close()

with code like this in the child:

    import os
    import sys

    # note that each of these print statements
    # writes a single long string

    print >>sys.stderr, 400 * 'this is a test\n'
    os.close(sys.stderr.fileno())
    print >>sys.stdout, 400 * 'this is another test\n'

In particular, note that `sys.stderr' must be closed after writing all
data, or `readlines()' won't return.  Also note that *note os.close():
10b4. must be used, as `sys.stderr.close()' won't close `stderr'
(otherwise assigning to `sys.stderr' will silently close it, so no
further errors can be printed).

  Applications which need to support a more general approach should
integrate I/O over pipes with their *note select(): 14f. loops, or use
separate threads to read each of the individual files provided by
whichever `popen*()' function or `Popen*' class was used.

See also
........

Module *note subprocess: 168.
     Module for spawning and managing subprocesses.


File: python.info,  Node: asyncore --- Asynchronous socket handler,  Next: asynchat --- Asynchronous socket command/response handler,  Prev: popen2 --- Subprocesses with accessible I/O streams,  Up: Interprocess Communication and Networking

5.17.6 `asyncore' -- Asynchronous socket handler
------------------------------------------------

This module provides the basic infrastructure for writing asynchronous
socket service clients and servers.

  There are only two ways to have a program on a single processor do
"more than one thing at a time." Multi-threaded programming is the
simplest and most popular way to do it, but there is another very
different technique, that lets you have nearly all the advantages of
multi-threading, without actually using multiple threads.  It's really
only practical if your program is largely I/O bound.  If your program
is processor bound, then pre-emptive scheduled threads are probably
what you really need.  Network servers are rarely processor bound,
however.

  If your operating system supports the `select()' system call in its
I/O library (and nearly all do), then you can use it to juggle multiple
communication channels at once; doing other work while your I/O is
taking place in the "background."  Although this strategy can seem
strange and complex, especially at first, it is in many ways easier to
understand and control than multi-threaded programming.  The *note
asyncore: 11. module solves many of the difficult problems for you,
making the task of building sophisticated high-performance network
servers and clients a snap.  For "conversational" applications and
protocols the companion *note asynchat: 10.  module is invaluable.

  The basic idea behind both modules is to create one or more network
_channels_, instances of class *note asyncore.dispatcher: 1725. and
*note asynchat.async_chat: 1726.  Creating the channels adds them to a
global map, used by the *note loop(): 1727. function if you do not
provide it with your own _map_.

  Once the initial channel(s) is(are) created, calling the *note
loop(): 1727. function activates channel service, which continues until
the last channel (including any that have been added to the map during
asynchronous service) is closed.

 -- Function: asyncore.loop ([timeout[, use_poll[, map[, count]]]])
     Enter a polling loop that terminates after count passes or all open
     channels have been closed.  All arguments are optional.  The
     _count_ parameter defaults to None, resulting in the loop
     terminating only when all channels have been closed.  The
     _timeout_ argument sets the timeout parameter for the appropriate
     *note select(): 14f. or `poll()' call, measured in seconds; the
     default is 30 seconds.  The _use_poll_ parameter, if true,
     indicates that `poll()' should be used in preference to *note
     select(): 14f.  (the default is `False').

     The _map_ parameter is a dictionary whose items are the channels
     to watch.  As channels are closed they are deleted from their map.
     If _map_ is omitted, a global map is used. Channels (instances of
     *note asyncore.dispatcher: 1725, *note asynchat.async_chat: 1726.
     and subclasses thereof) can freely be mixed in the map.

 -- Class: asyncore.dispatcher
     The *note dispatcher: 1725. class is a thin wrapper around a
     low-level socket object. To make it more useful, it has a few
     methods for event-handling which are called from the asynchronous
     loop.   Otherwise, it can be treated as a normal non-blocking
     socket object.

     The firing of low-level events at certain times or in certain
     connection states tells the asynchronous loop that certain
     higher-level events have taken place.  For example, if we have
     asked for a socket to connect to another host, we know that the
     connection has been made when the socket becomes writable for the
     first time (at this point you know that you may write to it with
     the expectation of success).  The implied higher-level events are:

     Event                      Description
     ------------------------------------------------------------------------ 
     `handle_connect()'         Implied by the first read or write event
     `handle_close()'           Implied by a read event with no data
                                available
     `handle_accept()'          Implied by a read event on a listening
                                socket

     During asynchronous processing, each mapped channel's *note
     readable(): 1728. and *note writable(): 1729. methods are used to
     determine whether the channel's socket should be added to the list
     of channels `select()'ed or `poll()'ed for read and write events.

     Thus, the set of channel events is larger than the basic socket
     events.  The full set of methods that can be overridden in your
     subclass follows:

      -- Method: handle_read ()
          Called when the asynchronous loop detects that a `read()'
          call on the channel's socket will succeed.

      -- Method: handle_write ()
          Called when the asynchronous loop detects that a writable
          socket can be written.  Often this method will implement the
          necessary buffering for performance.  For example:

              def handle_write(self):
                  sent = self.send(self.buffer)
                  self.buffer = self.buffer[sent:]



      -- Method: handle_expt ()
          Called when there is out of band (OOB) data for a socket
          connection.  This will almost never happen, as OOB is
          tenuously supported and rarely used.

      -- Method: handle_connect ()
          Called when the active opener's socket actually makes a
          connection.  Might send a "welcome" banner, or initiate a
          protocol negotiation with the remote endpoint, for example.

      -- Method: handle_close ()
          Called when the socket is closed.

      -- Method: handle_error ()
          Called when an exception is raised and not otherwise handled.
          The default version prints a condensed traceback.

      -- Method: handle_accept ()
          Called on listening channels (passive openers) when a
          connection can be established with a new remote endpoint that
          has issued a *note connect(): 1731.  call for the local
          endpoint.

      -- Method: readable ()
          Called each time around the asynchronous loop to determine
          whether a channel's socket should be added to the list on
          which read events can occur.  The default method simply
          returns `True', indicating that by default, all channels will
          be interested in read events.

      -- Method: writable ()
          Called each time around the asynchronous loop to determine
          whether a channel's socket should be added to the list on
          which write events can occur.  The default method simply
          returns `True', indicating that by default, all channels will
          be interested in write events.

     In addition, each channel delegates or extends many of the socket
     methods.  Most of these are nearly identical to their socket
     partners.

      -- Method: create_socket (family, type)
          This is identical to the creation of a normal socket, and
          will use the same options for creation.  Refer to the *note
          socket: 15d. documentation for information on creating
          sockets.

      -- Method: connect (address)
          As with the normal socket object, _address_ is a tuple with
          the first element the host to connect to, and the second the
          port number.

      -- Method: send (data)
          Send _data_ to the remote end-point of the socket.

      -- Method: recv (buffer_size)
          Read at most _buffer_size_ bytes from the socket's remote
          end-point.  An empty string implies that the channel has been
          closed from the other end.

      -- Method: listen (backlog)
          Listen for connections made to the socket.  The _backlog_
          argument specifies the maximum number of queued connections
          and should be at least 1; the maximum value is
          system-dependent (usually 5).

      -- Method: bind (address)
          Bind the socket to _address_.  The socket must not already be
          bound.  (The format of _address_ depends on the address
          family -- refer to the *note socket: 15d. documentation for
          more information.)  To mark the socket as re-usable (setting
          the `SO_REUSEADDR' option), call the *note dispatcher: 1725.
          object's `set_reuse_addr()' method.

      -- Method: accept ()
          Accept a connection.  The socket must be bound to an address
          and listening for connections.  The return value can be
          either `None' or a pair `(conn, address)' where _conn_ is a
          _new_ socket object usable to send and receive data on the
          connection, and _address_ is the address bound to the socket
          on the other end of the connection.  When `None' is returned
          it means the connection didn't take place, in which case the
          server should just ignore this event and keep listening for
          further incoming connections.

      -- Method: close ()
          Close the socket.  All future operations on the socket object
          will fail.  The remote end-point will receive no more data
          (after queued data is flushed).  Sockets are automatically
          closed when they are garbage-collected.

 -- Class: asyncore.dispatcher_with_send
     A *note dispatcher: 1725. subclass which adds simple buffered
     output capability, useful for simple clients. For more
     sophisticated usage use *note asynchat.async_chat: 1726.

 -- Class: asyncore.file_dispatcher
     A file_dispatcher takes a file descriptor or file object along
     with an optional map argument and wraps it for use with the
     `poll()' or `loop()' functions.  If provided a file object or
     anything with a `fileno()' method, that method will be called and
     passed to the *note file_wrapper: 173b. constructor.
     Availability: UNIX.

 -- Class: asyncore.file_wrapper
     A file_wrapper takes an integer file descriptor and calls *note
     os.dup(): 10b7. to duplicate the handle so that the original
     handle may be closed independently of the file_wrapper.  This
     class implements sufficient methods to emulate a socket for use by
     the *note file_dispatcher: 173a. class.  Availability: UNIX.

* Menu:

* asyncore Example basic HTTP client::
* asyncore Example basic echo server::


File: python.info,  Node: asyncore Example basic HTTP client,  Next: asyncore Example basic echo server,  Up: asyncore --- Asynchronous socket handler

5.17.6.1 asyncore Example basic HTTP client
...........................................

Here is a very basic HTTP client that uses the *note dispatcher: 1725.
class to implement its socket handling:

    import asyncore, socket

    class HTTPClient(asyncore.dispatcher):

        def __init__(self, host, path):
            asyncore.dispatcher.__init__(self)
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.connect( (host, 80) )
            self.buffer = 'GET %s HTTP/1.0\r\n\r\n' % path

        def handle_connect(self):
            pass

        def handle_close(self):
            self.close()

        def handle_read(self):
            print self.recv(8192)

        def writable(self):
            return (len(self.buffer) > 0)

        def handle_write(self):
            sent = self.send(self.buffer)
            self.buffer = self.buffer[sent:]


    client = HTTPClient('www.python.org', '/')
    asyncore.loop()



File: python.info,  Node: asyncore Example basic echo server,  Prev: asyncore Example basic HTTP client,  Up: asyncore --- Asynchronous socket handler

5.17.6.2 asyncore Example basic echo server
...........................................

Here is a basic echo server that uses the *note dispatcher: 1725. class
to accept connections and dispatches the incoming connections to a
handler:

    import asyncore
    import socket

    class EchoHandler(asyncore.dispatcher_with_send):

        def handle_read(self):
            data = self.recv(8192)
            if data:
                self.send(data)

    class EchoServer(asyncore.dispatcher):

        def __init__(self, host, port):
            asyncore.dispatcher.__init__(self)
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.set_reuse_addr()
            self.bind((host, port))
            self.listen(5)

        def handle_accept(self):
            pair = self.accept()
            if pair is None:
                pass
            else:
                sock, addr = pair
                print 'Incoming connection from %s' % repr(addr)
                handler = EchoHandler(sock)

    server = EchoServer('localhost', 8080)
    asyncore.loop()



File: python.info,  Node: asynchat --- Asynchronous socket command/response handler,  Prev: asyncore --- Asynchronous socket handler,  Up: Interprocess Communication and Networking

5.17.7 `asynchat' -- Asynchronous socket command/response handler
-----------------------------------------------------------------

This module builds on the *note asyncore: 11. infrastructure,
simplifying asynchronous clients and servers and making it easier to
handle protocols whose elements are terminated by arbitrary strings, or
are of variable length.  *note asynchat: 10. defines the abstract class
*note async_chat: 1726. that you subclass, providing implementations of
the `collect_incoming_data()' and `found_terminator()' methods. It uses
the same asynchronous loop as *note asyncore: 11, and the two types of
channel, *note asyncore.dispatcher: 1725.  and *note
asynchat.async_chat: 1726, can freely be mixed in the channel map.
Typically an *note asyncore.dispatcher: 1725. server channel generates
new *note asynchat.async_chat: 1726. channel objects as it receives
incoming connection requests.

 -- Class: asynchat.async_chat
     This class is an abstract subclass of *note asyncore.dispatcher:
     1725. To make practical use of the code you must subclass *note
     async_chat: 1726, providing meaningful *note
     collect_incoming_data(): 1742. and *note found_terminator(): 1743.
     methods.  The *note asyncore.dispatcher: 1725. methods can be
     used, although not all make sense in a message/response context.

     Like *note asyncore.dispatcher: 1725, *note async_chat: 1726.
     defines a set of events that are generated by an analysis of
     socket conditions after a `select()' call. Once the polling loop
     has been started the *note async_chat: 1726. object's methods are
     called by the event-processing framework with no action on the
     part of the programmer.

     Two class attributes can be modified, to improve performance, or
     possibly even to conserve memory.

      -- Data: ac_in_buffer_size
          The asynchronous input buffer size (default `4096').

      -- Data: ac_out_buffer_size
          The asynchronous output buffer size (default `4096').

     Unlike *note asyncore.dispatcher: 1725, *note async_chat: 1726.
     allows you to define a first-in-first-out queue (fifo) of
     _producers_. A producer need have only one method, `more()', which
     should return data to be transmitted on the channel.  The producer
     indicates exhaustion (_i.e._ that it contains no more data) by
     having its `more()' method return the empty string. At this point
     the *note async_chat: 1726. object removes the producer from the
     fifo and starts using the next producer, if any. When the producer
     fifo is empty the `handle_write()' method does nothing. You use
     the channel object's *note set_terminator(): 1746. method to
     describe how to recognize the end of, or an important breakpoint
     in, an incoming transmission from the remote endpoint.

     To build a functioning *note async_chat: 1726. subclass your
     input methods *note collect_incoming_data(): 1742. and *note
     found_terminator(): 1743. must handle the data that the channel
     receives asynchronously. The methods are described below.

 -- Method: async_chat.close_when_done ()
     Pushes a `None' on to the producer fifo. When this producer is
     popped off the fifo it causes the channel to be closed.

 -- Method: async_chat.collect_incoming_data (data)
     Called with _data_ holding an arbitrary amount of received data.
     The default method, which must be overridden, raises a *note
     NotImplementedError: 919. exception.

 -- Method: async_chat.discard_buffers ()
     In emergencies this method will discard any data held in the input
     and/or output buffers and the producer fifo.

 -- Method: async_chat.found_terminator ()
     Called when the incoming data stream  matches the termination
     condition set by *note set_terminator(): 1746. The default method,
     which must be overridden, raises a *note NotImplementedError: 919.
     exception. The buffered input data should be available via an
     instance attribute.

 -- Method: async_chat.get_terminator ()
     Returns the current terminator for the channel.

 -- Method: async_chat.push (data)
     Pushes data on to the channel's fifo to ensure its transmission.
     This is all you need to do to have the channel write the data out
     to the network, although it is possible to use your own producers
     in more complex schemes to implement encryption and chunking, for
     example.

 -- Method: async_chat.push_with_producer (producer)
     Takes a producer object and adds it to the producer fifo
     associated with the channel.  When all currently-pushed producers
     have been exhausted the channel will consume this producer's data
     by calling its `more()' method and send the data to the remote
     endpoint.

 -- Method: async_chat.set_terminator (term)
     Sets the terminating condition to be recognized on the channel.
     `term' may be any of three types of value, corresponding to three
     different ways to handle incoming protocol data.

     term            Description
     ------------------------------------------------------------------ 
     _string_        Will call *note found_terminator(): 1743. when
                     the string is found in the input stream
     _integer_       Will call *note found_terminator(): 1743. when
                     the indicated number of characters have been
                     received
     `None'          The channel continues to collect data forever

     Note that any data following the terminator will be available for
     reading by the channel after *note found_terminator(): 1743. is
     called.

* Menu:

* asynchat - Auxiliary Classes::
* asynchat Example::


File: python.info,  Node: asynchat - Auxiliary Classes,  Next: asynchat Example,  Up: asynchat --- Asynchronous socket command/response handler

5.17.7.1 asynchat - Auxiliary Classes
.....................................

 -- Class: asynchat.fifo ([list=None])
     A *note fifo: 174d. holding data which has been pushed by the
     application but not yet popped for writing to the channel.  A
     *note fifo: 174d. is a list used to hold data and/or producers
     until they are required.  If the _list_ argument is provided then
     it should contain producers or data items to be written to the
     channel.

      -- Method: is_empty ()
          Returns `True' if and only if the fifo is empty.

      -- Method: first ()
          Returns the least-recently *note push(): 1750.ed item from
          the fifo.

      -- Method: push (data)
          Adds the given data (which may be a string or a producer
          object) to the producer fifo.

      -- Method: pop ()
          If the fifo is not empty, returns `True, first()', deleting
          the popped item.  Returns `False, None' for an empty fifo.


File: python.info,  Node: asynchat Example,  Prev: asynchat - Auxiliary Classes,  Up: asynchat --- Asynchronous socket command/response handler

5.17.7.2 asynchat Example
.........................

The following partial example shows how HTTP requests can be read with
*note async_chat: 1726.  A web server might create an
`http_request_handler' object for each incoming client connection.
Notice that initially the channel terminator is set to match the blank
line at the end of the HTTP headers, and a flag indicates that the
headers are being read.

  Once the headers have been read, if the request is of type POST
(indicating that further data are present in the input stream) then the
`Content-Length:' header is used to set a numeric terminator to read the
right amount of data from the channel.

  The `handle_request()' method is called once all relevant input has
been marshalled, after setting the channel terminator to `None' to
ensure that any extraneous data sent by the web client are ignored.

    class http_request_handler(asynchat.async_chat):

        def __init__(self, sock, addr, sessions, log):
            asynchat.async_chat.__init__(self, sock=sock)
            self.addr = addr
            self.sessions = sessions
            self.ibuffer = []
            self.obuffer = ""
            self.set_terminator("\r\n\r\n")
            self.reading_headers = True
            self.handling = False
            self.cgi_data = None
            self.log = log

        def collect_incoming_data(self, data):
            """Buffer the data"""
            self.ibuffer.append(data)

        def found_terminator(self):
            if self.reading_headers:
                self.reading_headers = False
                self.parse_headers("".join(self.ibuffer))
                self.ibuffer = []
                if self.op.upper() == "POST":
                    clen = self.headers.getheader("content-length")
                    self.set_terminator(int(clen))
                else:
                    self.handling = True
                    self.set_terminator(None)
                    self.handle_request()
            elif not self.handling:
                self.set_terminator(None) # browsers sometimes over-send
                self.cgi_data = parse(self.headers, "".join(self.ibuffer))
                self.handling = True
                self.ibuffer = []
                self.handle_request()



File: python.info,  Node: Internet Data Handling,  Next: Structured Markup Processing Tools,  Prev: Interprocess Communication and Networking,  Up: The Python Standard Library

5.18 Internet Data Handling
===========================

This chapter describes modules which support handling data formats
commonly used on the Internet.

* Menu:

* email: email --- An email and MIME handling package. An email and MIME handling package
* json: json --- JSON encoder and decoder. JSON encoder and decoder
* mailcap: mailcap --- Mailcap file handling. Mailcap file handling
* mailbox: mailbox --- Manipulate mailboxes in various formats. Manipulate mailboxes in various formats
* mhlib: mhlib --- Access to MH mailboxes. Access to MH mailboxes
* mimetools: mimetools --- Tools for parsing MIME messages. Tools for parsing MIME messages
* mimetypes: mimetypes --- Map filenames to MIME types. Map filenames to MIME types
* MimeWriter: MimeWriter --- Generic MIME file writer. Generic MIME file writer
* mimify: mimify --- MIME processing of mail messages. MIME processing of mail messages
* multifile: multifile --- Support for files containing distinct parts. Support for files containing distinct parts
* rfc822: rfc822 --- Parse RFC 2822 mail headers. Parse RFC 2822 mail headers
* base64: base64 --- RFC 3548 Base16 Base32 Base64 Data Encodings. RFC 3548: Base16, Base32, Base64 Data Encodings
* binhex: binhex --- Encode and decode binhex4 files. Encode and decode binhex4 files
* binascii: binascii --- Convert between binary and ASCII. Convert between binary and ASCII
* quopri: quopri --- Encode and decode MIME quoted-printable data. Encode and decode MIME quoted-printable data
* uu: uu --- Encode and decode uuencode files. Encode and decode uuencode files

email --- An email and MIME handling package

* email; Representing an email message: email Representing an email message.
* email; Parsing email messages: email Parsing email messages.
* email; Generating MIME documents: email Generating MIME documents.
* email; Creating email and MIME objects from scratch: email Creating email and MIME objects from scratch.
* email; Internationalized headers: email Internationalized headers.
* email; Representing character sets: email Representing character sets.
* email; Encoders: email Encoders.
* email; Exception and Defect classes: email Exception and Defect classes.
* email; Miscellaneous utilities: email Miscellaneous utilities.
* email; Iterators: email Iterators.
* email; Examples: email Examples.
* Package History::
* Differences from mimelib::

email: Parsing email messages

* FeedParser API::
* Parser class API::
* Additional notes::

json --- JSON encoder and decoder

* Basic Usage::
* Encoders and decoders::

mailbox --- Manipulate mailboxes in various formats

* Mailbox objects::
* Message objects::
* Exceptions: Exceptions<5>.
* Deprecated classes and methods::
* Examples: Examples<9>.

Mailbox objects

* Maildir::
* mbox::
* MH::
* Babyl::
* MMDF::

Message objects

* MaildirMessage::
* mboxMessage::
* MHMessage::
* BabylMessage::
* MMDFMessage::

mhlib --- Access to MH mailboxes

* MH Objects::
* Folder Objects::
* Message Objects::

mimetools --- Tools for parsing MIME messages

* Additional Methods of Message Objects::

mimetypes --- Map filenames to MIME types

* MimeTypes Objects::

MimeWriter --- Generic MIME file writer

* MimeWriter Objects::

multifile --- Support for files containing distinct parts

* MultiFile Objects::
* MultiFile Example::

rfc822 --- Parse RFC 2822 mail headers

* Message Objects: Message Objects<2>.
* AddressList Objects::

binhex --- Encode and decode binhex4 files

* Notes: Notes<2>.


File: python.info,  Node: email --- An email and MIME handling package,  Next: json --- JSON encoder and decoder,  Up: Internet Data Handling

5.18.1 `email' -- An email and MIME handling package
----------------------------------------------------

New in version 2.2.

  The *note email: bc. package is a library for managing email
messages, including MIME and other RFC 2822(1)-based message documents.
It subsumes most of the functionality in several older standard
modules such as *note rfc822: 149, *note mimetools: 110, *note
multifile: 119, and other non-standard packages such as `mimecntl'.  It
is specifically _not_ designed to do any sending of email messages to
SMTP ( RFC 2821(2)), NNTP, or other servers; those are functions of
modules such as *note smtplib: 15b. and *note nntplib: 125. The *note
email: bc. package attempts to be as RFC-compliant as possible,
supporting in addition to RFC 2822(3), such MIME-related RFCs as RFC
2045(4), RFC 2046(5), RFC 2047(6), and RFC 2231(7).

  The primary distinguishing feature of the *note email: bc. package is
that it splits the parsing and generating of email messages from the
internal _object model_ representation of email.  Applications using
the *note email: bc. package deal primarily with objects; you can add
sub-objects to messages, remove sub-objects from messages, completely
re-arrange the contents, etc.  There is a separate parser and a
separate generator which handles the transformation from flat text to
the object model, and then back to flat text again.  There are also
handy subclasses for some common MIME object types, and a few
miscellaneous utilities that help with such common tasks as extracting
and parsing message field values, creating RFC-compliant dates, etc.

  The following sections describe the functionality of the *note email:
bc. package.  The ordering follows a progression that should be common
in applications: an email message is read as flat text from a file or
other source, the text is parsed to produce the object structure of the
email message, this structure is manipulated, and finally, the object
tree is rendered back into flat text.

  It is perfectly feasible to create the object structure out of whole
cloth -- i.e. completely from scratch.  From there, a similar
progression can be taken as above.

  Also included are detailed specifications of all the classes and
modules that the *note email: bc. package provides, the exception
classes you might encounter while using the *note email: bc. package,
some auxiliary utilities, and a few examples.  For users of the older
`mimelib' package, or previous versions of the *note email: bc.
package, a section on differences and porting is provided.

  Contents of the *note email: bc. package documentation:

* Menu:

* email; Representing an email message: email Representing an email message.
* email; Parsing email messages: email Parsing email messages.
* email; Generating MIME documents: email Generating MIME documents.
* email; Creating email and MIME objects from scratch: email Creating email and MIME objects from scratch.
* email; Internationalized headers: email Internationalized headers.
* email; Representing character sets: email Representing character sets.
* email; Encoders: email Encoders.
* email; Exception and Defect classes: email Exception and Defect classes.
* email; Miscellaneous utilities: email Miscellaneous utilities.
* email; Iterators: email Iterators.
* email; Examples: email Examples.
* Package History::
* Differences from mimelib::

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc2822.html

  (2) http://tools.ietf.org/html/rfc2821.html

  (3) http://tools.ietf.org/html/rfc2822.html

  (4) http://tools.ietf.org/html/rfc2045.html

  (5) http://tools.ietf.org/html/rfc2046.html

  (6) http://tools.ietf.org/html/rfc2047.html

  (7) http://tools.ietf.org/html/rfc2231.html


File: python.info,  Node: email Representing an email message,  Next: email Parsing email messages,  Up: email --- An email and MIME handling package

5.18.1.1 `email': Representing an email message
...............................................

The central class in the *note email: bc. package is the *note Message:
213. class, imported from the *note email.message: c3. module.  It is
the base class for the *note email: bc. object model.  *note Message:
213. provides the core functionality for setting and querying header
fields, and for accessing message bodies.

  Conceptually, a *note Message: 213. object consists of _headers_ and
_payloads_.  Headers are RFC 2822(1) style field names and values where
the field name and value are separated by a colon.  The colon is not
part of either the field name or the field value.

  Headers are stored and returned in case-preserving form but are
matched case-insensitively.  There may also be a single envelope
header, also known as the _Unix-From_ header or the `From_' header.
The payload is either a string in the case of simple message objects or
a list of *note Message: 213. objects for MIME container documents
(e.g. `multipart/*' and `message/rfc822').

  *note Message: 213. objects provide a mapping style interface for
accessing the message headers, and an explicit interface for accessing
both the headers and the payload.  It provides convenience methods for
generating a flat text representation of the message object tree, for
accessing commonly used header parameters, and for recursively walking
over the object tree.

  Here are the methods of the *note Message: 213. class:

 -- Class: email.message.Message
     The constructor takes no arguments.

      -- Method: as_string ([unixfrom])
          Return the entire message flattened as a string.  When
          optional _unixfrom_ is `True', the envelope header is
          included in the returned string.  _unixfrom_ defaults to
          `False'.  Flattening the message may trigger changes to the
          *note Message: 213. if defaults need to be filled in to
          complete the transformation to a string (for example, MIME
          boundaries may be generated or modified).

          Note that this method is provided as a convenience and may
          not always format the message the way you want.  For example,
          by default it mangles lines that begin with `From'.  For more
          flexibility, instantiate a *note Generator: 175c. instance
          and use its `flatten()' method directly.  For example:

              from cStringIO import StringIO
              from email.generator import Generator
              fp = StringIO()
              g = Generator(fp, mangle_from_=False, maxheaderlen=60)
              g.flatten(msg)
              text = fp.getvalue()



      -- Method: __str__ ()
          Equivalent to `as_string(unixfrom=True)'.

      -- Method: is_multipart ()
          Return `True' if the message's payload is a list of sub-*note
          Message: 213. objects, otherwise return `False'.  When *note
          is_multipart(): 175e. returns False, the payload should be a
          string object.

      -- Method: set_unixfrom (unixfrom)
          Set the message's envelope header to _unixfrom_, which should
          be a string.

      -- Method: get_unixfrom ()
          Return the message's envelope header.  Defaults to `None' if
          the envelope header was never set.

      -- Method: attach (payload)
          Add the given _payload_ to the current payload, which must be
          `None' or a list of *note Message: 213. objects before the
          call. After the call, the payload will always be a list of
          *note Message: 213. objects.  If you want to set the payload
          to a scalar object (e.g. a string), use *note set_payload():
          1762. instead.

      -- Method: get_payload ([i[, decode]])
          Return the current payload, which will be a list of *note
          Message: 213. objects when *note is_multipart(): 175e. is
          `True', or a string when *note is_multipart(): 175e. is
          `False'.  If the payload is a list and you mutate the list
          object, you modify the message's payload in place.

          With optional argument _i_, *note get_payload(): 1763. will
          return the _i_-th element of the payload, counting from zero,
          if *note is_multipart(): 175e. is `True'.  An *note
          IndexError: 4ce. will be raised if _i_ is less than 0 or
          greater than or equal to the number of items in the payload.
          If the payload is a string (i.e.  *note is_multipart(): 175e.
          is `False') and _i_ is given, a *note TypeError: 215. is
          raised.

          Optional _decode_ is a flag indicating whether the payload
          should be decoded or not, according to the
          `Content-Transfer-Encoding' header. When `True' and the
          message is not a multipart, the payload will be decoded if
          this header's value is `quoted-printable' or `base64'.  If
          some other encoding is used, or `Content-Transfer-Encoding'
          header is missing, or if the payload has bogus base64 data,
          the payload is returned as-is (undecoded).  If the message is
          a multipart and the _decode_ flag is `True', then `None' is
          returned.  The default for _decode_ is `False'.

      -- Method: set_payload (payload[, charset])
          Set the entire message object's payload to _payload_.  It is
          the client's responsibility to ensure the payload invariants.
          Optional _charset_ sets the message's default character set;
          see *note set_charset(): 1764. for details.

          Changed in version 2.2.2: _charset_ argument added.

      -- Method: set_charset (charset)
          Set the character set of the payload to _charset_, which can
          either be a *note Charset: 1765. instance (see *note
          email.charset: bd.), a string naming a character set, or
          `None'.  If it is a string, it will be converted to a *note
          Charset: 1765. instance.  If _charset_ is `None', the
          `charset' parameter will be removed from the `Content-Type'
          header (the message will not be otherwise modified).
          Anything else will generate a *note TypeError: 215.

          If there is no existing `MIME-Version' header one will be
          added.  If there is no existing `Content-Type' header, one
          will be added with a value of `text/plain'.  Whether the
          `Content-Type' header already exists or not, its `charset'
          parameter will be set to _charset.output_charset_.   If
          _charset.input_charset_ and _charset.output_charset_ differ,
          the payload will be re-encoded to the _output_charset_.  If
          there is no existing `Content-Transfer-Encoding' header, then
          the payload will be transfer-encoded, if needed, using the
          specified *note Charset: 1765, and a header with the
          appropriate value will be added.  If a
          `Content-Transfer-Encoding' header already exists, the
          payload is assumed to already be correctly encoded using that
          `Content-Transfer-Encoding' and is not modified.

          The message will be assumed to be of type `text/*', with the
          payload either in unicode or encoded with
          _charset.input_charset_.  It will be encoded or converted to
          _charset.output_charset_ and transfer encoded properly, if
          needed, when generating the plain text representation of the
          message.  MIME headers (`MIME-Version', `Content-Type',
          `Content-Transfer-Encoding') will be added as needed.

          New in version 2.2.2.

      -- Method: get_charset ()
          Return the *note Charset: 1765. instance associated with the
          message's payload.

          New in version 2.2.2.

     The following methods implement a mapping-like interface for
     accessing the message's RFC 2822(2) headers.  Note that there are
     some semantic differences between these methods and a normal
     mapping (i.e. dictionary) interface.  For example, in a dictionary
     there are no duplicate keys, but here there may be duplicate
     message headers.  Also, in dictionaries there is no guaranteed
     order to the keys returned by *note keys(): 1767, but in a *note
     Message: 213. object, headers are always returned in the order
     they appeared in the original message, or were added to the
     message later.  Any header deleted and then re-added are always
     appended to the end of the header list.

     These semantic differences are intentional and are biased toward
     maximal convenience.

     Note that in all cases, any envelope header present in the message
     is not included in the mapping interface.

      -- Method: __len__ ()
          Return the total number of headers, including duplicates.

      -- Method: __contains__ (name)
          Return true if the message object has a field named _name_.
          Matching is done case-insensitively and _name_ should not
          include the trailing colon.  Used for the `in' operator, e.g.:

              if 'message-id' in myMessage:
                  print 'Message-ID:', myMessage['message-id']



      -- Method: __getitem__ (name)
          Return the value of the named header field.  _name_ should
          not include the colon field separator.  If the header is
          missing, `None' is returned; a *note KeyError: 202. is never
          raised.

          Note that if the named field appears more than once in the
          message's headers, exactly which of those field values will
          be returned is undefined.  Use the *note get_all(): 176b.
          method to get the values of all the extant named headers.

      -- Method: __setitem__ (name, val)
          Add a header to the message with field name _name_ and value
          _val_.  The field is appended to the end of the message's
          existing fields.

          Note that this does _not_ overwrite or delete any existing
          header with the same name.  If you want to ensure that the
          new header is the only one present in the message with field
          name _name_, delete the field first, e.g.:

              del msg['subject']
              msg['subject'] = 'Python roolz!'



      -- Method: __delitem__ (name)
          Delete all occurrences of the field with name _name_ from the
          message's headers.  No exception is raised if the named field
          isn't present in the headers.

      -- Method: has_key (name)
          Return true if the message contains a header field named
          _name_, otherwise return false.

      -- Method: keys ()
          Return a list of all the message's header field names.

      -- Method: values ()
          Return a list of all the message's field values.

      -- Method: items ()
          Return a list of 2-tuples containing all the message's field
          headers and values.

      -- Method: get (name[, failobj])
          Return the value of the named header field.  This is
          identical to *note __getitem__(): 176a. except that optional
          _failobj_ is returned if the named header is missing
          (defaults to `None').

     Here are some additional useful methods:

      -- Method: get_all (name[, failobj])
          Return a list of all the values for the field named _name_.
          If there are no such named headers in the message, _failobj_
          is returned (defaults to `None').

      -- Method: add_header (_name, _value, **_params)
          Extended header setting.  This method is similar to *note
          __setitem__(): 176c.  except that additional header
          parameters can be provided as keyword arguments.  __name_ is
          the header field to add and __value_ is the _primary_ value
          for the header.

          For each item in the keyword argument dictionary __params_,
          the key is taken as the parameter name, with underscores
          converted to dashes (since dashes are illegal in Python
          identifiers).  Normally, the parameter will be added as
          `key="value"' unless the value is `None', in which case only
          the key will be added.  If the value contains non-ASCII
          characters, it must be specified as a three tuple in the
          format `(CHARSET, LANGUAGE, VALUE)', where `CHARSET' is a
          string naming the charset to be used to encode the value,
          `LANGUAGE' can usually be set to `None' or the empty string
          (see RFC 2231(3) for other possibilities), and `VALUE' is the
          string value containing non-ASCII code points.

          Here's an example:

              msg.add_header('Content-Disposition', 'attachment', filename='bud.gif')

          This will add a header that looks like

              Content-Disposition: attachment; filename="bud.gif"

          An example with with non-ASCII characters:

              msg.add_header('Content-Disposition', 'attachment',
                             filename=('iso-8859-1', '', 'Fußballer.ppt'))

          Which produces

              Content-Disposition: attachment; filename*="iso-8859-1''Fu%DFballer.ppt"



      -- Method: replace_header (_name, _value)
          Replace a header.  Replace the first header found in the
          message that matches __name_, retaining header order and
          field name case.  If no matching header was found, a *note
          KeyError: 202. is raised.

          New in version 2.2.2.

      -- Method: get_content_type ()
          Return the message's content type.  The returned string is
          coerced to lower case of the form `maintype/subtype'.  If
          there was no `Content-Type' header in the message the default
          type as given by *note get_default_type(): 1775. will be
          returned.  Since according to RFC 2045(4), messages always
          have a default type, *note get_content_type(): 1774.  will
          always return a value.

          RFC 2045(5) defines a message's default type to be
          `text/plain' unless it appears inside a `multipart/digest'
          container, in which case it would be `message/rfc822'.  If the
          `Content-Type' header has an invalid type specification, RFC
          2045(6) mandates that the default type be `text/plain'.

          New in version 2.2.2.

      -- Method: get_content_maintype ()
          Return the message's main content type.  This is the
          `maintype' part of the string returned by *note
          get_content_type(): 1774.

          New in version 2.2.2.

      -- Method: get_content_subtype ()
          Return the message's sub-content type.  This is the `subtype'
          part of the string returned by *note get_content_type(): 1774.

          New in version 2.2.2.

      -- Method: get_default_type ()
          Return the default content type.  Most messages have a
          default content type of `text/plain', except for messages
          that are subparts of `multipart/digest' containers.  Such
          subparts have a default content type of `message/rfc822'.

          New in version 2.2.2.

      -- Method: set_default_type (ctype)
          Set the default content type.  _ctype_ should either be
          `text/plain' or `message/rfc822', although this is not
          enforced.  The default content type is not stored in the
          `Content-Type' header.

          New in version 2.2.2.

      -- Method: get_params ([failobj[, header[, unquote]]])
          Return the message's `Content-Type' parameters, as a list.
          The elements of the returned list are 2-tuples of key/value
          pairs, as split on the `'='' sign.  The left hand side of the
          `'='' is the key, while the right hand side is the value.  If
          there is no `'='' sign in the parameter the value is the
          empty string, otherwise the value is as described in *note
          get_param(): 177a. and is unquoted if optional _unquote_ is
          `True' (the default).

          Optional _failobj_ is the object to return if there is no
          `Content-Type' header.  Optional _header_ is the header to
          search instead of `Content-Type'.

          Changed in version 2.2.2: _unquote_ argument added.

      -- Method: get_param (param[, failobj[, header[, unquote]]])
          Return the value of the `Content-Type' header's parameter
          _param_ as a string.  If the message has no `Content-Type'
          header or if there is no such parameter, then _failobj_ is
          returned (defaults to `None').

          Optional _header_ if given, specifies the message header to
          use instead of `Content-Type'.

          Parameter keys are always compared case insensitively.  The
          return value can either be a string, or a 3-tuple if the
          parameter was RFC 2231(7) encoded.  When it's a 3-tuple, the
          elements of the value are of the form `(CHARSET, LANGUAGE,
          VALUE)'.  Note that both `CHARSET' and `LANGUAGE' can be
          `None', in which case you should consider `VALUE' to be
          encoded in the `us-ascii' charset.  You can usually ignore
          `LANGUAGE'.

          If your application doesn't care whether the parameter was
          encoded as in RFC 2231(8), you can collapse the parameter
          value by calling *note email.utils.collapse_rfc2231_value():
          177b, passing in the return value from *note get_param():
          177a.  This will return a suitably decoded Unicode string
          when the value is a tuple, or the original string unquoted if
          it isn't.  For example:

              rawparam = msg.get_param('foo')
              param = email.utils.collapse_rfc2231_value(rawparam)

          In any case, the parameter value (either the returned string,
          or the `VALUE' item in the 3-tuple) is always unquoted,
          unless _unquote_ is set to `False'.

          Changed in version 2.2.2: _unquote_ argument added, and
          3-tuple return value possible.

      -- Method: set_param (param, value[, header[, requote[, charset[,
               language]]]])
          Set a parameter in the `Content-Type' header.  If the
          parameter already exists in the header, its value will be
          replaced with _value_.  If the `Content-Type' header as not
          yet been defined for this message, it will be set to
          `text/plain' and the new parameter value will be appended as
          per RFC 2045(9).

          Optional _header_ specifies an alternative header to
          `Content-Type', and all parameters will be quoted as necessary
          unless optional _requote_ is `False' (the default is `True').

          If optional _charset_ is specified, the parameter will be
          encoded according to RFC 2231(10). Optional _language_
          specifies the RFC 2231 language, defaulting to the empty
          string.  Both _charset_ and _language_ should be strings.

          New in version 2.2.2.

      -- Method: del_param (param[, header[, requote]])
          Remove the given parameter completely from the `Content-Type'
          header.  The header will be re-written in place without the
          parameter or its value.  All values will be quoted as
          necessary unless _requote_ is `False' (the default is
          `True').  Optional _header_ specifies an alternative to
          `Content-Type'.

          New in version 2.2.2.

      -- Method: set_type (type[, header][, requote])
          Set the main type and subtype for the `Content-Type' header.
          _type_ must be a string in the form `maintype/subtype',
          otherwise a *note ValueError: 233. is raised.

          This method replaces the `Content-Type' header, keeping all
          the parameters in place.  If _requote_ is `False', this
          leaves the existing header's quoting as is, otherwise the
          parameters will be quoted (the default).

          An alternative header can be specified in the _header_
          argument. When the `Content-Type' header is set a
          `MIME-Version' header is also added.

          New in version 2.2.2.

      -- Method: get_filename ([failobj])
          Return the value of the `filename' parameter of the
          `Content-Disposition' header of the message.  If the header
          does not have a `filename' parameter, this method falls back
          to looking for the `name' parameter on the `Content-Type'
          header.  If neither is found, or the header is missing, then
          _failobj_ is returned.  The returned string will always be
          unquoted as per *note email.utils.unquote(): 1780.

      -- Method: get_boundary ([failobj])
          Return the value of the `boundary' parameter of the
          `Content-Type' header of the message, or _failobj_ if either
          the header is missing, or has no `boundary' parameter.  The
          returned string will always be unquoted as per *note
          email.utils.unquote(): 1780.

      -- Method: set_boundary (boundary)
          Set the `boundary' parameter of the `Content-Type' header to
          _boundary_.  *note set_boundary(): 1782. will always quote
          _boundary_ if necessary.  A `HeaderParseError' is raised if
          the message object has no `Content-Type' header.

          Note that using this method is subtly different than deleting
          the old `Content-Type' header and adding a new one with the
          new boundary via *note add_header(): 1772, because *note
          set_boundary(): 1782. preserves the order of the
          `Content-Type' header in the list of headers. However, it
          does _not_ preserve any continuation lines which may have
          been present in the original `Content-Type' header.

      -- Method: get_content_charset ([failobj])
          Return the `charset' parameter of the `Content-Type' header,
          coerced to lower case.  If there is no `Content-Type' header,
          or if that header has no `charset' parameter, _failobj_ is
          returned.

          Note that this method differs from *note get_charset(): 1766.
          which returns the *note Charset: 1765. instance for the
          default encoding of the message body.

          New in version 2.2.2.

      -- Method: get_charsets ([failobj])
          Return a list containing the character set names in the
          message.  If the message is a `multipart', then the list will
          contain one element for each subpart in the payload,
          otherwise, it will be a list of length 1.

          Each item in the list will be a string which is the value of
          the `charset' parameter in the `Content-Type' header for the
          represented subpart.  However, if the subpart has no
          `Content-Type' header, no `charset' parameter, or is not of
          the `text' main MIME type, then that item in the returned list
          will be _failobj_.

      -- Method: walk ()
          The *note walk(): 1785. method is an all-purpose generator
          which can be used to iterate over all the parts and subparts
          of a message object tree, in depth-first traversal order.
          You will typically use *note walk(): 1785. as the iterator in
          a `for' loop; each iteration returns the next subpart.

          Here's an example that prints the MIME type of every part of
          a multipart message structure:

              >>> for part in msg.walk():
              ...     print part.get_content_type()
              multipart/report
              text/plain
              message/delivery-status
              text/plain
              text/plain
              message/rfc822



     Changed in version 2.5: The previously deprecated methods
     `get_type()', `get_main_type()', and `get_subtype()' were removed.

     *note Message: 213. objects can also optionally contain two
     instance attributes, which can be used when generating the plain
     text of a MIME message.

      -- Attribute: preamble
          The format of a MIME document allows for some text between
          the blank line following the headers, and the first multipart
          boundary string. Normally, this text is never visible in a
          MIME-aware mail reader because it falls outside the standard
          MIME armor.  However, when viewing the raw text of the
          message, or when viewing the message in a non-MIME aware
          reader, this text can become visible.

          The _preamble_ attribute contains this leading extra-armor
          text for MIME documents.  When the *note Parser: 1787.
          discovers some text after the headers but before the first
          boundary string, it assigns this text to the message's
          _preamble_ attribute.  When the *note Generator: 175c. is
          writing out the plain text representation of a MIME message,
          and it finds the message has a _preamble_ attribute, it will
          write this text in the area between the headers and the first
          boundary.  See *note email.parser: c5. and *note
          email.generator: c0. for details.

          Note that if the message object has no preamble, the
          _preamble_ attribute will be `None'.

      -- Attribute: epilogue
          The _epilogue_ attribute acts the same way as the _preamble_
          attribute, except that it contains text that appears between
          the last boundary and the end of the message.

          Changed in version 2.5: You do not need to set the epilogue
          to the empty string in order for the `Generator' to print a
          newline at the end of the file.

      -- Attribute: defects
          The _defects_ attribute contains a list of all the problems
          found when parsing this message.  See *note email.errors: bf.
          for a detailed description of the possible parsing defects.

          New in version 2.4.

  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc2822.html

  (2) http://tools.ietf.org/html/rfc2822.html

  (3) http://tools.ietf.org/html/rfc2231.html

  (4) http://tools.ietf.org/html/rfc2045.html

  (5) http://tools.ietf.org/html/rfc2045.html

  (6) http://tools.ietf.org/html/rfc2045.html

  (7) http://tools.ietf.org/html/rfc2231.html

  (8) http://tools.ietf.org/html/rfc2231.html

  (9) http://tools.ietf.org/html/rfc2045.html

  (10) http://tools.ietf.org/html/rfc2231.html


File: python.info,  Node: email Parsing email messages,  Next: email Generating MIME documents,  Prev: email Representing an email message,  Up: email --- An email and MIME handling package

5.18.1.2 `email': Parsing email messages
........................................

Message object structures can be created in one of two ways: they can
be created from whole cloth by instantiating *note Message: 213.
objects and stringing them together via `attach()' and `set_payload()'
calls, or they can be created by parsing a flat text representation of
the email message.

  The *note email: bc. package provides a standard parser that
understands most email document structures, including MIME documents.
You can pass the parser a string or a file object, and the parser will
return to you the root *note Message: 213. instance of the object
structure.  For simple, non-MIME messages the payload of this root
object will likely be a string containing the text of the message.  For
MIME messages, the root object will return `True' from its
`is_multipart()' method, and the subparts can be accessed via the
`get_payload()' and `walk()' methods.

  There are actually two parser interfaces available for use, the
classic *note Parser: 1787. API and the incremental *note FeedParser:
178c. API.  The classic *note Parser: 1787. API is fine if you have the
entire text of the message in memory as a string, or if the entire
message lives in a file on the file system.  *note FeedParser: 178c. is
more appropriate for when you're reading the message from a stream
which might block waiting for more input (e.g. reading an email message
from a socket).  The *note FeedParser: 178c. can consume and parse the
message incrementally, and only returns the root object when you close
the parser (1).

  Note that the parser can be extended in limited ways, and of course
you can implement your own parser completely from scratch.  There is no
magical connection between the *note email: bc. package's bundled
parser and the *note Message: 213. class, so your custom parser can
create message object trees any way it finds necessary.

* Menu:

* FeedParser API::
* Parser class API::
* Additional notes::

  ---------- Footnotes ----------

  (1) As of email package version 3.0, introduced in Python 2.4, the
classic `Parser' was re-implemented in terms of the `FeedParser', so the
semantics and results are identical between the two parsers.


File: python.info,  Node: FeedParser API,  Next: Parser class API,  Up: email Parsing email messages

5.18.1.3 FeedParser API
.......................

New in version 2.4.

  The *note FeedParser: 178c, imported from the `email.feedparser'
module, provides an API that is conducive to incremental parsing of
email messages, such as would be necessary when reading the text of an
email message from a source that can block (e.g. a socket).  The *note
FeedParser: 178c. can of course be used to parse an email message fully
contained in a string or a file, but the classic *note Parser: 1787.
API may be more convenient for such use cases.  The semantics and
results of the two parser APIs are identical.

  The *note FeedParser: 178c.'s API is simple; you create an instance,
feed it a bunch of text until there's no more to feed it, then close
the parser to retrieve the root message object.  The *note FeedParser:
178c. is extremely accurate when parsing standards-compliant messages,
and it does a very good job of parsing non-compliant messages,
providing information about how a message was deemed broken.  It will
populate a message object's _defects_ attribute with a list of any
problems it found in a message.  See the *note email.errors: bf. module
for the list of defects that it can find.

  Here is the API for the *note FeedParser: 178c.:

 -- Class: email.parser.FeedParser ([_factory])
     Create a *note FeedParser: 178c. instance.  Optional __factory_ is
     a no-argument callable that will be called whenever a new message
     object is needed.  It defaults to the *note email.message.Message:
     213. class.

      -- Method: feed (data)
          Feed the *note FeedParser: 178c. some more data.  _data_
          should be a string containing one or more lines.  The lines
          can be partial and the *note FeedParser: 178c. will stitch
          such partial lines together properly.  The lines in the
          string can have any of the common three line endings,
          carriage return, newline, or carriage return and newline
          (they can even be mixed).

      -- Method: close ()
          Closing a *note FeedParser: 178c. completes the parsing of
          all previously fed data, and returns the root message object.
          It is undefined what happens if you feed more data to a
          closed *note FeedParser: 178c.


File: python.info,  Node: Parser class API,  Next: Additional notes,  Prev: FeedParser API,  Up: email Parsing email messages

5.18.1.4 Parser class API
.........................

The *note Parser: 1787. class, imported from the *note email.parser:
c5. module, provides an API that can be used to parse a message when
the complete contents of the message are available in a string or file.
The *note email.parser: c5.  module also provides a second class,
called `HeaderParser' which can be used if you're only interested in
the headers of the message.  `HeaderParser' can be much faster in these
situations, since it does not attempt to parse the message body,
instead setting the payload to the raw body as a string. `HeaderParser'
has the same API as the *note Parser: 1787.  class.

 -- Class: email.parser.Parser ([_class])
     The constructor for the *note Parser: 1787. class takes an
     optional argument __class_.  This must be a callable factory (such
     as a function or a class), and it is used whenever a sub-message
     object needs to be created.  It defaults to *note Message: 213.
     (see *note email.message: c3.).  The factory will be called
     without arguments.

     The optional _strict_ flag is ignored.

     Deprecated since version 2.4: Because the *note Parser: 1787.
     class is a backward compatible API wrapper around the
     new-in-Python 2.4 *note FeedParser: 178c, _all_ parsing is
     effectively non-strict.  You should simply stop passing a _strict_
     flag to the *note Parser: 1787. constructor.

     Changed in version 2.2.2: The _strict_ flag was added.

     Changed in version 2.4: The _strict_ flag was deprecated.

     The other public *note Parser: 1787. methods are:

      -- Method: parse (fp[, headersonly])
          Read all the data from the file-like object _fp_, parse the
          resulting text, and return the root message object.  _fp_
          must support both the *note readline(): 145. and the `read()'
          methods on file-like objects.

          The text contained in _fp_ must be formatted as a block of RFC
          2822(1) style headers and header continuation lines,
          optionally preceded by a envelope header.  The header block
          is terminated either by the end of the data or by a blank
          line.  Following the header block is the body of the message
          (which may contain MIME-encoded subparts).

          Optional _headersonly_ is as with the *note parse(): 1791.
          method.

          Changed in version 2.2.2: The _headersonly_ flag was added.

      -- Method: parsestr (text[, headersonly])
          Similar to the *note parse(): 1791. method, except it takes a
          string object instead of a file-like object.  Calling this
          method on a string is exactly equivalent to wrapping _text_
          in a *note StringIO: 165. instance first and calling *note
          parse(): 1791.

          Optional _headersonly_ is a flag specifying whether to stop
          parsing after reading the headers or not.  The default is
          `False', meaning it parses the entire contents of the file.

          Changed in version 2.2.2: The _headersonly_ flag was added.

  Since creating a message object structure from a string or a file
object is such a common task, two functions are provided as a
convenience.  They are available in the top-level *note email: bc.
package namespace.

 -- Function: email.message_from_string (s[, _class[, strict]])
     Return a message object structure from a string.  This is exactly
     equivalent to `Parser().parsestr(s)'.  Optional __class_ and
     _strict_ are interpreted as with the `Parser' class constructor.

     Changed in version 2.2.2: The _strict_ flag was added.

 -- Function: email.message_from_file (fp[, _class[, strict]])
     Return a message object structure tree from an open file object.
     This is exactly equivalent to `Parser().parse(fp)'.  Optional
     __class_ and _strict_ are interpreted as with the `Parser' class
     constructor.

     Changed in version 2.2.2: The _strict_ flag was added.

  Here's an example of how you might use this at an interactive Python
prompt:

    >>> import email
    >>> msg = email.message_from_string(myString)


  ---------- Footnotes ----------

  (1) http://tools.ietf.org/html/rfc2822.html

